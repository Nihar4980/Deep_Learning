{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce5f4bfb-2172-4cb7-af16-bb5203ea644f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "782485d1-eedc-4a6e-90af-a8418b772797",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, in_channels = 3, embed_dim = 768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        self.proj = nn.Conv2d( in_channels, embed_dim, kernel_size = patch_size, stride = patch_size,)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x) # (n_samples, emed_dim, n_patches ** 0.5, n_patches ** 0.5)\n",
    "        x = x.flatten(2) # (n_samples, emed_dim, n_patches)\n",
    "        x = x.transpose(1,2) # n_samples, n_patches, embed_dim)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c38a598-d9ff-4bae-8217-1b95e5a4dd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Create a tensor of shape [max_len, dim] for encoding\n",
    "        pe = torch.zeros(max_len, dim)\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)  # [max_len, 1]\n",
    "        div_term = torch.exp(torch.arange(0, dim, 2).float() * -(math.log(10000.0) / dim))  # [dim/2]\n",
    "\n",
    "        # Apply sin and cos functions for positional encoding\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # even indices (sine)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # odd indices (cosine)\n",
    "\n",
    "        pe = pe.unsqueeze(0)  # Shape becomes [1, max_len, dim]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to the input tensor\n",
    "        return x + self.pe[:, :x.size(1),:]  # x.size(1) is the length of the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3be9fa1-7a53-427e-b4cc-05239ef655da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, n_heads=12, qkv_bias=True, attn_p=0., proj_p=0.):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim//n_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim*3, bias = qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_p)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        n_samples, n_tokens, dim = x.shape \n",
    " \n",
    "        if dim != self.dim:\n",
    "            raise ValueError\n",
    "            \n",
    "        qkv = self.qkv(x) # (n_samples, n_patches +1, 3 * dim)\n",
    "        qkv = qkv.reshape( n_samples, n_tokens, 3, self.n_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4) # (3, n_samples, n_heads, n_patches + 1,head_dim )\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        k_t = k.transpose(-2, -1) # (n_samples, n_heads, head_dim, n_patches +1)\n",
    "\n",
    "        dp = (q @ k_t) * self.scale\n",
    "\n",
    "        attn = dp.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        weighted_avg = attn @ v\n",
    "        weighted_avg = weighted_avg.transpose(1, 2)\n",
    "        weighted_avg = weighted_avg.flatten(2)\n",
    "\n",
    "        x = self.proj(weighted_avg)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac389f04-292f-49f6-bc82-150267243a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention_qkv(nn.Module):\n",
    "    def __init__(self, dim, n_heads=12, qkv_bias=True, attn_p=0., proj_p=0.):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim//n_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim*3, bias = qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        n_samples, n_tokens, dim = x.shape\n",
    " \n",
    "        if dim != self.dim:\n",
    "            raise ValueError\n",
    "            \n",
    "        qkv = self.qkv(x) # (n_samples, n_patches +1, 3 * dim)\n",
    "        qkv = qkv.reshape( n_samples, n_tokens, 3, self.n_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4) # (3, n_samples, n_heads, n_patches + 1,head_dim )\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        q = q * self.scale\n",
    "        attn = q.unsqueeze(3) @ k[:,:,self.attn_idx].transpose(-1,-2) #B,nh,L,1,K^2\n",
    "        attn = attn + self.relative_bias[self.bias_idx].permute(2, 0, 1).unsqueeze(2)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = (attn @ v[:,:,self.attn_idx]).squeeze(3).transpose(-1,-2).contiguous().view(B,C,H,W)\n",
    "        return x\n",
    "        \n",
    "    def get_bias_idx(self,H,W):\n",
    "        num_repeat_h = torch.ones(self.window_size,dtype=torch.long)\n",
    "        num_repeat_w = torch.ones(self.window_size,dtype=torch.long)\n",
    "        num_repeat_h[self.window_size//2] = H-(self.window_size-1)\n",
    "        num_repeat_w[self.window_size//2] = W-(self.window_size-1)\n",
    "        bias_hw = (self.idx_h.repeat_interleave(num_repeat_h).unsqueeze(-1) * (2*self.window_size-1)) + self.idx_w.repeat_interleave(num_repeat_w)\n",
    "        bias_idx = bias_hw.unsqueeze(-1) + self.idx_k\n",
    "        return bias_idx.view(-1,self.window_size**2)\n",
    "    \n",
    "    def get_attn_idx(self,H,W):\n",
    "        H_ = H - (self.window_size - 1)\n",
    "        W_ = W - (self.window_size - 1)\n",
    "        attn_idx = torch.arange(0,H_*W_,dtype=torch.float).view(1,1,H_,W_)\n",
    "        attn_idx = self.pad_idx(attn_idx).view(-1).type(torch.long)\n",
    "        attn_idx = self.get_unfold_idx(H,W)[attn_idx]\n",
    "        return attn_idx\n",
    "    \n",
    "    def get_unfold_idx(self,H,W):\n",
    "        H_ = H-(self.window_size-1)\n",
    "        W_ = W-(self.window_size-1)\n",
    "        h_idx = torch.arange(W_).repeat(H_)\n",
    "        w_idx = torch.arange(H_).repeat_interleave(W_) * W\n",
    "        k_idx_1 = torch.arange(self.window_size).repeat(self.window_size)\n",
    "        k_idx_2 = torch.arange(self.window_size).repeat_interleave(self.window_size) * W\n",
    "        k_idx = k_idx_1 + k_idx_2\n",
    "        hw_idx = h_idx + w_idx\n",
    "        unfold_idx = hw_idx[:,None] + k_idx\n",
    "        return unfold_idx\n",
    "    def set_input_size(self,input_size):\n",
    "        H,W = input_size\n",
    "        self.H,self.W = H,W\n",
    "        assert H >= self.window_size and W >= self.window_size,'input size must not be smaller than window size'\n",
    "        attn_idx = self.get_attn_idx(H,W)\n",
    "        bias_idx = self.get_bias_idx(H,W)\n",
    "        self.register_buffer(\"attn_idx\", attn_idx)\n",
    "        self.register_buffer(\"bias_idx\",bias_idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7cc6625-5f97-40ec-9987-4f3f056e2368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 196, 768])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(10,3,224,224)\n",
    "patch = PatchEmbed(224,16)\n",
    "x  = patch(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7c470c1-ed00-4ad6-bc8a-f146ab576a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 196, 768])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = PositionalEncoding(768,196)\n",
    "x = pos(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d57c489b-4897-431a-843b-cdc5d2a51f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 196, 1, 768])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.reshape(10,196,1,768)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0bcf5103-f680-423c-b848-9639e77ae080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import natten\n",
    "from natten import NeighborhoodAttention2D as NeighborhoodAttention\n",
    "attn = NeighborhoodAttention(\n",
    "            768,\n",
    "            kernel_size=7,\n",
    "            dilation=2,\n",
    "            num_heads=12,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8d2447ef-e59c-4218-9dad-d8a92211e2eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input axes must be greater than or equal to the product of kernel size and dilation. Got kernel size 7, dilation 2, but dimension size was 1.\nException raised from CheckArgsAgainstDim at /natten-build/csrc/./include/natten/pytorch/helpers.h:101 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7fd4b41af4d7 in /root/miniconda3/envs/tf/lib/python3.9/site-packages/torch/lib/libc10.so)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7fd4b417936b in /root/miniconda3/envs/tf/lib/python3.9/site-packages/torch/lib/libc10.so)\nframe #2: natten::pytorch::CheckArgsAgainstDim(int, int, int) + 0x9a (0x7fd332b2c95d in /root/miniconda3/envs/tf/lib/python3.9/site-packages/natten/libnatten.cpython-39-x86_64-linux-gnu.so)\nframe #3: natten::pytorch::CheckArgsAgainstDim(std::tuple<int, int> const&, std::tuple<int, int> const&, std::tuple<int, int> const&) + 0x85 (0x7fd332b32e0b in /root/miniconda3/envs/tf/lib/python3.9/site-packages/natten/libnatten.cpython-39-x86_64-linux-gnu.so)\nframe #4: natten::pytorch::na2d_qk_forward(at::Tensor&, at::Tensor const&, at::Tensor const&, c10::optional<at::Tensor> const&, std::tuple<int, int> const&, std::tuple<int, int> const&, std::tuple<bool, bool> const&) + 0x330 (0x7fd332b30ee8 in /root/miniconda3/envs/tf/lib/python3.9/site-packages/natten/libnatten.cpython-39-x86_64-linux-gnu.so)\nframe #5: <unknown function> + 0x59dc1d4 (0x7fd331b8f1d4 in /root/miniconda3/envs/tf/lib/python3.9/site-packages/natten/libnatten.cpython-39-x86_64-linux-gnu.so)\nframe #6: <unknown function> + 0x59d63ef (0x7fd331b893ef in /root/miniconda3/envs/tf/lib/python3.9/site-packages/natten/libnatten.cpython-39-x86_64-linux-gnu.so)\nframe #7: <unknown function> + 0x59d1932 (0x7fd331b84932 in /root/miniconda3/envs/tf/lib/python3.9/site-packages/natten/libnatten.cpython-39-x86_64-linux-gnu.so)\nframe #8: <unknown function> + 0x59d1c7c (0x7fd331b84c7c in /root/miniconda3/envs/tf/lib/python3.9/site-packages/natten/libnatten.cpython-39-x86_64-linux-gnu.so)\nframe #9: <unknown function> + 0x301d127 (0x7fd32f1d0127 in /root/miniconda3/envs/tf/lib/python3.9/site-packages/natten/libnatten.cpython-39-x86_64-linux-gnu.so)\nframe #10: /root/miniconda3/envs/tf/bin/python() [0x5072d7]\nframe #11: _PyObject_MakeTpCall + 0x2ec (0x4f06ac in /root/miniconda3/envs/tf/bin/python)\nframe #12: _PyEval_EvalFrameDefault + 0x526b (0x4ecbfb in /root/miniconda3/envs/tf/bin/python)\nframe #13: /root/miniconda3/envs/tf/bin/python() [0x4f8053]\nframe #14: _PyEval_EvalFrameDefault + 0x3764 (0x4eb0f4 in /root/miniconda3/envs/tf/bin/python)\nframe #15: /root/miniconda3/envs/tf/bin/python() [0x4e6a8a]\nframe #16: _PyFunction_Vectorcall + 0xd4 (0x4f7d84 in /root/miniconda3/envs/tf/bin/python)\nframe #17: THPFunction_apply(_object*, _object*) + 0x116a (0x7fd492ddf1ca in /root/miniconda3/envs/tf/lib/python3.9/site-packages/torch/lib/libtorch_python.so)\nframe #18: /root/miniconda3/envs/tf/bin/python() [0x507300]\nframe #19: PyObject_Call + 0x158 (0x5057c8 in /root/miniconda3/envs/tf/bin/python)\nframe #20: _PyEval_EvalFrameDefault + 0x5baf (0x4ed53f in /root/miniconda3/envs/tf/bin/python)\nframe #21: /root/miniconda3/envs/tf/bin/python() [0x4e6a8a]\nframe #22: /root/miniconda3/envs/tf/bin/python() [0x504f6c]\nframe #23: _PyEval_EvalFrameDefault + 0x4d44 (0x4ec6d4 in /root/miniconda3/envs/tf/bin/python)\nframe #24: /root/miniconda3/envs/tf/bin/python() [0x4e6a8a]\nframe #25: _PyFunction_Vectorcall + 0xd4 (0x4f7d84 in /root/miniconda3/envs/tf/bin/python)\nframe #26: _PyEval_EvalFrameDefault + 0x1231 (0x4e8bc1 in /root/miniconda3/envs/tf/bin/python)\nframe #27: /root/miniconda3/envs/tf/bin/python() [0x4f8053]\nframe #28: /root/miniconda3/envs/tf/bin/python() [0x505071]\nframe #29: _PyEval_EvalFrameDefault + 0x3764 (0x4eb0f4 in /root/miniconda3/envs/tf/bin/python)\nframe #30: /root/miniconda3/envs/tf/bin/python() [0x4e6a8a]\nframe #31: _PyObject_FastCallDictTstate + 0x13e (0x4eff1e in /root/miniconda3/envs/tf/bin/python)\nframe #32: _PyObject_Call_Prepend + 0x66 (0x502cc6 in /root/miniconda3/envs/tf/bin/python)\nframe #33: /root/miniconda3/envs/tf/bin/python() [0x5cb1e3]\nframe #34: _PyObject_MakeTpCall + 0x2ec (0x4f06ac in /root/miniconda3/envs/tf/bin/python)\nframe #35: _PyEval_EvalFrameDefault + 0x4c84 (0x4ec614 in /root/miniconda3/envs/tf/bin/python)\nframe #36: /root/miniconda3/envs/tf/bin/python() [0x4e6a8a]\nframe #37: _PyEval_EvalCodeWithName + 0x47 (0x4e6717 in /root/miniconda3/envs/tf/bin/python)\nframe #38: PyEval_EvalCodeEx + 0x39 (0x4e66c9 in /root/miniconda3/envs/tf/bin/python)\nframe #39: PyEval_EvalCode + 0x1b (0x59398b in /root/miniconda3/envs/tf/bin/python)\nframe #40: /root/miniconda3/envs/tf/bin/python() [0x5985a1]\nframe #41: /root/miniconda3/envs/tf/bin/python() [0x4f87f4]\nframe #42: _PyEval_EvalFrameDefault + 0x3c9 (0x4e7d59 in /root/miniconda3/envs/tf/bin/python)\nframe #43: /root/miniconda3/envs/tf/bin/python() [0x50ba7c]\nframe #44: _PyEval_EvalFrameDefault + 0x5e5c (0x4ed7ec in /root/miniconda3/envs/tf/bin/python)\nframe #45: /root/miniconda3/envs/tf/bin/python() [0x50ba7c]\nframe #46: _PyEval_EvalFrameDefault + 0x5e5c (0x4ed7ec in /root/miniconda3/envs/tf/bin/python)\nframe #47: /root/miniconda3/envs/tf/bin/python() [0x50ba7c]\nframe #48: /root/miniconda3/envs/tf/bin/python() [0x5035fd]\nframe #49: _PyEval_EvalFrameDefault + 0x686 (0x4e8016 in /root/miniconda3/envs/tf/bin/python)\nframe #50: /root/miniconda3/envs/tf/bin/python() [0x4f8053]\nframe #51: _PyEval_EvalFrameDefault + 0x3c9 (0x4e7d59 in /root/miniconda3/envs/tf/bin/python)\nframe #52: /root/miniconda3/envs/tf/bin/python() [0x4f8053]\nframe #53: _PyEval_EvalFrameDefault + 0x686 (0x4e8016 in /root/miniconda3/envs/tf/bin/python)\nframe #54: /root/miniconda3/envs/tf/bin/python() [0x4e6a8a]\nframe #55: /root/miniconda3/envs/tf/bin/python() [0x504fdd]\nframe #56: PyObject_Call + 0xb4 (0x505724 in /root/miniconda3/envs/tf/bin/python)\nframe #57: _PyEval_EvalFrameDefault + 0x3764 (0x4eb0f4 in /root/miniconda3/envs/tf/bin/python)\nframe #58: /root/miniconda3/envs/tf/bin/python() [0x4e6a8a]\nframe #59: /root/miniconda3/envs/tf/bin/python() [0x504fdd]\nframe #60: _PyEval_EvalFrameDefault + 0x1231 (0x4e8bc1 in /root/miniconda3/envs/tf/bin/python)\nframe #61: /root/miniconda3/envs/tf/bin/python() [0x50ba7c]\nframe #62: _PyEval_EvalFrameDefault + 0x5e5c (0x4ed7ec in /root/miniconda3/envs/tf/bin/python)\nframe #63: /root/miniconda3/envs/tf/bin/python() [0x50ba7c]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/natten/na2d.py:141\u001b[0m, in \u001b[0;36mNeighborhoodAttention2D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    139\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m qkv[\u001b[38;5;241m0\u001b[39m], qkv[\u001b[38;5;241m1\u001b[39m], qkv[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    140\u001b[0m q \u001b[38;5;241m=\u001b[39m q \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\n\u001b[0;32m--> 141\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[43mna2d_qk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdilation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrpb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrpb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m attn \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    150\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_drop(attn)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/natten/functional.py:1622\u001b[0m, in \u001b[0;36mna2d_qk\u001b[0;34m(query, key, kernel_size, dilation, additional_keys, is_causal, rpb)\u001b[0m\n\u001b[1;32m   1612\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m query\u001b[38;5;241m.\u001b[39mis_nested \u001b[38;5;129;01mor\u001b[39;00m key\u001b[38;5;241m.\u001b[39mis_nested:\n\u001b[1;32m   1613\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m na2d_qk_nested(\n\u001b[1;32m   1614\u001b[0m         query,\n\u001b[1;32m   1615\u001b[0m         key,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1620\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[1;32m   1621\u001b[0m     )\n\u001b[0;32m-> 1622\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mNeighborhoodAttention2DQKAutogradFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1623\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrpb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madditional_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\n\u001b[1;32m   1624\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:98\u001b[0m, in \u001b[0;36mcustom_fwd.<locals>.decorate_fwd\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cast_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     97\u001b[0m     args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fwd_used_autocast \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_autocast_enabled()\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfwd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     autocast_context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_autocast_enabled()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/natten/functional.py:475\u001b[0m, in \u001b[0;36mNeighborhoodAttention2DQKAutogradFunction.forward\u001b[0;34m(ctx, query, key, bias, additional_key, kernel_size_, dilation_, is_causal_)\u001b[0m\n\u001b[1;32m    472\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m additional_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m attn_add\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    473\u001b[0m     qk_cross_forward(query, additional_key, attn_add)\n\u001b[0;32m--> 475\u001b[0m \u001b[43mlibnatten\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mna2d_qk_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_na\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    478\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(query, key, bias, additional_key)\n\u001b[1;32m    479\u001b[0m ctx\u001b[38;5;241m.\u001b[39mkernel_size \u001b[38;5;241m=\u001b[39m kernel_size\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input axes must be greater than or equal to the product of kernel size and dilation. Got kernel size 7, dilation 2, but dimension size was 1.\nException raised from CheckArgsAgainstDim at /natten-build/csrc/./include/natten/pytorch/helpers.h:101 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7fd4b41af4d7 in /root/miniconda3/envs/tf/lib/python3.9/site-packages/torch/lib/libc10.so)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7fd4b417936b in /root/miniconda3/envs/tf/lib/python3.9/site-packages/torch/lib/libc10.so)\nframe #2: natten::pytorch::CheckArgsAgainstDim(int, int, int) + 0x9a (0x7fd332b2c95d in /root/miniconda3/envs/tf/lib/python3.9/site-packages/natten/libnatten.cpython-39-x86_64-linux-gnu.so)\nframe #3: natten::pytorch::CheckArgsAgainstDim(std::tuple<int, int> const&, std::tuple<int, int> const&, std::tuple<int, int> const&) + 0x85 (0x7fd332b32e0b in /root/miniconda3/envs/tf/lib/python3.9/site-packages/natten/libnatten.cpython-39-x86_64-linux-gnu.so)\nframe #4: natten::pytorch::na2d_qk_forward(at::Tensor&, at::Tensor const&, at::Tensor const&, c10::optional<at::Tensor> const&, std::tuple<int, int> const&, std::tuple<int, int> const&, std::tuple<bool, bool> const&) + 0x330 (0x7fd332b30ee8 in /root/miniconda3/envs/tf/lib/python3.9/site-packages/natten/libnatten.cpython-39-x86_64-linux-gnu.so)\nframe #5: <unknown function> + 0x59dc1d4 (0x7fd331b8f1d4 in /root/miniconda3/envs/tf/lib/python3.9/site-packages/natten/libnatten.cpython-39-x86_64-linux-gnu.so)\nframe #6: <unknown function> + 0x59d63ef (0x7fd331b893ef in /root/miniconda3/envs/tf/lib/python3.9/site-packages/natten/libnatten.cpython-39-x86_64-linux-gnu.so)\nframe #7: <unknown function> + 0x59d1932 (0x7fd331b84932 in /root/miniconda3/envs/tf/lib/python3.9/site-packages/natten/libnatten.cpython-39-x86_64-linux-gnu.so)\nframe #8: <unknown function> + 0x59d1c7c (0x7fd331b84c7c in /root/miniconda3/envs/tf/lib/python3.9/site-packages/natten/libnatten.cpython-39-x86_64-linux-gnu.so)\nframe #9: <unknown function> + 0x301d127 (0x7fd32f1d0127 in /root/miniconda3/envs/tf/lib/python3.9/site-packages/natten/libnatten.cpython-39-x86_64-linux-gnu.so)\nframe #10: /root/miniconda3/envs/tf/bin/python() [0x5072d7]\nframe #11: _PyObject_MakeTpCall + 0x2ec (0x4f06ac in /root/miniconda3/envs/tf/bin/python)\nframe #12: _PyEval_EvalFrameDefault + 0x526b (0x4ecbfb in /root/miniconda3/envs/tf/bin/python)\nframe #13: /root/miniconda3/envs/tf/bin/python() [0x4f8053]\nframe #14: _PyEval_EvalFrameDefault + 0x3764 (0x4eb0f4 in /root/miniconda3/envs/tf/bin/python)\nframe #15: /root/miniconda3/envs/tf/bin/python() [0x4e6a8a]\nframe #16: _PyFunction_Vectorcall + 0xd4 (0x4f7d84 in /root/miniconda3/envs/tf/bin/python)\nframe #17: THPFunction_apply(_object*, _object*) + 0x116a (0x7fd492ddf1ca in /root/miniconda3/envs/tf/lib/python3.9/site-packages/torch/lib/libtorch_python.so)\nframe #18: /root/miniconda3/envs/tf/bin/python() [0x507300]\nframe #19: PyObject_Call + 0x158 (0x5057c8 in /root/miniconda3/envs/tf/bin/python)\nframe #20: _PyEval_EvalFrameDefault + 0x5baf (0x4ed53f in /root/miniconda3/envs/tf/bin/python)\nframe #21: /root/miniconda3/envs/tf/bin/python() [0x4e6a8a]\nframe #22: /root/miniconda3/envs/tf/bin/python() [0x504f6c]\nframe #23: _PyEval_EvalFrameDefault + 0x4d44 (0x4ec6d4 in /root/miniconda3/envs/tf/bin/python)\nframe #24: /root/miniconda3/envs/tf/bin/python() [0x4e6a8a]\nframe #25: _PyFunction_Vectorcall + 0xd4 (0x4f7d84 in /root/miniconda3/envs/tf/bin/python)\nframe #26: _PyEval_EvalFrameDefault + 0x1231 (0x4e8bc1 in /root/miniconda3/envs/tf/bin/python)\nframe #27: /root/miniconda3/envs/tf/bin/python() [0x4f8053]\nframe #28: /root/miniconda3/envs/tf/bin/python() [0x505071]\nframe #29: _PyEval_EvalFrameDefault + 0x3764 (0x4eb0f4 in /root/miniconda3/envs/tf/bin/python)\nframe #30: /root/miniconda3/envs/tf/bin/python() [0x4e6a8a]\nframe #31: _PyObject_FastCallDictTstate + 0x13e (0x4eff1e in /root/miniconda3/envs/tf/bin/python)\nframe #32: _PyObject_Call_Prepend + 0x66 (0x502cc6 in /root/miniconda3/envs/tf/bin/python)\nframe #33: /root/miniconda3/envs/tf/bin/python() [0x5cb1e3]\nframe #34: _PyObject_MakeTpCall + 0x2ec (0x4f06ac in /root/miniconda3/envs/tf/bin/python)\nframe #35: _PyEval_EvalFrameDefault + 0x4c84 (0x4ec614 in /root/miniconda3/envs/tf/bin/python)\nframe #36: /root/miniconda3/envs/tf/bin/python() [0x4e6a8a]\nframe #37: _PyEval_EvalCodeWithName + 0x47 (0x4e6717 in /root/miniconda3/envs/tf/bin/python)\nframe #38: PyEval_EvalCodeEx + 0x39 (0x4e66c9 in /root/miniconda3/envs/tf/bin/python)\nframe #39: PyEval_EvalCode + 0x1b (0x59398b in /root/miniconda3/envs/tf/bin/python)\nframe #40: /root/miniconda3/envs/tf/bin/python() [0x5985a1]\nframe #41: /root/miniconda3/envs/tf/bin/python() [0x4f87f4]\nframe #42: _PyEval_EvalFrameDefault + 0x3c9 (0x4e7d59 in /root/miniconda3/envs/tf/bin/python)\nframe #43: /root/miniconda3/envs/tf/bin/python() [0x50ba7c]\nframe #44: _PyEval_EvalFrameDefault + 0x5e5c (0x4ed7ec in /root/miniconda3/envs/tf/bin/python)\nframe #45: /root/miniconda3/envs/tf/bin/python() [0x50ba7c]\nframe #46: _PyEval_EvalFrameDefault + 0x5e5c (0x4ed7ec in /root/miniconda3/envs/tf/bin/python)\nframe #47: /root/miniconda3/envs/tf/bin/python() [0x50ba7c]\nframe #48: /root/miniconda3/envs/tf/bin/python() [0x5035fd]\nframe #49: _PyEval_EvalFrameDefault + 0x686 (0x4e8016 in /root/miniconda3/envs/tf/bin/python)\nframe #50: /root/miniconda3/envs/tf/bin/python() [0x4f8053]\nframe #51: _PyEval_EvalFrameDefault + 0x3c9 (0x4e7d59 in /root/miniconda3/envs/tf/bin/python)\nframe #52: /root/miniconda3/envs/tf/bin/python() [0x4f8053]\nframe #53: _PyEval_EvalFrameDefault + 0x686 (0x4e8016 in /root/miniconda3/envs/tf/bin/python)\nframe #54: /root/miniconda3/envs/tf/bin/python() [0x4e6a8a]\nframe #55: /root/miniconda3/envs/tf/bin/python() [0x504fdd]\nframe #56: PyObject_Call + 0xb4 (0x505724 in /root/miniconda3/envs/tf/bin/python)\nframe #57: _PyEval_EvalFrameDefault + 0x3764 (0x4eb0f4 in /root/miniconda3/envs/tf/bin/python)\nframe #58: /root/miniconda3/envs/tf/bin/python() [0x4e6a8a]\nframe #59: /root/miniconda3/envs/tf/bin/python() [0x504fdd]\nframe #60: _PyEval_EvalFrameDefault + 0x1231 (0x4e8bc1 in /root/miniconda3/envs/tf/bin/python)\nframe #61: /root/miniconda3/envs/tf/bin/python() [0x50ba7c]\nframe #62: _PyEval_EvalFrameDefault + 0x5e5c (0x4ed7ec in /root/miniconda3/envs/tf/bin/python)\nframe #63: /root/miniconda3/envs/tf/bin/python() [0x50ba7c]\n"
     ]
    }
   ],
   "source": [
    "attn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51f72a39-b14f-4320-af7e-9938774a95e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Attention_qkv' object has no attribute 'attn_idx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m attn \u001b[38;5;241m=\u001b[39m Attention_qkv(\u001b[38;5;241m768\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m q,k,v \u001b[38;5;241m=\u001b[39m \u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m q\u001b[38;5;241m.\u001b[39mshape, k\u001b[38;5;241m.\u001b[39mshape, v\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[12], line 22\u001b[0m, in \u001b[0;36mAttention_qkv.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m qkv[\u001b[38;5;241m0\u001b[39m], qkv[\u001b[38;5;241m1\u001b[39m], qkv[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     21\u001b[0m q \u001b[38;5;241m=\u001b[39m q \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\n\u001b[0;32m---> 22\u001b[0m attn \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m3\u001b[39m) \u001b[38;5;241m@\u001b[39m k[:,:,\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_idx\u001b[49m]\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m#B,nh,L,1,K^2\u001b[39;00m\n\u001b[1;32m     23\u001b[0m attn \u001b[38;5;241m=\u001b[39m attn \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelative_bias[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_idx]\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     24\u001b[0m attn \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Attention_qkv' object has no attribute 'attn_idx'"
     ]
    }
   ],
   "source": [
    "attn = Attention_qkv(768)\n",
    "q,k,v = attn(x)\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "130dae81-cc9f-444c-b66c-eaa9a3c38506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 12, 1, 64])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k[:,:,0].unsqueeze(2).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "de603821-43cf-47d4-998e-dac60bfdda63",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = q * (attn.dim)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "76c6e9e7-fc95-4944-85e2-dacaeffc6d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 12, 196, 64])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "a53fe9d1-0386-4240-bb61-e21cb26a7462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7,\n",
       "         0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7,\n",
       "         0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7]),\n",
       " tensor([ 0,  0,  0,  0,  0,  0,  0,  0, 14, 14, 14, 14, 14, 14, 14, 14, 28, 28,\n",
       "         28, 28, 28, 28, 28, 28, 42, 42, 42, 42, 42, 42, 42, 42, 56, 56, 56, 56,\n",
       "         56, 56, 56, 56, 70, 70, 70, 70, 70, 70, 70, 70, 84, 84, 84, 84, 84, 84,\n",
       "         84, 84, 98, 98, 98, 98, 98, 98, 98, 98]),\n",
       " tensor([  0,   1,   2,   3,   4,   5,   6,   7,  14,  15,  16,  17,  18,  19,\n",
       "          20,  21,  28,  29,  30,  31,  32,  33,  34,  35,  42,  43,  44,  45,\n",
       "          46,  47,  48,  49,  56,  57,  58,  59,  60,  61,  62,  63,  70,  71,\n",
       "          72,  73,  74,  75,  76,  77,  84,  85,  86,  87,  88,  89,  90,  91,\n",
       "          98,  99, 100, 101, 102, 103, 104, 105]))"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_size =7\n",
    "H,W = 14,14\n",
    "H_ = H-(window_size-1)\n",
    "W_ = W-(window_size-1)\n",
    "h_idx = torch.arange(W_).repeat(H_)\n",
    "w_idx = torch.arange(H_).repeat_interleave(W_) * W\n",
    "hw_idx = h_idx + w_idx\n",
    "(h_idx, w_idx, hw_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "3c444c3a-7e60-4b0c-98cf-d10ced945ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2,\n",
       "         3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5,\n",
       "         6]),\n",
       " tensor([ 0,  0,  0,  0,  0,  0,  0, 14, 14, 14, 14, 14, 14, 14, 28, 28, 28, 28,\n",
       "         28, 28, 28, 42, 42, 42, 42, 42, 42, 42, 56, 56, 56, 56, 56, 56, 56, 70,\n",
       "         70, 70, 70, 70, 70, 70, 84, 84, 84, 84, 84, 84, 84]),\n",
       " tensor([ 0,  1,  2,  3,  4,  5,  6, 14, 15, 16, 17, 18, 19, 20, 28, 29, 30, 31,\n",
       "         32, 33, 34, 42, 43, 44, 45, 46, 47, 48, 56, 57, 58, 59, 60, 61, 62, 70,\n",
       "         71, 72, 73, 74, 75, 76, 84, 85, 86, 87, 88, 89, 90]))"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_idx_1 = torch.arange(window_size).repeat(window_size)\n",
    "k_idx_2 = torch.arange(window_size).repeat_interleave(window_size) * W\n",
    "k_idx = k_idx_1 + k_idx_2\n",
    "(k_idx_1, k_idx_2, k_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "ccf4250d-42c1-4f2e-a426-920e5127e25e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   1,   2,  ...,  88,  89,  90],\n",
       "        [  1,   2,   3,  ...,  89,  90,  91],\n",
       "        [  2,   3,   4,  ...,  90,  91,  92],\n",
       "        ...,\n",
       "        [103, 104, 105,  ..., 191, 192, 193],\n",
       "        [104, 105, 106,  ..., 192, 193, 194],\n",
       "        [105, 106, 107,  ..., 193, 194, 195]])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unfold_idx = hw_idx.unsqueeze(1) + k_idx #hw_idx[:,None] \n",
    "unfold_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "f2c187bb-6d17-44ea-8f4c-1daa4a06b315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],\n",
       "          [ 8.,  9., 10., 11., 12., 13., 14., 15.],\n",
       "          [16., 17., 18., 19., 20., 21., 22., 23.],\n",
       "          [24., 25., 26., 27., 28., 29., 30., 31.],\n",
       "          [32., 33., 34., 35., 36., 37., 38., 39.],\n",
       "          [40., 41., 42., 43., 44., 45., 46., 47.],\n",
       "          [48., 49., 50., 51., 52., 53., 54., 55.],\n",
       "          [56., 57., 58., 59., 60., 61., 62., 63.]]]])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_idx = torch.arange(0,H_*W_).float().view(1,1,H_,W_)\n",
    "attn_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "0660ace5-aedd-4d43-9883-a135dac598ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReplicationPad2d((3, 3, 3, 3))"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_idx = nn.ReplicationPad2d(window_size//2)\n",
    "pad_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "f8a21305-fbeb-45ec-8a24-4dbdc912bce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  0,  0,  0,  1,  2,  3,  4,  5,  6,  7,  7,  7,  7,  0,  0,  0,  0,\n",
       "         1,  2,  3,  4,  5,  6,  7,  7,  7,  7,  0,  0,  0,  0,  1,  2,  3,  4,\n",
       "         5,  6,  7,  7,  7,  7,  0,  0,  0,  0,  1,  2,  3,  4,  5,  6,  7,  7,\n",
       "         7,  7,  8,  8,  8,  8,  9, 10, 11, 12, 13, 14, 15, 15, 15, 15, 16, 16,\n",
       "        16, 16, 17, 18, 19, 20, 21, 22, 23, 23, 23, 23, 24, 24, 24, 24, 25, 26,\n",
       "        27, 28, 29, 30, 31, 31, 31, 31, 32, 32, 32, 32, 33, 34, 35, 36, 37, 38,\n",
       "        39, 39, 39, 39, 40, 40, 40, 40, 41, 42, 43, 44, 45, 46, 47, 47, 47, 47,\n",
       "        48, 48, 48, 48, 49, 50, 51, 52, 53, 54, 55, 55, 55, 55, 56, 56, 56, 56,\n",
       "        57, 58, 59, 60, 61, 62, 63, 63, 63, 63, 56, 56, 56, 56, 57, 58, 59, 60,\n",
       "        61, 62, 63, 63, 63, 63, 56, 56, 56, 56, 57, 58, 59, 60, 61, 62, 63, 63,\n",
       "        63, 63, 56, 56, 56, 56, 57, 58, 59, 60, 61, 62, 63, 63, 63, 63])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_idx = pad_idx(attn_idx).view(-1).type(torch.long)\n",
    "attn_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "eec2631e-841a-4c13-920d-4eb19889c369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   1,   2,  ...,  88,  89,  90],\n",
       "        [  0,   1,   2,  ...,  88,  89,  90],\n",
       "        [  0,   1,   2,  ...,  88,  89,  90],\n",
       "        ...,\n",
       "        [105, 106, 107,  ..., 193, 194, 195],\n",
       "        [105, 106, 107,  ..., 193, 194, 195],\n",
       "        [105, 106, 107,  ..., 193, 194, 195]])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_idx  = unfold_idx[attn_idx]\n",
    "attn_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "eba6eea3-2966-4f62-be87-b87243d3bc74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  ..., 46, 47, 48],\n",
       "        [ 0,  1,  2,  ..., 46, 47, 48],\n",
       "        [ 0,  1,  2,  ..., 46, 47, 48],\n",
       "        ...,\n",
       "        [ 0,  1,  2,  ..., 46, 47, 48],\n",
       "        [ 0,  1,  2,  ..., 46, 47, 48],\n",
       "        [ 0,  1,  2,  ..., 46, 47, 48]])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias = torch.arange(window_size ** 2).repeat(H * W, 1)\n",
    "bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "35d97a0c-f0ca-4af8-8553-4750c74008a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([196, 49])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "0c943845-4824-4558-9f78-50f671bbfdff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 12, 196, 64])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "7fe01ee4-cf48-47b9-9419-cb542f9a1997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 3.9289e+00,  3.8185e+00,  3.1616e+00,  ...,  3.4523e+00,\n",
       "            3.2540e+00,  2.7054e+00],\n",
       "          [ 3.1262e+00,  2.8160e+00,  2.5354e+00,  ...,  2.6834e+00,\n",
       "            2.1798e+00,  1.7825e+00],\n",
       "          [ 3.0375e+00,  2.4665e+00,  2.0519e+00,  ...,  2.8424e+00,\n",
       "            2.4302e+00,  1.8020e+00],\n",
       "          ...,\n",
       "          [-8.9167e-01, -6.3551e-01, -5.2425e-01,  ...,  2.3182e+00,\n",
       "            2.8595e+00,  1.8840e+00],\n",
       "          [-1.4740e-01, -1.7072e-01,  8.0530e-02,  ...,  2.7605e+00,\n",
       "            3.2866e+00,  2.1529e+00],\n",
       "          [ 4.4728e-01,  3.8404e-01,  4.9118e-01,  ...,  3.6537e+00,\n",
       "            3.7063e+00,  2.6141e+00]],\n",
       "\n",
       "         [[-1.0858e+00, -1.1290e+00, -8.9149e-01,  ..., -1.1985e-01,\n",
       "            5.2028e-01, -1.1441e+00],\n",
       "          [-8.4863e-01, -7.7516e-01, -3.7013e-01,  ..., -5.9547e-01,\n",
       "            3.4185e-01, -1.5049e+00],\n",
       "          [-6.8205e-01, -5.9150e-01, -2.1612e-01,  ..., -7.3382e-01,\n",
       "            4.2068e-01, -1.6308e+00],\n",
       "          ...,\n",
       "          [ 4.6277e-01,  1.0012e-01,  5.3811e-02,  ..., -2.2028e-01,\n",
       "           -5.8960e-02, -1.0919e+00],\n",
       "          [ 9.8754e-01,  2.7692e-01, -1.9574e-02,  ...,  1.6725e-01,\n",
       "            3.4391e-01, -8.0498e-01],\n",
       "          [ 8.3890e-01,  1.5650e-01, -4.0051e-02,  ..., -2.3123e-01,\n",
       "            1.2384e-01, -5.6352e-01]],\n",
       "\n",
       "         [[-5.4187e-01, -1.3462e+00, -7.0599e-01,  ...,  9.0592e-01,\n",
       "            8.6558e-01,  2.6831e-01],\n",
       "          [ 5.2220e-01, -1.6919e-01,  1.8427e-01,  ...,  1.1430e+00,\n",
       "            1.0005e+00,  6.6987e-01],\n",
       "          [-3.4450e-02, -8.2587e-01, -4.5072e-01,  ...,  3.9704e-01,\n",
       "            6.1784e-01,  6.9312e-02],\n",
       "          ...,\n",
       "          [-3.7634e-01, -1.2758e+00, -1.0992e+00,  ...,  8.7941e-01,\n",
       "            1.3034e+00, -1.5584e-01],\n",
       "          [-1.8975e-01, -1.1608e+00, -1.2462e+00,  ...,  1.2702e+00,\n",
       "            1.5942e+00, -2.8005e-02],\n",
       "          [-9.7685e-02, -8.2429e-01, -9.9669e-01,  ...,  1.0162e+00,\n",
       "            1.1311e+00, -3.5094e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.8205e-01,  1.7290e-01,  1.3612e-01,  ...,  2.0296e-01,\n",
       "            5.1005e-01,  9.3950e-01],\n",
       "          [ 8.4734e-01,  8.8174e-01,  9.1463e-01,  ...,  2.7766e-01,\n",
       "            4.1097e-01,  5.7777e-01],\n",
       "          [ 1.8092e+00,  1.7476e+00,  1.6311e+00,  ...,  7.0084e-01,\n",
       "            1.0312e+00,  1.6157e+00],\n",
       "          ...,\n",
       "          [-3.7287e-01, -4.8576e-01, -4.6910e-01,  ..., -3.0932e+00,\n",
       "           -3.0225e+00, -2.0794e+00],\n",
       "          [ 2.7952e-01, -9.7697e-02, -3.1965e-01,  ..., -2.9236e+00,\n",
       "           -2.4921e+00, -1.6953e+00],\n",
       "          [ 8.8112e-01,  3.8255e-01, -9.2502e-02,  ..., -1.9694e+00,\n",
       "           -1.6853e+00, -1.2312e+00]],\n",
       "\n",
       "         [[-9.2883e-01, -5.3713e-01, -7.5395e-01,  ..., -1.8150e+00,\n",
       "           -1.4395e+00, -3.6336e-02],\n",
       "          [-1.3336e+00, -9.7757e-01, -9.9287e-01,  ..., -1.5615e+00,\n",
       "           -9.6714e-01,  4.7948e-01],\n",
       "          [-6.3273e-01, -4.7346e-01, -2.7000e-01,  ..., -1.5554e+00,\n",
       "           -1.1571e+00,  4.1841e-01],\n",
       "          ...,\n",
       "          [-2.2456e+00, -1.8709e+00, -1.9076e+00,  ..., -1.9178e+00,\n",
       "           -1.3325e+00, -1.4043e+00],\n",
       "          [-1.6115e+00, -1.1306e+00, -1.4988e+00,  ..., -1.3902e+00,\n",
       "           -1.0333e+00, -8.9289e-01],\n",
       "          [-9.3543e-01, -7.1416e-01, -1.0100e+00,  ..., -7.0763e-01,\n",
       "           -5.2857e-01, -4.6830e-01]],\n",
       "\n",
       "         [[ 1.5151e+00,  7.6441e-01,  1.3802e+00,  ...,  2.1782e+00,\n",
       "            1.7621e+00,  1.5417e+00],\n",
       "          [ 1.1225e+00,  6.3490e-01,  1.3644e+00,  ...,  1.7423e+00,\n",
       "            1.3734e+00,  1.2160e+00],\n",
       "          [ 9.4100e-01,  4.7015e-01,  1.5050e+00,  ...,  1.6781e+00,\n",
       "            1.1497e+00,  9.9281e-01],\n",
       "          ...,\n",
       "          [-1.3181e+00, -5.2350e-01,  1.6112e-02,  ..., -5.7984e-01,\n",
       "           -8.9529e-01, -6.7069e-02],\n",
       "          [-1.2167e+00, -7.8490e-01, -3.3001e-02,  ..., -1.5876e-01,\n",
       "           -6.4668e-01,  3.5933e-02],\n",
       "          [-1.4849e+00, -1.1203e+00, -2.2325e-01,  ...,  1.1180e+00,\n",
       "            9.2678e-01,  1.1750e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 4.0782e+00,  4.0453e+00,  2.3735e+00,  ...,  4.0787e+00,\n",
       "            4.3027e+00,  3.0229e+00],\n",
       "          [ 3.0932e+00,  3.3446e+00,  1.9234e+00,  ...,  3.2473e+00,\n",
       "            3.5911e+00,  2.6397e+00],\n",
       "          [ 3.5834e+00,  3.9751e+00,  2.2575e+00,  ...,  2.8076e+00,\n",
       "            2.9410e+00,  2.2099e+00],\n",
       "          ...,\n",
       "          [-1.9182e-01,  5.9605e-01,  4.7055e-02,  ...,  2.4506e+00,\n",
       "            2.4256e+00,  2.1616e+00],\n",
       "          [-3.1943e-01,  5.5762e-02, -2.2995e-01,  ...,  2.5847e+00,\n",
       "            2.8040e+00,  2.4634e+00],\n",
       "          [ 2.3884e-01,  7.4071e-01, -8.8805e-02,  ...,  2.8066e+00,\n",
       "            3.1465e+00,  2.6279e+00]],\n",
       "\n",
       "         [[-7.8500e-01, -6.2537e-01, -8.6818e-01,  ..., -3.5640e-01,\n",
       "            1.4484e-01, -7.7890e-01],\n",
       "          [-5.3968e-01, -6.8616e-01, -5.2288e-01,  ..., -8.5566e-01,\n",
       "           -5.0295e-01, -1.3996e+00],\n",
       "          [-3.8152e-01, -7.2650e-01, -5.5373e-01,  ..., -1.6754e-01,\n",
       "            5.0345e-01, -4.8005e-01],\n",
       "          ...,\n",
       "          [ 1.6941e-01,  2.4287e-01,  5.2466e-02,  ..., -1.1266e-01,\n",
       "           -2.6534e-01, -1.4046e+00],\n",
       "          [ 5.4679e-01,  3.9951e-01,  4.5189e-02,  ...,  3.1905e-01,\n",
       "            5.5340e-02, -1.0163e+00],\n",
       "          [ 4.0578e-01,  3.2279e-01, -1.0323e-01,  ...,  5.7008e-01,\n",
       "            4.4983e-01, -2.9376e-01]],\n",
       "\n",
       "         [[-1.4637e+00, -7.0790e-01, -6.3123e-01,  ...,  1.0410e+00,\n",
       "            2.8651e-01,  7.9162e-01],\n",
       "          [-9.2451e-01, -1.7759e-01, -2.3751e-01,  ...,  1.5174e+00,\n",
       "            5.7110e-01,  9.4737e-01],\n",
       "          [-1.7486e+00, -1.3868e+00, -1.6246e+00,  ...,  2.0958e-01,\n",
       "           -3.9349e-02,  1.8368e-01],\n",
       "          ...,\n",
       "          [-6.5939e-01, -6.4078e-01, -1.7403e+00,  ...,  1.8007e+00,\n",
       "            1.4410e+00,  1.2858e+00],\n",
       "          [-7.9688e-01, -7.6684e-01, -1.8753e+00,  ...,  9.2205e-01,\n",
       "            4.1629e-01,  2.6736e-01],\n",
       "          [-6.7162e-01, -1.0343e+00, -2.1267e+00,  ...,  2.5204e-01,\n",
       "            6.0226e-02,  9.8395e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.5584e+00,  1.0835e+00,  6.1536e-01,  ...,  7.0306e-01,\n",
       "            1.1744e+00,  5.7868e-01],\n",
       "          [ 1.9911e+00,  1.6523e+00,  1.1407e+00,  ..., -1.2634e-01,\n",
       "            4.1906e-01,  5.3062e-02],\n",
       "          [ 1.8253e+00,  1.7340e+00,  1.1162e+00,  ..., -2.8918e-01,\n",
       "            6.7888e-01,  5.5987e-01],\n",
       "          ...,\n",
       "          [ 4.5948e-01, -2.7167e-01, -4.5016e-01,  ..., -3.6458e+00,\n",
       "           -3.6001e+00, -3.3823e+00],\n",
       "          [ 8.9367e-01,  1.9392e-01, -4.4272e-01,  ..., -2.0958e+00,\n",
       "           -1.9309e+00, -1.8880e+00],\n",
       "          [ 1.1880e+00,  5.5494e-01, -1.3342e-01,  ..., -1.5576e+00,\n",
       "           -1.5314e+00, -1.5445e+00]],\n",
       "\n",
       "         [[-8.4100e-01, -6.6940e-01,  5.1982e-02,  ..., -1.3937e+00,\n",
       "           -8.2264e-01, -1.4102e-01],\n",
       "          [-1.6798e+00, -1.3853e+00, -8.4074e-01,  ..., -1.3863e+00,\n",
       "           -8.1564e-01,  1.6179e-01],\n",
       "          [-1.1549e+00, -7.9373e-01,  8.3996e-02,  ..., -1.1812e+00,\n",
       "           -5.5937e-01,  7.0478e-01],\n",
       "          ...,\n",
       "          [-2.2881e+00, -1.8731e+00, -1.5584e+00,  ..., -6.2729e-01,\n",
       "           -8.3301e-01, -7.8080e-01],\n",
       "          [-2.2214e+00, -1.8201e+00, -1.4110e+00,  ..., -1.0259e+00,\n",
       "           -1.2163e+00, -9.8345e-01],\n",
       "          [-5.7252e-01, -1.2426e-01, -2.6963e-01,  ..., -1.1131e+00,\n",
       "           -9.0284e-01, -5.3529e-01]],\n",
       "\n",
       "         [[ 1.8546e+00,  1.2415e+00,  1.8877e+00,  ...,  1.5547e+00,\n",
       "            1.2760e+00,  9.6896e-01],\n",
       "          [ 1.3499e+00,  9.6205e-01,  1.8422e+00,  ...,  9.9309e-01,\n",
       "            1.0936e+00,  7.5983e-01],\n",
       "          [ 1.5541e+00,  1.3373e+00,  1.9511e+00,  ...,  1.2096e+00,\n",
       "            1.3229e+00,  1.0737e+00],\n",
       "          ...,\n",
       "          [-1.4558e+00, -9.8842e-01,  1.8228e-01,  ..., -9.6886e-01,\n",
       "           -8.3392e-01, -1.2296e+00],\n",
       "          [-1.8493e+00, -1.4654e+00, -1.6512e-01,  ..., -2.4800e-01,\n",
       "            1.6754e-01, -6.1558e-01],\n",
       "          [-1.2817e+00, -1.0659e+00,  2.1713e-01,  ..., -2.8394e-02,\n",
       "            5.0875e-01, -9.3990e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 3.5305e+00,  2.6432e+00,  1.9661e+00,  ...,  2.6063e+00,\n",
       "            2.4330e+00,  2.6605e+00],\n",
       "          [ 3.5695e+00,  2.3854e+00,  1.3914e+00,  ...,  2.4249e+00,\n",
       "            2.8322e+00,  3.1205e+00],\n",
       "          [ 3.5330e+00,  2.5661e+00,  1.8003e+00,  ...,  1.5307e+00,\n",
       "            2.0212e+00,  2.0970e+00],\n",
       "          ...,\n",
       "          [-5.7779e-01, -5.7746e-01, -4.4140e-01,  ...,  1.8148e+00,\n",
       "            2.0217e+00,  2.2027e+00],\n",
       "          [-9.0508e-02, -2.7431e-01, -2.1162e-01,  ...,  1.7168e+00,\n",
       "            2.1499e+00,  2.2093e+00],\n",
       "          [ 3.3558e-01,  1.8189e-01,  3.7751e-01,  ...,  2.4357e+00,\n",
       "            2.9418e+00,  2.7986e+00]],\n",
       "\n",
       "         [[ 5.1829e-01, -4.5212e-02,  3.1303e-01,  ..., -3.6743e-01,\n",
       "           -6.7726e-01, -9.8573e-01],\n",
       "          [-5.6264e-01, -9.8504e-01, -4.2052e-01,  ..., -1.0457e+00,\n",
       "           -1.1711e+00, -1.3454e+00],\n",
       "          [-1.0203e+00, -1.4727e+00, -8.1813e-01,  ..., -2.0285e-01,\n",
       "           -2.5267e-03, -4.2033e-02],\n",
       "          ...,\n",
       "          [-5.3271e-01, -1.1044e+00, -6.0973e-01,  ..., -2.1716e-01,\n",
       "           -4.8651e-01, -7.3896e-01],\n",
       "          [ 1.2603e-01, -3.7560e-01,  1.0710e-01,  ...,  2.5176e-02,\n",
       "           -3.0018e-01, -2.4931e-01],\n",
       "          [ 4.0713e-01, -2.4868e-01, -7.1724e-02,  ...,  1.6052e+00,\n",
       "            1.2405e+00,  1.3135e+00]],\n",
       "\n",
       "         [[-1.6136e+00, -1.4144e+00, -1.0392e+00,  ...,  4.7559e-01,\n",
       "            2.7915e-01, -4.1477e-01],\n",
       "          [-6.9641e-01, -3.2811e-01, -1.9910e-01,  ...,  1.4187e+00,\n",
       "            1.4900e+00,  5.0217e-01],\n",
       "          [-1.9306e+00, -1.9138e+00, -1.5728e+00,  ...,  2.6441e-01,\n",
       "            9.1210e-01,  4.5157e-02],\n",
       "          ...,\n",
       "          [ 2.5575e-01,  4.6853e-01, -8.4211e-01,  ...,  1.0880e+00,\n",
       "            1.2376e+00,  1.2720e+00],\n",
       "          [-5.2510e-01, -5.7626e-01, -1.8649e+00,  ...,  2.7097e-01,\n",
       "            4.8068e-01,  3.1488e-01],\n",
       "          [-3.1269e-01, -3.9994e-01, -1.7447e+00,  ...,  8.4246e-01,\n",
       "            6.8540e-01,  5.1834e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.1808e-01,  4.9183e-01,  1.2857e+00,  ...,  1.4741e+00,\n",
       "            1.4090e+00,  1.1464e+00],\n",
       "          [ 8.3652e-01,  1.1716e+00,  1.7337e+00,  ...,  1.2232e+00,\n",
       "            1.2729e+00,  1.0204e+00],\n",
       "          [ 3.3666e-01,  8.0801e-01,  1.4215e+00,  ...,  7.3086e-01,\n",
       "            1.0548e+00,  9.2835e-01],\n",
       "          ...,\n",
       "          [-6.6248e-01, -5.5577e-01,  7.2875e-01,  ..., -2.7173e+00,\n",
       "           -2.4794e+00, -2.0812e+00],\n",
       "          [-6.5281e-01, -5.9747e-01,  5.3494e-01,  ..., -2.0749e+00,\n",
       "           -2.4234e+00, -1.9184e+00],\n",
       "          [ 1.5026e-01,  9.2861e-02,  1.0781e+00,  ..., -1.2161e+00,\n",
       "           -1.8970e+00, -1.1631e+00]],\n",
       "\n",
       "         [[ 2.5885e-01, -3.1175e-02,  2.4916e-01,  ..., -6.9341e-01,\n",
       "            2.5445e-01,  6.4857e-01],\n",
       "          [-4.4820e-01, -6.8643e-01, -8.8649e-02,  ..., -1.4259e+00,\n",
       "           -5.2825e-01,  1.2602e-01],\n",
       "          [-1.1636e+00, -1.3461e+00, -4.4505e-01,  ..., -1.6425e+00,\n",
       "           -8.3633e-01, -2.4403e-02],\n",
       "          ...,\n",
       "          [-1.4363e+00, -1.5461e+00, -1.8369e+00,  ..., -1.3142e+00,\n",
       "           -7.0747e-01, -1.0138e+00],\n",
       "          [-1.5776e+00, -1.8187e+00, -2.2891e+00,  ..., -5.9843e-01,\n",
       "           -1.7870e-01, -7.9952e-01],\n",
       "          [-5.6473e-01, -1.2105e+00, -1.8029e+00,  ..., -9.4871e-01,\n",
       "           -5.3523e-01, -1.3597e+00]],\n",
       "\n",
       "         [[ 1.7937e+00,  1.4548e+00,  1.2794e+00,  ...,  1.9021e+00,\n",
       "            1.3481e+00,  1.6789e+00],\n",
       "          [ 1.7208e+00,  1.4964e+00,  1.4441e+00,  ...,  2.5431e+00,\n",
       "            2.0633e+00,  1.9937e+00],\n",
       "          [ 1.0050e+00,  1.2639e+00,  1.0228e+00,  ...,  1.4778e+00,\n",
       "            1.0760e+00,  1.1080e+00],\n",
       "          ...,\n",
       "          [-1.5481e+00, -1.1511e+00, -7.9250e-01,  ..., -1.3228e+00,\n",
       "           -1.3000e+00, -5.3462e-01],\n",
       "          [-1.6047e+00, -1.0312e+00, -6.2105e-01,  ..., -1.2383e-01,\n",
       "            1.4298e-01,  4.7890e-01],\n",
       "          [-1.1742e+00, -5.5910e-01, -1.6623e-01,  ...,  7.1788e-01,\n",
       "            1.0128e+00,  1.4690e+00]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 3.6774e+00,  2.8635e+00,  2.9274e+00,  ...,  3.8800e+00,\n",
       "            3.2686e+00,  3.5618e+00],\n",
       "          [ 4.1945e+00,  3.2827e+00,  3.1957e+00,  ...,  3.9553e+00,\n",
       "            3.2938e+00,  3.9639e+00],\n",
       "          [ 4.1177e+00,  3.0436e+00,  3.2333e+00,  ...,  3.4638e+00,\n",
       "            2.8011e+00,  3.4573e+00],\n",
       "          ...,\n",
       "          [-4.0676e-01, -4.4653e-01,  1.6214e-03,  ...,  2.9831e+00,\n",
       "            2.7222e+00,  2.7032e+00],\n",
       "          [ 6.7293e-01,  5.6639e-01,  1.1048e+00,  ...,  3.2366e+00,\n",
       "            2.9357e+00,  3.2277e+00],\n",
       "          [ 2.3885e-01, -5.8019e-02,  4.7869e-01,  ...,  2.6565e+00,\n",
       "            2.3205e+00,  2.5164e+00]],\n",
       "\n",
       "         [[-7.8809e-01, -1.6215e+00, -1.3758e+00,  ..., -1.4505e+00,\n",
       "           -4.7940e-01, -1.2439e+00],\n",
       "          [-8.8335e-01, -1.6222e+00, -1.4845e+00,  ..., -3.0344e-01,\n",
       "            5.4727e-01, -6.3372e-02],\n",
       "          [-1.1214e+00, -1.4900e+00, -1.1624e+00,  ..., -4.9836e-01,\n",
       "            4.4353e-01,  4.8943e-03],\n",
       "          ...,\n",
       "          [-1.2237e-01,  5.7602e-01,  5.8763e-01,  ...,  3.2559e-01,\n",
       "            4.6965e-01, -2.8716e-01],\n",
       "          [-1.0273e-01,  1.3599e-01,  2.6785e-01,  ...,  8.0263e-01,\n",
       "            1.0958e+00,  2.6513e-01],\n",
       "          [-4.3470e-02,  2.5770e-01,  2.9663e-01,  ...,  3.7147e-01,\n",
       "            1.1424e+00,  4.4939e-01]],\n",
       "\n",
       "         [[-1.6909e+00, -1.3762e+00, -1.9250e+00,  ...,  1.6198e-01,\n",
       "            4.6817e-01,  2.5658e-01],\n",
       "          [-6.7183e-01, -7.7170e-01, -1.4525e+00,  ...,  8.7773e-01,\n",
       "            1.4041e+00,  9.7788e-01],\n",
       "          [-9.6830e-01, -9.1270e-01, -1.4447e+00,  ...,  8.2997e-01,\n",
       "            1.4493e+00,  1.0385e+00],\n",
       "          ...,\n",
       "          [-8.2026e-01, -7.2252e-01, -1.1192e+00,  ...,  1.7438e+00,\n",
       "            1.6676e+00,  9.8943e-01],\n",
       "          [-8.0416e-01, -4.8150e-01, -1.1909e+00,  ...,  1.5099e+00,\n",
       "            1.4631e+00,  6.4276e-01],\n",
       "          [-1.1962e+00, -6.3222e-01, -1.5939e+00,  ...,  7.4927e-01,\n",
       "            7.7752e-01, -3.9468e-03]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.6778e+00,  1.3055e+00,  1.4366e+00,  ...,  1.4263e+00,\n",
       "            1.6268e+00,  9.2774e-01],\n",
       "          [ 1.3091e+00,  1.1855e+00,  1.2804e+00,  ...,  1.1136e+00,\n",
       "            1.3391e+00,  1.0438e+00],\n",
       "          [ 1.6517e+00,  1.4786e+00,  1.8654e+00,  ...,  6.7071e-01,\n",
       "            9.2373e-01,  6.6430e-01],\n",
       "          ...,\n",
       "          [-5.1398e-01, -1.2393e-01, -7.9912e-01,  ..., -3.2659e+00,\n",
       "           -2.6897e+00, -2.2401e+00],\n",
       "          [ 8.2643e-02,  8.5559e-02, -5.0719e-01,  ..., -3.0811e+00,\n",
       "           -2.5202e+00, -2.2451e+00],\n",
       "          [ 1.4812e+00,  9.7782e-01, -4.1545e-02,  ..., -2.0924e+00,\n",
       "           -1.7701e+00, -1.3854e+00]],\n",
       "\n",
       "         [[-3.4286e-01, -1.2566e+00, -7.9069e-01,  ..., -1.3179e+00,\n",
       "           -2.8336e-01,  1.2625e+00],\n",
       "          [ 3.6552e-02, -7.4490e-01, -1.6157e-01,  ..., -1.5215e+00,\n",
       "           -4.6439e-01,  9.5052e-01],\n",
       "          [ 4.6585e-01, -3.7653e-01, -2.3929e-01,  ..., -1.2704e+00,\n",
       "           -4.0837e-01,  8.3365e-01],\n",
       "          ...,\n",
       "          [-1.9893e+00, -2.8497e+00, -2.3889e+00,  ..., -1.7237e+00,\n",
       "           -1.7178e+00, -1.4118e+00],\n",
       "          [-3.6557e-01, -1.6272e+00, -1.5885e+00,  ..., -8.4061e-01,\n",
       "           -1.0097e+00, -3.3368e-01],\n",
       "          [-4.6413e-01, -2.2176e+00, -2.0108e+00,  ..., -1.0036e+00,\n",
       "           -1.2179e+00, -4.2600e-01]],\n",
       "\n",
       "         [[ 1.1020e+00,  1.3923e+00,  1.9885e+00,  ...,  2.1441e+00,\n",
       "            2.1416e+00,  1.4940e+00],\n",
       "          [ 1.4765e+00,  1.7253e+00,  2.2180e+00,  ...,  1.6860e+00,\n",
       "            1.4638e+00,  1.2767e+00],\n",
       "          [ 7.0113e-01,  1.0741e+00,  1.8164e+00,  ...,  1.6887e+00,\n",
       "            1.1057e+00,  1.1577e+00],\n",
       "          ...,\n",
       "          [-1.8766e+00, -1.0878e+00, -2.0970e-01,  ..., -8.8479e-01,\n",
       "           -1.0974e+00, -1.3044e+00],\n",
       "          [-1.6196e+00, -1.1246e+00, -1.8641e-01,  ..., -1.6757e-01,\n",
       "           -2.0445e-01, -2.3622e-01],\n",
       "          [-1.4060e+00, -9.3024e-01,  1.1509e-01,  ..., -1.0641e-01,\n",
       "           -3.6080e-01, -4.0081e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 2.2611e+00,  2.6086e+00,  1.8149e+00,  ...,  3.6205e+00,\n",
       "            3.0924e+00,  3.6632e+00],\n",
       "          [ 2.6064e+00,  2.6976e+00,  2.0459e+00,  ...,  3.1094e+00,\n",
       "            2.6204e+00,  3.2729e+00],\n",
       "          [ 2.7752e+00,  2.7598e+00,  1.9037e+00,  ...,  2.8899e+00,\n",
       "            2.5929e+00,  2.9865e+00],\n",
       "          ...,\n",
       "          [-8.8412e-01, -1.1909e-01, -3.9903e-02,  ...,  2.5086e+00,\n",
       "            2.6450e+00,  2.9573e+00],\n",
       "          [-1.0033e+00, -4.6876e-02, -1.4165e-01,  ...,  2.9240e+00,\n",
       "            2.9637e+00,  3.4826e+00],\n",
       "          [ 3.2430e-01,  1.1915e+00,  1.0959e+00,  ...,  4.1622e+00,\n",
       "            3.9745e+00,  4.4891e+00]],\n",
       "\n",
       "         [[-1.8017e+00, -1.2936e+00, -1.5344e+00,  ...,  1.7927e-01,\n",
       "            4.9870e-01,  7.9993e-02],\n",
       "          [-1.9264e+00, -1.7457e+00, -1.5722e+00,  ...,  4.0051e-02,\n",
       "            2.9480e-01, -7.5220e-02],\n",
       "          [-8.9578e-01, -8.9127e-01, -7.6209e-01,  ...,  8.8336e-01,\n",
       "            9.1570e-01,  6.3126e-01],\n",
       "          ...,\n",
       "          [-4.4331e-01, -4.2852e-01,  1.0379e-01,  ...,  1.0116e-01,\n",
       "           -3.0064e-01, -1.0069e+00],\n",
       "          [ 2.3307e-01,  1.9601e-03,  4.7686e-01,  ...,  9.6601e-01,\n",
       "            4.5872e-01, -4.8489e-01],\n",
       "          [ 3.4226e-01,  2.8734e-01,  7.9105e-01,  ...,  1.8302e+00,\n",
       "            1.2800e+00,  5.5256e-01]],\n",
       "\n",
       "         [[-2.3189e+00, -1.8730e+00, -1.9157e+00,  ..., -5.2901e-01,\n",
       "            4.2721e-01, -2.3562e-01],\n",
       "          [-1.4820e+00, -9.7787e-01, -1.2214e+00,  ...,  2.6169e-01,\n",
       "            7.1993e-01,  5.7624e-01],\n",
       "          [-2.0940e+00, -1.7480e+00, -1.6541e+00,  ...,  3.5769e-01,\n",
       "            1.0359e+00,  9.2796e-01],\n",
       "          ...,\n",
       "          [-1.0576e+00, -7.0670e-01, -6.8959e-01,  ...,  8.6032e-01,\n",
       "            1.8109e+00,  7.6688e-01],\n",
       "          [-1.8359e+00, -1.3917e+00, -1.4382e+00,  ...,  7.9617e-01,\n",
       "            1.5143e+00,  5.7496e-01],\n",
       "          [-9.2893e-01, -5.8735e-01, -8.7269e-01,  ...,  4.5065e-01,\n",
       "            1.4219e+00,  4.7689e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.1309e+00,  8.1404e-01,  1.0833e+00,  ...,  7.7271e-01,\n",
       "            1.7786e+00,  1.3985e+00],\n",
       "          [ 1.0991e+00,  8.1555e-01,  1.1536e+00,  ...,  7.0439e-01,\n",
       "            1.2461e+00,  1.3232e+00],\n",
       "          [ 9.9112e-01,  8.3626e-01,  1.3423e+00,  ..., -4.7528e-03,\n",
       "            4.0875e-01,  5.3090e-01],\n",
       "          ...,\n",
       "          [-1.1057e-01,  1.3637e-01,  2.9292e-03,  ..., -2.9394e+00,\n",
       "           -2.2505e+00, -1.8538e+00],\n",
       "          [-4.8051e-02,  2.0217e-01, -3.8388e-01,  ..., -2.5752e+00,\n",
       "           -2.0933e+00, -1.4646e+00],\n",
       "          [ 1.3056e+00,  1.0525e+00,  3.9436e-01,  ..., -1.6716e+00,\n",
       "           -1.5796e+00, -9.9754e-01]],\n",
       "\n",
       "         [[ 7.0831e-01,  3.7287e-01, -5.9019e-02,  ..., -1.5581e+00,\n",
       "           -2.4816e-02,  1.5205e-01],\n",
       "          [ 3.8887e-01,  1.9151e-01, -1.5772e-01,  ..., -2.3531e+00,\n",
       "           -5.0650e-01, -5.1560e-01],\n",
       "          [-6.3848e-01, -5.5069e-01, -7.0611e-01,  ..., -2.2496e+00,\n",
       "           -5.2299e-01, -5.1604e-01],\n",
       "          ...,\n",
       "          [-1.6548e+00, -7.5129e-01, -1.2886e+00,  ..., -1.4217e+00,\n",
       "           -9.4215e-01, -5.2721e-01],\n",
       "          [-1.9590e+00, -1.5004e+00, -1.8992e+00,  ..., -1.4543e+00,\n",
       "           -9.4754e-01, -7.1198e-01],\n",
       "          [-1.1324e+00, -1.0359e+00, -1.7502e+00,  ..., -1.4575e+00,\n",
       "           -8.7061e-01, -5.8428e-01]],\n",
       "\n",
       "         [[ 1.1403e+00,  1.1911e+00,  1.8693e+00,  ...,  2.0759e+00,\n",
       "            1.2078e+00,  9.2366e-01],\n",
       "          [ 1.2713e+00,  1.5324e+00,  2.4078e+00,  ...,  2.0223e+00,\n",
       "            1.2407e+00,  1.1705e+00],\n",
       "          [ 3.3608e-01,  8.1647e-01,  1.5382e+00,  ...,  1.5006e+00,\n",
       "            8.4487e-01,  4.1622e-01],\n",
       "          ...,\n",
       "          [-1.8112e+00, -9.1161e-01, -2.2103e-01,  ..., -6.0770e-01,\n",
       "           -8.7447e-01, -8.0624e-01],\n",
       "          [-1.3079e+00, -3.3894e-01,  3.5023e-01,  ..., -4.8592e-01,\n",
       "           -4.5413e-01, -5.6743e-01],\n",
       "          [-1.2605e+00, -1.6868e-01,  4.8143e-01,  ...,  4.9025e-01,\n",
       "            5.8335e-01,  5.9969e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 4.7001e+00,  4.6792e+00,  3.3279e+00,  ...,  4.1737e+00,\n",
       "            4.6028e+00,  3.4434e+00],\n",
       "          [ 3.1354e+00,  3.2494e+00,  2.0765e+00,  ...,  2.6957e+00,\n",
       "            3.0106e+00,  2.1119e+00],\n",
       "          [ 2.8017e+00,  2.9087e+00,  1.8853e+00,  ...,  2.1742e+00,\n",
       "            2.7278e+00,  1.8929e+00],\n",
       "          ...,\n",
       "          [-4.6338e-01, -5.8479e-02, -3.3775e-01,  ...,  2.9386e+00,\n",
       "            2.5282e+00,  2.3290e+00],\n",
       "          [-1.6770e-01,  6.3988e-01,  6.9831e-01,  ...,  3.4990e+00,\n",
       "            2.8842e+00,  2.9309e+00],\n",
       "          [ 1.0666e+00,  1.2414e+00,  1.0531e+00,  ...,  3.6772e+00,\n",
       "            3.1599e+00,  2.8550e+00]],\n",
       "\n",
       "         [[-1.2805e+00, -1.8855e+00, -1.8755e+00,  ..., -6.1672e-01,\n",
       "           -1.6107e+00, -1.7573e+00],\n",
       "          [-8.2788e-01, -1.1608e+00, -1.2036e+00,  ..., -4.1485e-01,\n",
       "           -1.4348e+00, -1.8162e+00],\n",
       "          [-4.1253e-01, -1.1636e+00, -9.9537e-01,  ...,  9.8993e-02,\n",
       "           -6.6452e-01, -1.1217e+00],\n",
       "          ...,\n",
       "          [-2.0320e-01, -7.2209e-01, -1.0710e-01,  ...,  3.6811e-03,\n",
       "           -5.2098e-01, -8.3630e-01],\n",
       "          [ 1.2072e-01, -2.9818e-01, -1.0904e-01,  ..., -2.6607e-01,\n",
       "           -4.6865e-01, -8.9070e-01],\n",
       "          [ 4.0095e-01, -1.9501e-01,  5.9360e-02,  ...,  6.6974e-01,\n",
       "            4.2744e-01, -3.4168e-01]],\n",
       "\n",
       "         [[-1.3834e+00, -1.5871e+00, -2.0164e+00,  ...,  3.0370e-01,\n",
       "            1.0523e+00,  3.1233e-01],\n",
       "          [-5.4452e-01, -4.9526e-01, -8.4944e-01,  ...,  4.7080e-01,\n",
       "            1.4824e+00,  8.5745e-01],\n",
       "          [-1.3642e+00, -1.4003e+00, -1.8258e+00,  ..., -9.9239e-02,\n",
       "            1.0435e+00,  4.4490e-01],\n",
       "          ...,\n",
       "          [-8.1553e-02,  9.5293e-02, -5.7552e-01,  ...,  2.0144e+00,\n",
       "            1.5057e+00,  1.4026e+00],\n",
       "          [-1.2029e+00, -1.0618e+00, -2.2840e+00,  ...,  1.5966e+00,\n",
       "            1.0194e+00,  1.0810e+00],\n",
       "          [-1.8605e-01, -2.1541e-01, -1.3577e+00,  ...,  1.3123e+00,\n",
       "            8.6598e-01,  5.6346e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.5246e+00,  1.7257e+00,  1.6995e+00,  ...,  1.7316e+00,\n",
       "            1.2502e+00,  9.5666e-01],\n",
       "          [ 1.8830e+00,  1.9230e+00,  1.7814e+00,  ...,  9.6111e-01,\n",
       "            9.6500e-01,  9.5550e-01],\n",
       "          [ 1.0423e+00,  1.4008e+00,  1.2283e+00,  ...,  1.1865e+00,\n",
       "            7.9217e-01,  4.9445e-01],\n",
       "          ...,\n",
       "          [-6.8015e-01,  1.3009e-01, -1.0005e+00,  ..., -3.3576e+00,\n",
       "           -3.1587e+00, -2.8634e+00],\n",
       "          [-5.2455e-01,  3.8172e-01, -5.8560e-01,  ..., -2.6149e+00,\n",
       "           -2.4010e+00, -2.0193e+00],\n",
       "          [-1.0788e-01,  9.0084e-01, -1.3144e-01,  ..., -1.1761e+00,\n",
       "           -1.1709e+00, -9.1272e-01]],\n",
       "\n",
       "         [[-1.2925e+00, -7.8084e-01, -6.0022e-01,  ..., -2.1750e+00,\n",
       "           -9.3824e-01, -3.3303e-01],\n",
       "          [-2.5041e-01,  2.7546e-01,  3.8655e-01,  ..., -1.0713e+00,\n",
       "            1.3382e-01,  5.3891e-01],\n",
       "          [-1.2126e+00, -6.6834e-01, -4.1031e-01,  ..., -2.3639e+00,\n",
       "           -9.4406e-01, -4.8943e-01],\n",
       "          ...,\n",
       "          [-2.0476e+00, -1.5097e+00, -1.7231e+00,  ..., -1.6096e+00,\n",
       "           -1.5171e-01, -1.4217e+00],\n",
       "          [-1.1849e+00, -6.0524e-01, -1.2647e+00,  ..., -1.3782e+00,\n",
       "           -3.0890e-01, -1.3997e+00],\n",
       "          [-1.1630e+00, -6.9097e-01, -1.5781e+00,  ..., -1.2121e+00,\n",
       "           -2.0908e-01, -9.8274e-01]],\n",
       "\n",
       "         [[ 1.0117e+00,  9.4210e-01,  1.0027e+00,  ...,  8.1081e-01,\n",
       "            8.3867e-01,  3.6752e-01],\n",
       "          [ 6.4592e-01,  8.0578e-01,  5.9646e-01,  ...,  7.7721e-01,\n",
       "            7.5895e-01,  4.1958e-01],\n",
       "          [ 8.6035e-01,  8.8333e-01,  1.0712e+00,  ...,  9.7595e-01,\n",
       "            9.0806e-01,  3.5617e-01],\n",
       "          ...,\n",
       "          [-1.4753e+00, -3.1256e-01, -8.6206e-01,  ..., -1.2625e+00,\n",
       "           -1.2260e+00, -1.7579e+00],\n",
       "          [-8.5842e-01,  6.2799e-01, -2.8665e-02,  ...,  1.7266e-02,\n",
       "            1.8754e-01, -1.5673e-01],\n",
       "          [-1.4105e+00, -5.1684e-01, -6.5744e-01,  ...,  5.4584e-03,\n",
       "            2.3236e-01, -5.8617e-01]]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k[:,:,attn_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "53b25a2b-3cf5-43eb-aa41-7d8900d7f73f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 12, 196, 64, 49])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k[:,:,attn_idx].transpose(-1,-2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "28e6f1f7-02be-4f07-b8d6-d94ab86bde8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 12, 196, 49, 64])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k[:,:,attn_idx].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "984c9157-7c57-4c28-83cf-8897625a3c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 12, 196, 1, 64])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.unsqueeze(3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "7132324f-996b-44c2-a16f-ad236f4c815f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 12, 196, 1, 49])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = q.unsqueeze(3) @ k[:,:,attn_idx].transpose(-1,-2)\n",
    "attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "3b92ab36-4f06-4711-aee1-3248faf43fb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 1, 1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1, 1]))"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_repeat_h = torch.ones(window_size,dtype=torch.long)\n",
    "num_repeat_w = torch.ones(window_size,dtype=torch.long)\n",
    "num_repeat_h, num_repeat_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "646e21e8-d210-4118-8be5-f1dd0cf29f15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 1, 1, 8, 1, 1, 1]), tensor([1, 1, 1, 8, 1, 1, 1]))"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_repeat_h[window_size//2] = H-(window_size-1)\n",
    "num_repeat_w[window_size//2] = W-(window_size-1)\n",
    "num_repeat_h, num_repeat_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2b8ba5-a3b8-4f7b-a3e0-e1a3871b9c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeighborhoodAttention(nn.Module): #It can only use static size as input,but you can define a new input size if you wish.\n",
    "    def __init__(self,input_size, dim, num_heads,window_size=7, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        assert window_size%2 == 1,'windowsize must be odd.'\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "        self.qkv = nn.Conv2d(dim,dim*3,1, bias=qkv_bias)\n",
    "        self.proj = nn.Conv2d(dim, dim, 1)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.pad_idx = nn.ReplicationPad2d(self.window_size//2)\n",
    "        self.relative_bias = nn.Parameter(torch.zeros((2*self.window_size-1)**2,num_heads))\n",
    "        trunc_normal_(self.relative_bias, std=.02)\n",
    "        self.idx_h = torch.arange(0,window_size)\n",
    "        self.idx_w = torch.arange(0,window_size)\n",
    "        self.idx_k = ((self.idx_h.unsqueeze(-1) * (2*self.window_size-1)) + self.idx_w).view(-1)\n",
    "        self.set_input_size(input_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.attention(x)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "    def attention(self,x):\n",
    "        B,C,H,W = x.shape\n",
    "        assert H >= self.window_size and W >= self.window_size,'input size must not be smaller than window size'\n",
    "        qkv = self.qkv(x).view(B, 3,self.num_heads,C//self.num_heads,H*W).permute(1, 0, 2, 4, 3)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        q = q * self.scale\n",
    "        attn = q.unsqueeze(3) @ k[:,:,self.attn_idx].transpose(-1,-2) #B,nh,L,1,K^2\n",
    "        attn = attn + self.relative_bias[self.bias_idx].permute(2, 0, 1).unsqueeze(2)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = (attn @ v[:,:,self.attn_idx]).squeeze(3).transpose(-1,-2).contiguous().view(B,C,H,W)\n",
    "        return x\n",
    "        \n",
    "    def get_bias_idx(self,H,W):\n",
    "        num_repeat_h = torch.ones(self.window_size,dtype=torch.long)\n",
    "        num_repeat_w = torch.ones(self.window_size,dtype=torch.long)\n",
    "        num_repeat_h[self.window_size//2] = H-(self.window_size-1)\n",
    "        num_repeat_w[self.window_size//2] = W-(self.window_size-1)\n",
    "        bias_hw = (self.idx_h.repeat_interleave(num_repeat_h).unsqueeze(-1) * (2*self.window_size-1)) + self.idx_w.repeat_interleave(num_repeat_w)\n",
    "        bias_idx = bias_hw.unsqueeze(-1) + self.idx_k\n",
    "        return bias_idx.view(-1,self.window_size**2)\n",
    "    \n",
    "    def get_attn_idx(self,H,W):\n",
    "        H_ = H - (self.window_size - 1)\n",
    "        W_ = W - (self.window_size - 1)\n",
    "        attn_idx = torch.arange(0,H_*W_,dtype=torch.float).view(1,1,H_,W_)\n",
    "        attn_idx = self.pad_idx(attn_idx).view(-1).type(torch.long)\n",
    "        attn_idx = self.get_unfold_idx(H,W)[attn_idx]\n",
    "        return attn_idx\n",
    "    \n",
    "    def get_unfold_idx(self,H,W):\n",
    "        H_ = H-(self.window_size-1)\n",
    "        W_ = W-(self.window_size-1)\n",
    "        h_idx = torch.arange(W_).repeat(H_)\n",
    "        w_idx = torch.arange(H_).repeat_interleave(W_) * W\n",
    "        k_idx_1 = torch.arange(self.window_size).repeat(self.window_size)\n",
    "        k_idx_2 = torch.arange(self.window_size).repeat_interleave(self.window_size) * W\n",
    "        k_idx = k_idx_1 + k_idx_2\n",
    "        hw_idx = h_idx + w_idx\n",
    "        unfold_idx = hw_idx[:,None] + k_idx\n",
    "        return unfold_idx\n",
    "    \n",
    "    def set_input_size(self,input_size):\n",
    "        H,W = input_size\n",
    "        self.H,self.W = H,W\n",
    "        assert H >= self.window_size and W >= self.window_size,'input size must not be smaller than window size'\n",
    "        attn_idx = self.get_attn_idx(H,W)\n",
    "        bias_idx = self.get_bias_idx(H,W)\n",
    "        self.register_buffer(\"attn_idx\", attn_idx)\n",
    "        self.register_buffer(\"bias_idx\",bias_idx)\n",
    "        \n",
    "class NATLayer(nn.Module):\n",
    "    def __init__(self,input_size, dim, num_heads,window_size=7,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n",
    "                 act_layer=nn.GELU, norm_layer=Channel_Layernorm, layer_scale=None):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = NeighborhoodAttention(input_size, dim, num_heads,window_size,qkv_bias, qk_scale, attn_drop, drop)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=drop)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attn(x)\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "    \n",
    "    def set_input_size(self,input_size):\n",
    "        self.attn.set_input_size(input_size)\n",
    "        \n",
    "def test():\n",
    "    print('it is cpu')\n",
    "    model = NATLayer((28,28),128,4)\n",
    "    img = torch.rand(2,128,56,56)\n",
    "    try:\n",
    "        print(model(img).shape)\n",
    "    except:\n",
    "        print('error')\n",
    "        model.set_input_size((56,56))\n",
    "        print(model(img).shape)\n",
    "    print('cpu_success\\n')\n",
    "\n",
    "def test_cuda():\n",
    "    print('it is cuda')\n",
    "    model = NATLayer((28,28),128,4).cuda()\n",
    "    img = torch.rand(2,128,56,56).cuda()\n",
    "    try:\n",
    "        print(model(img).shape)\n",
    "    except:\n",
    "        print('error')\n",
    "        model.set_input_size((56,56))\n",
    "        print(model(img).shape)\n",
    "    print('success')\n",
    "    print('cuda_success\\n')\n",
    "        \n",
    "if __name__ == '__main__' :\n",
    "    test()\n",
    "    if torch.cuda.is_available():\n",
    "        test_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a88fef2-589a-4748-a226-62e7f123a932",
   "metadata": {},
   "outputs": [],
   "source": [
    "att = NeighborhoodAttention(\n",
    "            dim= 64,\n",
    "            kernel_size=7,\n",
    "            dilation=[1],\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            qk_scale=qk_scale,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=drop,\n",
    "            **extra_args,\n",
    "        )\n",
    "di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83fd463-e4ad-4065-8940-f817e8b80db8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fecd0c3f-aa7f-459a-b6ac-355f040054ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define two vectors\n",
    "a = torch.tensor([1.0, 2.0, 3.0])\n",
    "b = torch.tensor([4.0, 5.0, 6.0])\n",
    "\n",
    "# Compute dot product using einsum\n",
    "dot_product = torch.einsum(\"i,i->\", a, b)\n",
    "\n",
    "print(dot_product)  # Output: tensor(32.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "36f1e28a-70a5-4670-bba3-f003c3ffa548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(32.)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a@b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ba302141-3a22-459a-a653-9e5d8c377cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[19., 22.],\n",
      "        [43., 50.]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "B = torch.tensor([[5.0, 6.0], [7.0, 8.0]])\n",
    "\n",
    "# Matrix multiplication using einsum\n",
    "result = torch.einsum(\"ij,jk->ik\", A, B)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fa286cb6-0078-4359-a9d8-f1f3b0e57740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19., 22.],\n",
       "        [43., 50.]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A@B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ea3695b0-07b7-45df-8f58-b8ec814ffbfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4963, 0.7682],\n",
       "         [0.0885, 0.1320]]),\n",
       " tensor([[0.3074, 0.6341],\n",
       "         [0.4901, 0.8964]]))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "x1, x2 = torch.rand(2,2), torch.rand(2,2)\n",
    "x1,x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "eda5c230-f60d-45c9-aede-e9f1687138d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5291, 1.0033],\n",
      "        [0.0919, 0.1745]])\n"
     ]
    }
   ],
   "source": [
    "result = torch.einsum(\"ij,jk->ik\", x1,x2)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "bf5ba495-32cd-4feb-b73a-a7cb760a9780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5291, 1.0033],\n",
       "        [0.0919, 0.1745]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1@x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2e780799-2027-4271-8c72-12ce711fe46c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4963, 0.7682, 0.0885, 0.1320, 0.3074]]),\n",
       " tensor([[0.6341],\n",
       "         [0.4901],\n",
       "         [0.8964],\n",
       "         [0.4556],\n",
       "         [0.6323]]))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "x1, x2 = torch.rand(1,5), torch.rand(5,1)\n",
    "x1,x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e76dc6f1-aa34-498c-a3b2-87ab06b0d91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0250)\n"
     ]
    }
   ],
   "source": [
    "result = torch.einsum(\"ij,jk->\", x1,x2)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "96a23052-2909-4669-8562-42bbb7959116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0250]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1@x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "418fc923-827d-4410-b0bd-a9960dd99f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.4913, -0.2041, -0.0885,  0.5239],\n",
       "          [-0.6659,  0.8504, -1.3527, -1.6959],\n",
       "          [ 0.7854,  0.9928, -0.1932, -0.3090]],\n",
       " \n",
       "         [[ 0.5026, -0.8594,  0.7502, -0.5855],\n",
       "          [ 1.4437,  0.2660,  0.1665,  0.8744],\n",
       "          [-0.1435, -0.1116, -0.6136,  0.0316]],\n",
       " \n",
       "         [[ 2.0050,  0.0537,  0.6181, -0.4128],\n",
       "          [-0.8411, -2.3160, -0.1023,  0.7924],\n",
       "          [ 0.5627,  0.2596, -0.1740, -0.6787]],\n",
       " \n",
       "         [[ 0.9383,  0.4889, -0.6731,  0.8728],\n",
       "          [-1.2001, -0.0048, -0.5181, -0.3067],\n",
       "          [-0.4731,  0.3356,  1.5091,  2.0820]],\n",
       " \n",
       "         [[ 1.7067,  2.3804, -1.1256, -0.3170],\n",
       "          [-0.1407,  0.8058,  0.3276, -0.7607],\n",
       "          [-1.5991,  0.0185, -0.7504,  0.1854]]]),\n",
       " tensor([[ 1.0395,  0.3582, -0.0033, -0.5344],\n",
       "         [ 0.2823,  0.4342, -0.8025, -1.2952],\n",
       "         [-0.7502, -1.3120, -0.2188, -2.4351],\n",
       "         [-0.4288,  0.2329,  0.7969, -0.1848],\n",
       "         [-0.3701, -1.2103, -0.6227, -0.4637]]))"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.randn(5, 3, 4)  # Batch of 5 matrices (3x4)\n",
    "b = torch.randn(5, 4)\n",
    "A,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f56787f7-313b-420d-91fe-230e915fcf55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse Attention Output:\n",
      " [[0.66065966 0.56824659 0.53219128 0.49005517]\n",
      " [0.67007638 0.36214333 0.67851005 0.3613909 ]\n",
      " [0.2659878  0.56335287 0.48349048 0.53660343]\n",
      " [0.43979126 0.6548967  0.32939767 0.58181043]\n",
      " [0.58531914 0.48864323 0.10726895 0.61624787]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sparse_attention(Q, K, V, rho):\n",
    "    \"\"\"\n",
    "    Compute sparse attention where each query only attends to a subset of keys.\n",
    "    \n",
    "    Parameters:\n",
    "    Q: (n, d) Query matrix\n",
    "    K: (m, d) Key matrix\n",
    "    V: (m, d) Value matrix\n",
    "    rho: function that takes index i and returns selected key indices\n",
    "    \n",
    "    Returns:\n",
    "    A: (n, d) Attention output matrix\n",
    "    \"\"\"\n",
    "    n, d = Q.shape\n",
    "    A = np.zeros((n, d))  # Initialize output matrix\n",
    "    \n",
    "    for i in range(n):\n",
    "        selected_indices = rho(i)  # Get the subset of keys for query i\n",
    "        K_subset = K[selected_indices]  # Select keys\n",
    "        V_subset = V[selected_indices]  # Select values\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = Q[i] @ K_subset.T  # (1, d) @ (d, k) -> (1, k)\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        alpha = np.exp(scores - np.max(scores))  # Stability trick\n",
    "        alpha /= np.sum(alpha)  # Normalize\n",
    "        \n",
    "        # Compute weighted sum of values\n",
    "        A[i] = alpha @ V_subset  # (1, k) @ (k, d) -> (1, d)\n",
    "    \n",
    "    return A\n",
    "\n",
    "# Example Usage\n",
    "np.random.seed(42)\n",
    "n, m, d = 5, 10, 4  # 5 queries, 10 keys/values, embedding dim 4\n",
    "Q = np.random.rand(n, d)\n",
    "K = np.random.rand(m, d)\n",
    "V = np.random.rand(m, d)\n",
    "\n",
    "# Define rho: selecting 3 random keys for each query\n",
    "def rho(i):\n",
    "    return np.random.choice(m, 3, replace=False)  # Select 3 keys per query\n",
    "\n",
    "A = sparse_attention(Q, K, V, rho)\n",
    "print(\"Sparse Attention Output:\\n\", A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "58cbd4b2-556c-4748-8143-a918b7667067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialtion = [1,1,2,4,1]\n",
    "[i for i in dialtion for j in range(0,10,2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "cb41d97e-1841-4ffb-87ef-c1b7f6a3258e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0.])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = torch.Tensor([1,0,0])\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "5ba45d6b-4135-4a57-8b8e-60b6f7b288d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0.])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat /= torch.sum(mat)\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "c5bde78a-46e4-4ee8-a2cc-b7bff1be17d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5761, 0.2119, 0.2119])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(mat,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6a7eba7-7aba-4db0-a635-870360ffd894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (4.48.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2025.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from requests->transformers) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8611fe28-8e99-4fad-a6b2-a2835ec9b6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DinatConfig, DinatModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a93cbe7f-85f4-4577-8497-3e0ec6ec5373",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"patch_size\": 4,\n",
    "    \"num_channels\": 3,\n",
    "    \"embed_dim\": 64,\n",
    "    \"depths\": [3, 4, 6, 5],\n",
    "    \"num_heads\": [2, 4, 8, 16],\n",
    "    \"kernel_size\": 7,\n",
    "    \"dilations\": [[1, 8, 1], [1, 4, 1, 4], [1, 2, 1, 2, 1, 2], [1, 1, 1, 1, 1]],\n",
    "    \"mlp_ratio\": 3.0,\n",
    "    \"qkv_bias\": True,\n",
    "    \"hidden_dropout_prob\": 0.0,\n",
    "    \"attention_probs_dropout_prob\": 0.0,\n",
    "    \"drop_path_rate\": 0.1,\n",
    "    \"hidden_act\": \"gelu\",\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"layer_norm_eps\": 1e-05,\n",
    "    \"layer_scale_init_value\": 0.0,\n",
    "    \"out_features\": None,\n",
    "    \"out_indices\": None\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73482513-7117-4969-a72a-619ec7c2383a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nDinatModel requires the natten library but it was not found in your environment. You can install it by referring to:\nshi-labs.com/natten . You can also install it with pip (may take longer to build):\n`pip install natten`. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mDinatModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshi-labs/dinat-mini-in1k-224\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\transformers\\modeling_utils.py:4111\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4105\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_autoset_attn_implementation(\n\u001b[0;32m   4106\u001b[0m         config, use_flash_attention_2\u001b[38;5;241m=\u001b[39muse_flash_attention_2, torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype, device_map\u001b[38;5;241m=\u001b[39mdevice_map\n\u001b[0;32m   4107\u001b[0m     )\n\u001b[0;32m   4109\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[0;32m   4110\u001b[0m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[1;32m-> 4111\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(config, \u001b[38;5;241m*\u001b[39mmodel_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   4113\u001b[0m \u001b[38;5;66;03m# make sure we use the model's config since the __init__ call might have copied it\u001b[39;00m\n\u001b[0;32m   4114\u001b[0m config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\transformers\\models\\dinat\\modeling_dinat.py:680\u001b[0m, in \u001b[0;36mDinatModel.__init__\u001b[1;34m(self, config, add_pooling_layer)\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config, add_pooling_layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n\u001b[1;32m--> 680\u001b[0m     \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnatten\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m    683\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_levels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(config\u001b[38;5;241m.\u001b[39mdepths)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\transformers\\utils\\import_utils.py:1678\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1676\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m-> 1678\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[1;31mImportError\u001b[0m: \nDinatModel requires the natten library but it was not found in your environment. You can install it by referring to:\nshi-labs.com/natten . You can also install it with pip (may take longer to build):\n`pip install natten`. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "DinatModel.from_pretrained(\"shi-labs/dinat-mini-in1k-224\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a7e45c6-8559-4c01-9089-841ab8431d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting natten\n",
      "  Downloading natten-0.17.4.tar.gz (10.9 MB)\n",
      "     ---------------------------------------- 0.0/10.9 MB ? eta -:--:--\n",
      "     ----------- ---------------------------- 3.1/10.9 MB 61.4 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 4.2/10.9 MB 16.7 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 5.2/10.9 MB 11.4 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 6.3/10.9 MB 9.0 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 7.3/10.9 MB 7.7 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 8.4/10.9 MB 7.1 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 9.4/10.9 MB 6.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 10.5/10.9 MB 6.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 10.9/10.9 MB 5.8 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: packaging in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from natten) (23.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from natten) (2.2.0+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from torch>=2.0.0->natten) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from torch>=2.0.0->natten) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from torch>=2.0.0->natten) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from torch>=2.0.0->natten) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from torch>=2.0.0->natten) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from torch>=2.0.0->natten) (2025.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from jinja2->torch>=2.0.0->natten) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from sympy->torch>=2.0.0->natten) (1.3.0)\n",
      "Building wheels for collected packages: natten\n",
      "  Building wheel for natten (setup.py): started\n",
      "  Building wheel for natten (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for natten\n",
      "Failed to build natten\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "   python setup.py bdist_wheel did not run successfully.\n",
      "   exit code: 1\n",
      "  > [102 lines of output]\n",
      "      Building NATTEN for CPU ONLY.\n",
      "      Number of workers: 5\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-cpython-39\n",
      "      creating build\\lib.win-amd64-cpython-39\\natten\n",
      "      copying src\\natten\\context.py -> build\\lib.win-amd64-cpython-39\\natten\n",
      "      copying src\\natten\\experimental.py -> build\\lib.win-amd64-cpython-39\\natten\n",
      "      copying src\\natten\\flops.py -> build\\lib.win-amd64-cpython-39\\natten\n",
      "      copying src\\natten\\functional.py -> build\\lib.win-amd64-cpython-39\\natten\n",
      "      copying src\\natten\\na1d.py -> build\\lib.win-amd64-cpython-39\\natten\n",
      "      copying src\\natten\\na2d.py -> build\\lib.win-amd64-cpython-39\\natten\n",
      "      copying src\\natten\\na3d.py -> build\\lib.win-amd64-cpython-39\\natten\n",
      "      copying src\\natten\\natten1d.py -> build\\lib.win-amd64-cpython-39\\natten\n",
      "      copying src\\natten\\natten2d.py -> build\\lib.win-amd64-cpython-39\\natten\n",
      "      copying src\\natten\\natten3d.py -> build\\lib.win-amd64-cpython-39\\natten\n",
      "      copying src\\natten\\nested.py -> build\\lib.win-amd64-cpython-39\\natten\n",
      "      copying src\\natten\\ops.py -> build\\lib.win-amd64-cpython-39\\natten\n",
      "      copying src\\natten\\types.py -> build\\lib.win-amd64-cpython-39\\natten\n",
      "      copying src\\natten\\__init__.py -> build\\lib.win-amd64-cpython-39\\natten\n",
      "      creating build\\lib.win-amd64-cpython-39\\natten\\utils\n",
      "      copying src\\natten/utils\\checks.py -> build\\lib.win-amd64-cpython-39\\natten/utils\n",
      "      copying src\\natten/utils\\log.py -> build\\lib.win-amd64-cpython-39\\natten/utils\n",
      "      copying src\\natten/utils\\misc.py -> build\\lib.win-amd64-cpython-39\\natten/utils\n",
      "      copying src\\natten/utils\\tensor.py -> build\\lib.win-amd64-cpython-39\\natten/utils\n",
      "      copying src\\natten/utils\\testing.py -> build\\lib.win-amd64-cpython-39\\natten/utils\n",
      "      copying src\\natten/utils\\__init__.py -> build\\lib.win-amd64-cpython-39\\natten/utils\n",
      "      creating build\\lib.win-amd64-cpython-39\\natten\\autotuner\n",
      "      copying src\\natten/autotuner\\fna_backward.py -> build\\lib.win-amd64-cpython-39\\natten/autotuner\n",
      "      copying src\\natten/autotuner\\fna_forward.py -> build\\lib.win-amd64-cpython-39\\natten/autotuner\n",
      "      copying src\\natten/autotuner\\misc.py -> build\\lib.win-amd64-cpython-39\\natten/autotuner\n",
      "      copying src\\natten/autotuner\\__init__.py -> build\\lib.win-amd64-cpython-39\\natten/autotuner\n",
      "      creating build\\lib.win-amd64-cpython-39\\natten\\autotuner\\configs\n",
      "      copying src\\natten/autotuner/configs\\fna_backward_128x128.py -> build\\lib.win-amd64-cpython-39\\natten/autotuner/configs\n",
      "      copying src\\natten/autotuner/configs\\fna_backward_128x64.py -> build\\lib.win-amd64-cpython-39\\natten/autotuner/configs\n",
      "      copying src\\natten/autotuner/configs\\fna_backward_64x64.py -> build\\lib.win-amd64-cpython-39\\natten/autotuner/configs\n",
      "      copying src\\natten/autotuner/configs\\fna_forward_32x128.py -> build\\lib.win-amd64-cpython-39\\natten/autotuner/configs\n",
      "      copying src\\natten/autotuner/configs\\fna_forward_64x128.py -> build\\lib.win-amd64-cpython-39\\natten/autotuner/configs\n",
      "      copying src\\natten/autotuner/configs\\fna_forward_64x64.py -> build\\lib.win-amd64-cpython-39\\natten/autotuner/configs\n",
      "      copying src\\natten/autotuner/configs\\__init__.py -> build\\lib.win-amd64-cpython-39\\natten/autotuner/configs\n",
      "      running build_ext\n",
      "      Traceback (most recent call last):\n",
      "        File \"C:\\Users\\Nihar\\AppData\\Local\\Temp\\pip-install-wk28c5_i\\natten_e3c8a32664754e9ca8700b9cbe069a46\\setup.py\", line 162, in build_extension\n",
      "          subprocess.check_output([\"cmake\", \"--version\"])\n",
      "        File \"C:\\Users\\Nihar\\anaconda3\\envs\\tf\\lib\\subprocess.py\", line 424, in check_output\n",
      "          return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n",
      "        File \"C:\\Users\\Nihar\\anaconda3\\envs\\tf\\lib\\subprocess.py\", line 505, in run\n",
      "          with Popen(*popenargs, **kwargs) as process:\n",
      "        File \"C:\\Users\\Nihar\\anaconda3\\envs\\tf\\lib\\subprocess.py\", line 951, in __init__\n",
      "          self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "        File \"C:\\Users\\Nihar\\anaconda3\\envs\\tf\\lib\\subprocess.py\", line 1436, in _execute_child\n",
      "          hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "      FileNotFoundError: [WinError 2] The system cannot find the file specified\n",
      "      \n",
      "      During handling of the above exception, another exception occurred:\n",
      "      \n",
      "      Traceback (most recent call last):\n",
      "        File \"<string>\", line 2, in <module>\n",
      "        File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "        File \"C:\\Users\\Nihar\\AppData\\Local\\Temp\\pip-install-wk28c5_i\\natten_e3c8a32664754e9ca8700b9cbe069a46\\setup.py\", line 243, in <module>\n",
      "          setup(\n",
      "        File \"C:\\Users\\Nihar\\anaconda3\\envs\\tf\\lib\\site-packages\\setuptools\\__init__.py\", line 103, in setup\n",
      "          return distutils.core.setup(**attrs)\n",
      "        File \"C:\\Users\\Nihar\\anaconda3\\envs\\tf\\lib\\site-packages\\setuptools\\_distutils\\core.py\", line 185, in setup\n",
      "          return run_commands(dist)\n",
      "        File \"C:\\Users\\Nihar\\anaconda3\\envs\\tf\\lib\\site-packages\\setuptools\\_distutils\\core.py\", line 201, in run_commands\n",
      "          dist.run_commands()\n",
      "        File \"C:\\Users\\Nihar\\anaconda3\\envs\\tf\\lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 969, in run_commands\n",
      "          self.run_command(cmd)\n",
      "        File \"C:\\Users\\Nihar\\anaconda3\\envs\\tf\\lib\\site-packages\\setuptools\\dist.py\", line 989, in run_command\n",
      "          super().run_command(command)\n",
      "        File \"C:\\Users\\Nihar\\anaconda3\\envs\\tf\\lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 988, in run_command\n",
      "          cmd_obj.run()\n",
      "        File \"C:\\Users\\Nihar\\anaconda3\\envs\\tf\\lib\\site-packages\\wheel\\bdist_wheel.py\", line 364, in run\n",
      "          self.run_command(\"build\")\n",
      "        File \"C:\\Users\\Nihar\\anaconda3\\envs\\tf\\lib\\site-packages\\setuptools\\_distutils\\cmd.py\", line 318, in run_command\n",
      "          self.distribution.run_command(command)\n",
      "        File \"C:\\Users\\Nihar\\anaconda3\\envs\\tf\\lib\\site-packages\\setuptools\\dist.py\", line 989, in run_command\n",
      "          super().run_command(command)\n",
      "        File \"C:\\Users\\Nihar\\anaconda3\\envs\\tf\\lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 988, in run_command\n",
      "          cmd_obj.run()\n",
      "        File \"C:\\Users\\Nihar\\anaconda3\\envs\\tf\\lib\\site-packages\\setuptools\\_distutils\\command\\build.py\", line 131, in run\n",
      "          self.run_command(cmd_name)\n",
      "        File \"C:\\Users\\Nihar\\anaconda3\\envs\\tf\\lib\\site-packages\\setuptools\\_distutils\\cmd.py\", line 318, in run_command\n",
      "          self.distribution.run_command(command)\n",
      "        File \"C:\\Users\\Nihar\\anaconda3\\envs\\tf\\lib\\site-packages\\setuptools\\dist.py\", line 989, in run_command\n",
      "          super().run_command(command)\n",
      "        File \"C:\\Users\\Nihar\\anaconda3\\envs\\tf\\lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 988, in run_command\n",
      "          cmd_obj.run()\n",
      "        File \"C:\\Users\\Nihar\\anaconda3\\envs\\tf\\lib\\site-packages\\setuptools\\command\\build_ext.py\", line 88, in run\n",
      "          _build_ext.run(self)\n",
      "        File \"C:\\Users\\Nihar\\anaconda3\\envs\\tf\\lib\\site-packages\\setuptools\\_distutils\\command\\build_ext.py\", line 345, in run\n",
      "          self.build_extensions()\n",
      "        File \"C:\\Users\\Nihar\\anaconda3\\envs\\tf\\lib\\site-packages\\setuptools\\_distutils\\command\\build_ext.py\", line 467, in build_extensions\n",
      "          self._build_extensions_serial()\n",
      "        File \"C:\\Users\\Nihar\\anaconda3\\envs\\tf\\lib\\site-packages\\setuptools\\_distutils\\command\\build_ext.py\", line 493, in _build_extensions_serial\n",
      "          self.build_extension(ext)\n",
      "        File \"C:\\Users\\Nihar\\AppData\\Local\\Temp\\pip-install-wk28c5_i\\natten_e3c8a32664754e9ca8700b9cbe069a46\\setup.py\", line 164, in build_extension\n",
      "          raise RuntimeError(\"Cannot find CMake executable\")\n",
      "      RuntimeError: Cannot find CMake executable\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for natten\n",
      "ERROR: Failed to build installable wheels for some pyproject.toml based projects (natten)\n"
     ]
    }
   ],
   "source": [
    "pip install natten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f016332f-2ce6-4534-a9e8-448f52a008fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 32, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DilatedNeighborhoodAttention(nn.Module):\n",
    "    def __init__(self, dim, kernel_size=7, dilation=1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation = dilation\n",
    "        self.padding = (kernel_size // 2) * dilation\n",
    "        \n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, H, W, C = x.shape\n",
    "        q, k, v = self.qkv(x).chunk(3, dim=-1)\n",
    "        \n",
    "        # Unfold input to extract neighborhoods with dilation\n",
    "        unfolded_k = F.unfold(k.permute(0, 3, 1, 2),\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              dilation=self.dilation,\n",
    "                              padding=self.padding)\n",
    "        unfolded_v = F.unfold(v.permute(0, 3, 1, 2),\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              dilation=self.dilation,\n",
    "                              padding=self.padding)\n",
    "        \n",
    "        # Reshape for attention\n",
    "        unfolded_k = unfolded_k.view(B, C, self.kernel_size ** 2, H, W).permute(0, 3, 4, 2, 1)\n",
    "        unfolded_v = unfolded_v.view(B, C, self.kernel_size ** 2, H, W).permute(0, 3, 4, 2, 1)\n",
    "        \n",
    "        q = q.unsqueeze(-2)  # (B, H, W, 1, C)\n",
    "        attn = (q * unfolded_k).sum(-1) / (C ** 0.5)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        \n",
    "        out = (attn.unsqueeze(-1) * unfolded_v).sum(-2)\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "class DiNATBlock(nn.Module):\n",
    "    def __init__(self, dim, kernel_size=7, dilation=1, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = DilatedNeighborhoodAttention(dim, kernel_size, dilation)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, int(dim * mlp_ratio)),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(int(dim * mlp_ratio), dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "# Example Usage\n",
    "B, H, W, C = 1, 32, 32, 64  # Batch size, height, width, channels\n",
    "x = torch.randn(B, H, W, C)\n",
    "block = DiNATBlock(dim=C, kernel_size=7, dilation=2)\n",
    "output = block(x)\n",
    "print(output.shape)  # Expected output: (1, 32, 32, 64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36befd22-fc70-4e7c-860f-e5ba9e3888c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 64, 49, 32, 32]' is invalid for input of size 1254400",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 60\u001b[0m\n\u001b[0;32m     58\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(B, H, W, C)\n\u001b[0;32m     59\u001b[0m block \u001b[38;5;241m=\u001b[39m DiNATBlock(dim\u001b[38;5;241m=\u001b[39mC, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m, dilation\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 60\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 52\u001b[0m, in \u001b[0;36mDiNATBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 52\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 28\u001b[0m, in \u001b[0;36mDilatedNeighborhoodAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     23\u001b[0m unfolded_v \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39munfold(v\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m),\n\u001b[0;32m     24\u001b[0m                       kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size,\n\u001b[0;32m     25\u001b[0m                       dilation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Reshape for attention\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m unfolded_k \u001b[38;5;241m=\u001b[39m \u001b[43munfolded_k\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     29\u001b[0m unfolded_v \u001b[38;5;241m=\u001b[39m unfolded_v\u001b[38;5;241m.\u001b[39mview(B, C, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, H, W)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     31\u001b[0m q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# (B, H, W, 1, C)\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[1, 64, 49, 32, 32]' is invalid for input of size 1254400"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DilatedNeighborhoodAttention(nn.Module):\n",
    "    def __init__(self, dim, kernel_size=7, dilation=1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation = dilation\n",
    "        \n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, H, W, C = x.shape\n",
    "        q, k, v = self.qkv(x).chunk(3, dim=-1)\n",
    "        \n",
    "        # Unfold input to extract neighborhoods with dilation\n",
    "        unfolded_k = F.unfold(k.permute(0, 3, 1, 2),\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              dilation=self.dilation)\n",
    "        unfolded_v = F.unfold(v.permute(0, 3, 1, 2),\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              dilation=self.dilation)\n",
    "        \n",
    "        # Reshape for attention\n",
    "        unfolded_k = unfolded_k.view(B, C, self.kernel_size ** 2, H, W).permute(0, 3, 4, 2, 1)\n",
    "        unfolded_v = unfolded_v.view(B, C, self.kernel_size ** 2, H, W).permute(0, 3, 4, 2, 1)\n",
    "        \n",
    "        q = q.unsqueeze(-2)  # (B, H, W, 1, C)\n",
    "        attn = (q * unfolded_k).sum(-1) / (C ** 0.5)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        \n",
    "        out = (attn.unsqueeze(-1) * unfolded_v).sum(-2)\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "class DiNATBlock(nn.Module):\n",
    "    def __init__(self, dim, kernel_size=7, dilation=1, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = DilatedNeighborhoodAttention(dim, kernel_size, dilation)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, int(dim * mlp_ratio)),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(int(dim * mlp_ratio), dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "# Example Usage\n",
    "B, H, W, C = 1, 32, 32, 64  # Batch size, height, width, channels\n",
    "x = torch.randn(B, H, W, C)\n",
    "block = DiNATBlock(dim=C, kernel_size=7, dilation=2)\n",
    "output = block(x)\n",
    "print(output.shape)  # Expected output: (1, 32, 32, 64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76c77284-45d2-4753-a1a8-54595e736bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DilatedNeighborhoodAttention(nn.Module):\n",
    "    def __init__(self, dim, kernel_size=7, dilation=1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation = dilation\n",
    "        \n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, H, W, C = x.shape\n",
    "        q, k, v = self.qkv(x).chunk(3, dim=-1)\n",
    "        return q, k, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "841b375b-3af7-4f44-a260-0d1df463dc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, H, W, C = 1, 32, 32, 64  # Batch size, height, width, channels\n",
    "x = torch.randn(B, H, W, C)\n",
    "block = DilatedNeighborhoodAttention(dim=C, kernel_size=7, dilation=2)\n",
    "q, k, v = block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "859ddfda-7735-4c1b-bf6a-a093c5e308ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 32, 64])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b17b3c7c-7528-47b7-b29f-3e26cd2828bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tor = torch.empty(0)\n",
    "for i in range(0,H):\n",
    "             for j  in range(0,W):\n",
    "                tor = torch.cat((tor,q[:,i,j,:].unsqueeze(1).unsqueeze(1)),dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7ea39370-f20b-47d7-a099-63e2fe304396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "676"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = []\n",
    "kernel_size = 7\n",
    "for i in range(kernel_size ,H+1):\n",
    "             for j  in range(kernel_size ,W+1):\n",
    "                 a.append(i*j)\n",
    "len(a)\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a58a8799-afa0-428d-b109-46a0c6ddc660",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 1 in argument 0, but got int",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m tensor_result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Start with an empty tensor\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m----> 7\u001b[0m     tensor_result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_result\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Concatenate\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(tensor_result)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected Tensor as element 1 in argument 0, but got int"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor_result = torch.empty(0)  # Start with an empty tensor\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    tensor_result = torch.cat((tensor_result, i), dim=0)  # Concatenate\n",
    "\n",
    "print(tensor_result)  # Output: tensor([0., 1., 2., 3., 4.])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43b009e-be0b-4ed0-b19f-223111deb252",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
