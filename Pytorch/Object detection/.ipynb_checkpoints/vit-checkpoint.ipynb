{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "429bf434-0d62-4ba0-b9f9-31a8dbcca3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eee704dc-4306-4477-9641-4eeb13655104",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"Split image into patches and then embed them.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_size : int\n",
    "        Size of the image (it is a square).\n",
    "\n",
    "    patch_size : int\n",
    "        Size of the patch (it is a square).\n",
    "\n",
    "    in_chans : int\n",
    "        Number of input channels.\n",
    "\n",
    "    embed_dim : int\n",
    "        The embedding dimension.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    n_patches : int\n",
    "        Number of patches inside of our image.\n",
    "\n",
    "    proj : nn.Conv2d\n",
    "        Convolutional layer that does both the splitting into patches and their embedding.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size, patch_size, in_channels = 3, embed_dim = 768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels, embed_dim, kernel_size = patch_size, stride = patch_size,)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Run forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape '(n_samples, in_chans, img_size, img_size)'.\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Shape '(n_samples, n_patches, embed_dim)'.\n",
    "        \"\"\"\n",
    "        x = self.proj(x) # (n_samples, emed_dim, n_patches ** 0.5, n_patches ** 0.5)\n",
    "        print(x.shape)\n",
    "        x = x.flatten(2) # (n_samples, emed_dim, n_patches)\n",
    "        print(x.shape)\n",
    "        x = x.transpose(1,2) # n_samples, n_patches, embed_dim)\n",
    "        print(x.shape)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "23118314-bfd5-44a2-9b8e-b5b87db8fd42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768, 14, 14])\n",
      "torch.Size([1, 768, 196])\n",
      "torch.Size([1, 196, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 196, 768])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1,3,224,224)\n",
    "\n",
    "model = PatchEmbed(img_size=224,patch_size=16,embed_dim=768,in_channels=3)\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f4e44498-90b0-48b1-b5c1-91f369da1572",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\" Attention mechanism\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim : int\n",
    "        The input and out dimension of per token features.\n",
    "\n",
    "    n_heads: int\n",
    "        Number of attention heads.\n",
    "\n",
    "    qkv_bias : bool\n",
    "        If True, then we include bias to the query, key and value projections.\n",
    "\n",
    "    attn_p : float\n",
    "        Dropout probability applied to the query, key and value tensors.\n",
    "\n",
    "    proj_p : float\n",
    "        Dropout probability applied to the output tensor.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, n_heads=12, qkv_bias=True, attn_p=0., proj_p=0.):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim//n_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim*3, bias = qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_p)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Run forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: torch.Tensor\n",
    "            Shape '(n_samples, n_patches +1, dim)'.\n",
    "\n",
    "        Returns \n",
    "        ------\n",
    "        torch.Tensor\n",
    "            Shape '(n_samples, n_patches +1, dim)'.\n",
    "        \"\"\"\n",
    "        n_samples, n_tokens, dim = x.shape\n",
    "\n",
    "        if dim != self.dim:\n",
    "            raise ValueError\n",
    "\n",
    "        qkv = self.qkv(x) # (n_samples, n_patches +1, 3 * dim)\n",
    "        qkv = qkv.reshape(\n",
    "                n_samples, n_tokens, 3, self, self.n_heads, self.head_dim)\n",
    "        \n",
    "        qkv = qkv.permute(\n",
    "            2, 0, 3, 1, 4)\n",
    "        # (3, n_samples, n_heads, n_patches + 1,head_dim )\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        k_t = k.transpose(-2, -1) # (n_samples, n_heads, head_dim, n_patches +1)\n",
    "\n",
    "        dp = (q @ k_t) * self.scale\n",
    "\n",
    "        attn = dp.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        weighted_avg = attn @ v\n",
    "        weighted_avg = weighted_avg.transpose(1, 2)\n",
    "        weighted_avg = weighted_avg.flatten(2)\n",
    "\n",
    "        x = self.proj(weighted_avg)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c2f9c98d-9daa-4fff-a3c9-f5dd00f77545",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, p=0.):\n",
    "        super().__init()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.drop(self.act(self.fc1(x))))             \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6b839574-a125-4597-9706-4cc3a298752a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor([1,2,3])\n",
    "x@x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009322ce-32e4-4266-bd26-ebc98625fbcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1db482-912f-4ed9-aba0-cafc854d3a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af978e4f-6747-4a4d-9576-3c36f03d4b38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bb03d4d6-d912-483c-a2b7-93f6dee299c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(224/16) **2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2004c5d4-ae1d-4e0f-9fa7-754e204bca0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "448"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "224 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "595fff58-0f2e-4080-b19d-9056d3a63a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0900, 0.2447, 0.6652])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cb3e976e-87d1-42ac-ac5c-c69d4a96bf84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768, 8, 8])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " x= conv(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9186878-f986-4d44-8f51-e98dfae0fb51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768, 64])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.flatten(2)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34e2306b-2ec5-4482-a66e-af521d9af062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.transpose(1,2)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0d0cf01-944b-4ec5-b2b1-62b12507c8ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5683f34-c810-4476-9a33-099ea1471f72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(27.7128)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sqrt(torch.tensor(768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea3038f5-90aa-49b6-8fcd-d1f02d537ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nihar\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\functional.py:1354: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.\n",
      "  warnings.warn(\"dropout2d: Received a 3D input to dropout2d and assuming that channel-wise \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
       "         [-1.1850,  2.0897, -0.2977,  ...,  0.4573, -0.8970,  1.1202],\n",
       "         ...,\n",
       "         [-0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
       "         [-0.6121, -0.3459, -1.9492,  ...,  0.4251, -0.6151,  0.2304],\n",
       "         [-0.0000,  0.0000, -0.0000,  ..., -0.0000,  0.0000, -0.0000]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = nn.Dropout2d(0.5)\n",
    "p(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598ae2bd-a93b-4796-bc79-d02dfe00a195",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/position-embeddings-for-vision-transformers-explained-a6f9add341d5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "503008ca-6a50-4207-9ed1-74f56aacdf6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.training\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89d347d8-3f2a-4dd2-bc73-8dc1efc9489c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nihar\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Create a tensor of shape [max_len, dim] for encoding\n",
    "        pe = torch.zeros(max_len, dim)\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)  # [max_len, 1]\n",
    "        div_term = torch.exp(torch.arange(0, dim, 2).float() * -(math.log(10000.0) / dim))  # [dim/2]\n",
    "\n",
    "        # Apply sin and cos functions for positional encoding\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # even indices (sine)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # odd indices (cosine)\n",
    "\n",
    "        pe = pe.unsqueeze(0)  # Shape becomes [1, max_len, dim]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to the input tensor\n",
    "        return x + self.pe[:, :x.size(1)]  # x.size(1) is the length of the sequence\n",
    "\n",
    "# Example usage for ViT\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, dim=768, num_classes=1000):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        self.dim = dim\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Calculate the number of patches\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        # Define the embedding layer for patches\n",
    "        self.patch_embeddings = nn.Conv2d(3, dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "        # Positional encoding\n",
    "        self.positional_encoding = PositionalEncoding(dim, max_len=self.num_patches)\n",
    "\n",
    "        # Transformer layers (simplified version)\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=dim, nhead=8),\n",
    "            num_layers=12\n",
    "        )\n",
    "\n",
    "        # Classification head\n",
    "        self.fc = nn.Linear(dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract patches and embed them\n",
    "        x = self.patch_embeddings(x)  # Shape: [batch_size, dim, num_patches, num_patches]\n",
    "        x = x.flatten(2).transpose(1, 2)  # Shape: [batch_size, num_patches, dim]\n",
    "\n",
    "        # Add positional encoding\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        # Pass through transformer encoder\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # Classification head (take the output of the [CLS] token)\n",
    "        x = x.mean(dim=1)  # Global average pooling\n",
    "\n",
    "        # Final classification layer\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Example input (batch_size=2, img_size=224)\n",
    "model = VisionTransformer(img_size=224, patch_size=16)\n",
    "sample_input = torch.randn(2, 3, 224, 224)  # Batch of 2 images with 3 channels (RGB)\n",
    "output = model(sample_input)\n",
    "print(output.shape)  # Should output: torch.Size([2, 1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d15ba41-7f2f-49ae-88ef-7a1c7ffbeb6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 768])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(5000, 768).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b790c537-1b53-49a8-93f0-197e2abecb01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = torch.arange(0, 5000).float().unsqueeze(1)\n",
    "position.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a3ceec-88a8-4f96-ac0c-865a157f9eff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
