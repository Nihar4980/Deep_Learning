{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "429bf434-0d62-4ba0-b9f9-31a8dbcca3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eee704dc-4306-4477-9641-4eeb13655104",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"Split image into patches and then embed them.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_size : int\n",
    "        Size of the image (it is a square).\n",
    "\n",
    "    patch_size : int\n",
    "        Size of the patch (it is a square).\n",
    "\n",
    "    in_chans : int\n",
    "        Number of input channels.\n",
    "\n",
    "    embed_dim : int\n",
    "        The embedding dimension.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    n_patches : int\n",
    "        Number of patches inside of our image.\n",
    "\n",
    "    proj : nn.Conv2d\n",
    "        Convolutional layer that does both the splitting into patches and their embedding.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size, patch_size, in_channels = 3, embed_dim = 768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels, embed_dim, kernel_size = patch_size, stride = patch_size,)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Run forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape '(n_samples, in_chans, img_size, img_size)'.\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Shape '(n_samples, n_patches, embed_dim)'.\n",
    "        \"\"\"\n",
    "        x = self.proj(x) # (n_samples, emed_dim, n_patches ** 0.5, n_patches ** 0.5)\n",
    "       # print(x.shape)\n",
    "        x = x.flatten(2) # (n_samples, emed_dim, n_patches)\n",
    "       # print(x.shape)\n",
    "        x = x.transpose(1,2) # n_samples, n_patches, embed_dim)\n",
    "        #print(x.shape)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23118314-bfd5-44a2-9b8e-b5b87db8fd42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768, 74, 74])\n",
      "torch.Size([1, 768, 5476])\n",
      "torch.Size([1, 5476, 768])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1,3,224,224)\n",
    "\n",
    "\n",
    "model = PatchEmbed(img_size=224,patch_size=3,embed_dim=768,in_channels=3)\n",
    "x1 =model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f559cc7-d7fc-4093-9ad2-cfc7b4e89bb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4e44498-90b0-48b1-b5c1-91f369da1572",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\" Attention mechanism\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim : int\n",
    "        The input and out dimension of per token features.\n",
    "\n",
    "    n_heads: int\n",
    "        Number of attention heads.\n",
    "\n",
    "    qkv_bias : bool\n",
    "        If True, then we include bias to the query, key and value projections.\n",
    "\n",
    "    attn_p : float\n",
    "        Dropout probability applied to the query, key and value tensors.\n",
    "\n",
    "    proj_p : float\n",
    "        Dropout probability applied to the output tensor.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, n_heads=12, qkv_bias=True, attn_p=0., proj_p=0.):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim//n_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim*3, bias = qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_p)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Run forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: torch.Tensor\n",
    "            Shape '(n_samples, n_patches +1, dim)'.\n",
    "\n",
    "        Returns \n",
    "        ------\n",
    "        torch.Tensor\n",
    "            Shape '(n_samples, n_patches +1, dim)'.\n",
    "        \"\"\"\n",
    "        n_samples, n_tokens, dim = x.shape\n",
    "        print(x.shape)\n",
    "\n",
    "        if dim != self.dim:\n",
    "            raise ValueError\n",
    "\n",
    "        qkv = self.qkv(x) # (n_samples, n_patches +1, 3 * dim)\n",
    "        print(qkv.shape)\n",
    "        qkv = qkv.reshape(\n",
    "                n_samples, n_tokens, 3, self.n_heads, self.head_dim)\n",
    "        print(qkv.shape)\n",
    "        qkv = qkv.permute(\n",
    "            2, 0, 3, 1, 4)\n",
    "        # (3, n_samples, n_heads, n_patches + 1,head_dim )\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        k_t = k.transpose(-2, -1) # (n_samples, n_heads, head_dim, n_patches +1)\n",
    "\n",
    "        dp = (q @ k_t) * self.scale\n",
    "\n",
    "        attn = dp.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        weighted_avg = attn @ v\n",
    "        weighted_avg = weighted_avg.transpose(1, 2)\n",
    "        weighted_avg = weighted_avg.flatten(2)\n",
    "\n",
    "        x = self.proj(weighted_avg)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96591651-37b4-40e8-af83-9fa4da5a83d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5476, 768])\n",
      "torch.Size([1, 5476, 2304])\n",
      "torch.Size([1, 5476, 3, 12, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0270, -0.0347,  0.0014,  ...,  0.0459, -0.0907,  0.0335],\n",
       "         [ 0.0278, -0.0457,  0.0079,  ...,  0.0528, -0.0858,  0.0275],\n",
       "         [ 0.0302, -0.0391,  0.0138,  ...,  0.0553, -0.0886,  0.0247],\n",
       "         ...,\n",
       "         [ 0.0313, -0.0393,  0.0075,  ...,  0.0540, -0.0890,  0.0318],\n",
       "         [ 0.0302, -0.0475,  0.0092,  ...,  0.0518, -0.0853,  0.0327],\n",
       "         [ 0.0208, -0.0407,  0.0072,  ...,  0.0581, -0.0877,  0.0249]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod = Attention(768)\n",
    "mod(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c2f9c98d-9daa-4fff-a3c9-f5dd00f77545",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, p=0.):\n",
    "        super().__init()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.drop(self.act(self.fc1(x))))             \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6b839574-a125-4597-9706-4cc3a298752a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Block(nn.Module):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "009322ce-32e4-4266-bd26-ebc98625fbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min(list1):\n",
    "    list1.sort()\n",
    "    return list1[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e1db482-912f-4ed9-aba0-cafc854d3a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min([1,2,1,1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af978e4f-6747-4a4d-9576-3c36f03d4b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "nums = [-1,0,1,2,-1,-4]\n",
    "\n",
    "l = []\n",
    "for i in range(len(nums) - 2):  # Step 2: Iterate over the array\n",
    "        if i > 0 : #and nums[i] == nums[i - 1]:\n",
    "            print(i)\n",
    "    \n",
    "#Output: [[-1,-1,2],[-1,0,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bb03d4d6-d912-483c-a2b7-93f6dee299c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(224/16) **2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2004c5d4-ae1d-4e0f-9fa7-754e204bca0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "448"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "224 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "595fff58-0f2e-4080-b19d-9056d3a63a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0900, 0.2447, 0.6652])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cb3e976e-87d1-42ac-ac5c-c69d4a96bf84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768, 8, 8])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " x= conv(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9186878-f986-4d44-8f51-e98dfae0fb51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768, 64])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.flatten(2)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34e2306b-2ec5-4482-a66e-af521d9af062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.transpose(1,2)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0d0cf01-944b-4ec5-b2b1-62b12507c8ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5683f34-c810-4476-9a33-099ea1471f72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(27.7128)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sqrt(torch.tensor(768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea3038f5-90aa-49b6-8fcd-d1f02d537ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nihar\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\functional.py:1354: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.\n",
      "  warnings.warn(\"dropout2d: Received a 3D input to dropout2d and assuming that channel-wise \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
       "         [-1.1850,  2.0897, -0.2977,  ...,  0.4573, -0.8970,  1.1202],\n",
       "         ...,\n",
       "         [-0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
       "         [-0.6121, -0.3459, -1.9492,  ...,  0.4251, -0.6151,  0.2304],\n",
       "         [-0.0000,  0.0000, -0.0000,  ..., -0.0000,  0.0000, -0.0000]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = nn.Dropout2d(0.5)\n",
    "p(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598ae2bd-a93b-4796-bc79-d02dfe00a195",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/position-embeddings-for-vision-transformers-explained-a6f9add341d5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "503008ca-6a50-4207-9ed1-74f56aacdf6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.training\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89d347d8-3f2a-4dd2-bc73-8dc1efc9489c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nihar\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Create a tensor of shape [max_len, dim] for encoding\n",
    "        pe = torch.zeros(max_len, dim)\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)  # [max_len, 1]\n",
    "        div_term = torch.exp(torch.arange(0, dim, 2).float() * -(math.log(10000.0) / dim))  # [dim/2]\n",
    "\n",
    "        # Apply sin and cos functions for positional encoding\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # even indices (sine)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # odd indices (cosine)\n",
    "\n",
    "        pe = pe.unsqueeze(0)  # Shape becomes [1, max_len, dim]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to the input tensor\n",
    "        return x + self.pe[:, :x.size(1)]  # x.size(1) is the length of the sequence\n",
    "\n",
    "# Example usage for ViT\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, dim=768, num_classes=1000):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        self.dim = dim\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Calculate the number of patches\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        # Define the embedding layer for patches\n",
    "        self.patch_embeddings = nn.Conv2d(3, dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "        # Positional encoding\n",
    "        self.positional_encoding = PositionalEncoding(dim, max_len=self.num_patches)\n",
    "\n",
    "        # Transformer layers (simplified version)\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=dim, nhead=8),\n",
    "            num_layers=12\n",
    "        )\n",
    "\n",
    "        # Classification head\n",
    "        self.fc = nn.Linear(dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract patches and embed them\n",
    "        x = self.patch_embeddings(x)  # Shape: [batch_size, dim, num_patches, num_patches]\n",
    "        x = x.flatten(2).transpose(1, 2)  # Shape: [batch_size, num_patches, dim]\n",
    "\n",
    "        # Add positional encoding\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        # Pass through transformer encoder\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # Classification head (take the output of the [CLS] token)\n",
    "        x = x.mean(dim=1)  # Global average pooling\n",
    "\n",
    "        # Final classification layer\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Example input (batch_size=2, img_size=224)\n",
    "model = VisionTransformer(img_size=224, patch_size=16)\n",
    "sample_input = torch.randn(2, 3, 224, 224)  # Batch of 2 images with 3 channels (RGB)\n",
    "output = model(sample_input)\n",
    "print(output.shape)  # Should output: torch.Size([2, 1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d15ba41-7f2f-49ae-88ef-7a1c7ffbeb6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 768])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(5000, 768).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b790c537-1b53-49a8-93f0-197e2abecb01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = torch.arange(0, 5000).float().unsqueeze(1)\n",
    "position.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a3ceec-88a8-4f96-ac0c-865a157f9eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DilatedNeighborhoodAttention(nn.Module):\n",
    "    def __init__(self, dim, kernel_size=7, dilation=1, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation = dilation\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, H, W, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, H, W, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(3, 0, 4, 1, 2, 5)  # (3, B, num_heads, H, W, head_dim)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # Split into queries, keys, and values\n",
    "        \n",
    "        # Compute attention with dilation\n",
    "        attn_map = self.compute_dilated_attention(q, k, H, W)\n",
    "        attn_output = (attn_map @ v).permute(0, 2, 3, 1, 4).reshape(B, H, W, C)\n",
    "        return self.proj(attn_output)\n",
    "    \n",
    "    def compute_dilated_attention(self, q, k, H, W):\n",
    "        \"\"\"Computes dilated attention weights\"\"\"\n",
    "        pad = self.dilation * (self.kernel_size // 2)\n",
    "        k_padded = F.pad(k, (0, 0, pad, pad, pad, pad))\n",
    "        attn_weights = []\n",
    "        \n",
    "        for i in range(self.kernel_size):\n",
    "            for j in range(self.kernel_size):\n",
    "                k_slice = k_padded[:, :, i::self.dilation, j::self.dilation, :]\n",
    "                attn_weights.append((q * k_slice).sum(-1))\n",
    "        \n",
    "        attn_weights = torch.stack(attn_weights, dim=-1)\n",
    "        attn_weights = F.softmax(attn_weights * self.scale, dim=-1)\n",
    "        return attn_weights\n",
    "\n",
    "# Example Usage\n",
    "B, H, W, C = 1, 32, 32, 64  # Batch size, Height, Width, Channels\n",
    "x = torch.randn(B, H, W, C)\n",
    "attn_layer = DilatedNeighborhoodAttention(dim=C, kernel_size=7, dilation=2, num_heads=8)\n",
    "out = attn_layer(x)\n",
    "print(out.shape)  # Expected output: (B, H, W, C)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
