{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "429bf434-0d62-4ba0-b9f9-31a8dbcca3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eee704dc-4306-4477-9641-4eeb13655104",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"Split image into patches and then embed them.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_size : int\n",
    "        Size of the image (it is a square).\n",
    "\n",
    "    patch_size : int\n",
    "        Size of the patch (it is a square).\n",
    "\n",
    "    in_chans : int\n",
    "        Number of input channels.\n",
    "\n",
    "    embed_dim : int\n",
    "        The embedding dimension.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    n_patches : int\n",
    "        Number of patches inside of our image.\n",
    "\n",
    "    proj : nn.Conv2d\n",
    "        Convolutional layer that does both the splitting into patches and their embedding.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size, patch_size, in_channels = 3, embed_dim = 768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        self.proj = nn.Conv2d( in_channels, embed_dim, kernel_size = patch_size, stride = patch_size,)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Run forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape '(n_samples, in_chans, img_size, img_size)'.\n",
    "       \n",
    "        Returns\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Shape '(n_samples, n_patches, embed_dim)'.\n",
    "        \"\"\"\n",
    "        x = self.proj(x) # (n_samples, emed_dim, n_patches ** 0.5, n_patches ** 0.5)\n",
    "        x = x.flatten(2) # (n_samples, emed_dim, n_patches)\n",
    "        x = x.transpose(1,2) # n_samples, n_patches, embed_dim)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23118314-bfd5-44a2-9b8e-b5b87db8fd42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5476, 768])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1,3,224,224)\n",
    "\n",
    "model = PatchEmbed(img_size=224,patch_size=3,embed_dim=768,in_channels=3)\n",
    "x1 = model(x)\n",
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcc24274-b9a7-4d6e-8f1f-11a843efd331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2500, 1.2500, 1.2500, 1.2500, 0.0000],\n",
       "        [0.0000, 1.2500, 1.2500, 1.2500, 1.2500],\n",
       "        [1.2500, 1.2500, 0.0000, 1.2500, 1.2500]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module = nn.Dropout(p=0.2)\n",
    "inp = torch.ones(3,5)\n",
    "module(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4e44498-90b0-48b1-b5c1-91f369da1572",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\" Attention mechanism\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim : int\n",
    "        The input and out dimension of per token features.\n",
    "\n",
    "    n_heads: int\n",
    "        Number of attention heads.\n",
    "\n",
    "    qkv_bias : bool\n",
    "        If True, then we include bias to the query, key and value projections.\n",
    "\n",
    "    attn_p : float\n",
    "        Dropout probability applied to the query, key and value tensors.\n",
    "\n",
    "    proj_p : float\n",
    "        Dropout probability applied to the output tensor.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    scale : float\n",
    "        Normalizing constant for the dot product.\n",
    "    qkv : nn.Linear\n",
    "        Linear projection for the query, key and value.\n",
    "    proj : nn.Linear\n",
    "        Linear mapping that takes in the concatenated output of all attention\n",
    "        heads and maps it into a new space.\n",
    "    attn_drop, proj_drop : nn.Dropout\n",
    "        Dropout layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, n_heads=12, qkv_bias=True, attn_p=0., proj_p=0.):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim//n_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim*3, bias = qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_p)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Run forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: torch.Tensor\n",
    "            Shape '(n_samples, n_patches +1, dim)'.\n",
    "\n",
    "        Returns \n",
    "        ------\n",
    "        torch.Tensor\n",
    "            Shape '(n_samples, n_patches +1, dim)'.\n",
    "        \"\"\"\n",
    "        n_samples, n_tokens, dim = x.shape\n",
    " \n",
    "        if dim != self.dim:\n",
    "            raise ValueError\n",
    "            \n",
    "        qkv = self.qkv(x) # (n_samples, n_patches +1, 3 * dim)\n",
    "        qkv = qkv.reshape( n_samples, n_tokens, 3, self.n_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4) # (3, n_samples, n_heads, n_patches + 1,head_dim )\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        k_t = k.transpose(-2, -1) # (n_samples, n_heads, head_dim, n_patches +1)\n",
    "\n",
    "        dp = (q @ k_t) * self.scale\n",
    "\n",
    "        attn = dp.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        weighted_avg = attn @ v\n",
    "        weighted_avg = weighted_avg.transpose(1, 2)\n",
    "        weighted_avg = weighted_avg.flatten(2)\n",
    "\n",
    "        x = self.proj(weighted_avg)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3eb2377e-cec5-4f53-92b4-cbfc1a37c240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5476, 768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = Attention(768)\n",
    "model1(x1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8c6fc99-ffb4-4eec-9ea2-430c013dd751",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, p=0.):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.drop(self.act(self.fc1(x)))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "543fb994-eab1-4162-869b-53fe168e6273",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim : int\n",
    "        Embedding dimensions.\n",
    "\n",
    "    n_heads : int\n",
    "        number of attention heads.\n",
    "\n",
    "    mlp_ratio : float\n",
    "        Determines the hidden dimension size of the \"MLP' module with rewspect to 'dim'.\n",
    "\n",
    "    qkv_bias : bool\n",
    "        If True the we include bias to the query, key and value projections.\n",
    "\n",
    "    p, attn_p : float\n",
    "        Dropout probability\n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    norm1, norm2 : LayerNorm.\n",
    "        Layer normalization.\n",
    "\n",
    "    attn : Attention\n",
    "        Attention module.\n",
    "\n",
    "    mlp : MLP\n",
    "        MLP module.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, n_heads, mlp_ratio= 4.0, qkv_bias=True, p=0., attn_p = 0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.attn = Attention(dim,n_heads=n_heads, qkv_bias = qkv_bias,attn_p = attn_p, proj_p = p)\n",
    "        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        hidden_features = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(in_features=dim, hidden_features = hidden_features, out_features=dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\" Run forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape'(n_samples, n_patches + 1, dim)'\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        torch.Tensor\n",
    "            Shape'(n_samples, n_patches + 1, dim)'      \n",
    "        \"\"\"\n",
    "        x += self.attn(self.norm1(x))\n",
    "        x += self.mlp(self.norm2(x))\n",
    "\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f33f2fb-8669-48a7-8d34-76e7115588a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visiontransformer(nn.Module):\n",
    "    \"\"\"Simplified implementation of the Vision transformer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_size : int\n",
    "        both height and the width of the image (it is a square).\n",
    "\n",
    "    patch_size : int\n",
    "        both height and the width of the patch (it is a square).\n",
    "\n",
    "    in_chans : int\n",
    "        Number of input channels.\n",
    "        \n",
    "    n_classes: int\n",
    "        Number of classes\n",
    "        \n",
    "    embed_dim : int\n",
    "        Dimensionality of the token/patch embeddings.\n",
    "        \n",
    "    depth : int \n",
    "        Number of blocks\n",
    "        \n",
    "    n_heads : int\n",
    "        Number of attention heads.\n",
    "        \n",
    "    mlp_ratio : float\n",
    "        Determines the hidden dimension of the 'MLP' module.\n",
    "        \n",
    "    qkv_bias : bool\n",
    "        If True, we include bias to the query, key, and value projections.\n",
    "\n",
    "    p, attn_p : float\n",
    "        Dropout probability.\n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    patch_embed : PatchEmbed\n",
    "        Instance of 'PatchEmbed' layer.\n",
    "        \n",
    "    cls_token : nn.Parameter\n",
    "        Learnable parameter that will represent the first token in the sequence.\n",
    "        It has 'embed_dim' elements.\n",
    "\n",
    "    pos_emb : nn.Parameter\n",
    "        Positional embedding of the cls token + all the patches.\n",
    "        It has '(n+patches + 1) * embed_dim' elements.\n",
    "    pos_drop : nn.Dropout\n",
    "        Dropout layer.\n",
    "    blocks : nn.ModuleList\n",
    "        List of 'Block' modules.\n",
    "\n",
    "    norm : nn.LayerNorm\n",
    "        Layer normalization\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        img_size = 384,\n",
    "        patch_size =16,\n",
    "        in_chans = 3,\n",
    "        n_classes = 1000,\n",
    "        embed_dim =768,\n",
    "        depth =12,\n",
    "        n_heads =12,\n",
    "        mlp_ratio = 4.,\n",
    "        qkv_bias =True,\n",
    "        p=0.,\n",
    "        attn_p=0.,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbed(img_size= img_size, patch_size= patch_size, in_channels=in_chans, embed_dim=embed_dim)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1,1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1,1 + self.patch_embed.n_patches, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=p)\n",
    "        self.block = nn.ModuleList([Block(dim = embed_dim, \n",
    "                           n_heads = n_heads,\n",
    "                           mlp_ratio = mlp_ratio, \n",
    "                           qkv_bias = qkv_bias, \n",
    "                           p = p, \n",
    "                           attn_p = attn_p,\n",
    "                          )\n",
    "                          for _ in range(depth)\n",
    "                                   ])\n",
    "        self.norm = nn.LayerNorm(embed_dim, eps= 1e-6)\n",
    "        self.head = nn.Linear(embed_dim, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        n_samples = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "        cls_token = self.cls_token.expand(n_samples, -1, -1)\n",
    "        x = torch.cat((cls_token, x), dim =1)\n",
    "        x += self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for block in self.block:\n",
    "            x = block(x)\n",
    "        x = self.norm(x)\n",
    "        print(x.shape)\n",
    "        cls_token_final = x[:,0] # just the CLS token\n",
    "        print(cls_token_final.shape)\n",
    "        x = self.head(cls_token_final)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a11dfb6f-0c49-4c12-8484-e2b0e1653cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 577, 768])\n",
      "torch.Size([10, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Visiontransformer()\n",
    "x = torch.randn(10,3,384,384)\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "227699ad-d986-40ba-b6db-1675379d0c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(10, 577, 768)\n",
    "x[:,2,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb93f1a-33b1-4602-a2e4-86bd7b3b0de0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d295183-9086-406b-b965-93a334b189ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Parameter(torch.zeros(1,1, 768)).expand(10,-1,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be1daf44-a2ee-453e-a466-94a3bd3edb17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "576"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(384 // 16) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd998977-ab81-4bca-9e72-ccaab544a798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6ebc7d2-b7b7-4336-a6bb-6df23ca20ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bf743ce-53f3-4126-b98e-b4c7f0d16153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "#from timm.models.layers import DropPath\n",
    "import natten\n",
    "from natten import NeighborhoodAttention2D as NeighborhoodAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37747268-7b73-4f68-9a21-2f3d25c1d969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1+cu118'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89538c1f-9e0b-4806-8eb5-ebf23d955e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /root/miniconda3/envs/tf/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Thu Feb 20 11:34:58 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.46                 Driver Version: 531.61       CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3070 L...    On | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   52C    P0               32W /  N/A|      0MiB /  8192MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d777ba4-2275-46ad-ba16-64b678bb1a33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272cce59-0676-4bbb-863c-1accd7c539e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debfcc31-9a9b-4940-b52c-770ca192e261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ab14db-9f16-42df-96fb-d2082c19a509",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a51eb0-0d99-46b1-9a03-1408ca9acac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7f4fed-a5a4-4d5a-b863-627137ed10e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc5e012-ecc0-4825-a8df-88465d308007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753e3b98-9b55-4ed6-aa54-54064d6ae973",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbe6d0cf-e4f9-47c1-a557-16b787fd1f28",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "NeighborhoodAttention2D expected a rank-4 input tensor; got x.dim()=3.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 58\u001b[0m\n\u001b[1;32m     56\u001b[0m model \u001b[38;5;241m=\u001b[39m DiNAT()\n\u001b[1;32m     57\u001b[0m img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Should be (1, 1000)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[9], line 51\u001b[0m, in \u001b[0;36mDiNAT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     49\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_embed(x)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 51\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[9], line 22\u001b[0m, in \u001b[0;36mNATransformerLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 22\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     23\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x)))\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/natten/na2d.py:96\u001b[0m, in \u001b[0;36mNeighborhoodAttention2D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m---> 96\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeighborhoodAttention2D expected a rank-4 input tensor; got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m         )\n\u001b[1;32m    100\u001b[0m     B, H, W, C \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_fna_enabled():\n",
      "\u001b[0;31mValueError\u001b[0m: NeighborhoodAttention2D expected a rank-4 input tensor; got x.dim()=3."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "#from timm.models.layers import DropPath\n",
    "import natten\n",
    "from natten import NeighborhoodAttention2D as NeighborhoodAttention\n",
    "\n",
    "class NATransformerLayer(nn.Module):\n",
    "    \"\"\"Neighborhood Attention Transformer Layer with MLP.\"\"\"\n",
    "    def __init__(self, dim, num_heads, kernel_size=7, dilation=1, drop_path=0.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = NeighborhoodAttention(dim, kernel_size, dilation, num_heads)\n",
    "        #self.drop_path = DropPath(drop_path) if drop_path > 0 else nn.Identity()\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, 4 * dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * dim, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.attn(self.norm1(x)))\n",
    "        x = x + (self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"Converts input image into patches.\"\"\"\n",
    "    def __init__(self, patch_size=4, in_chans=3, embed_dim=96):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return self.norm(x)\n",
    "\n",
    "class DiNAT(nn.Module):\n",
    "    \"\"\"Simplified DiNAT model.\"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=4, embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24]):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbed(patch_size, 3, embed_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            NATransformerLayer(embed_dim * (2**i), num_heads[i]) for i in range(len(depths))\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim * (2**(len(depths) - 1)))\n",
    "        self.head = nn.Linear(embed_dim * (2**(len(depths) - 1)), 1000)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.norm(x.mean(dim=1))\n",
    "        return self.head(x)\n",
    "\n",
    "# Example usage\n",
    "model = DiNAT()\n",
    "img = torch.randn(1, 3, 224, 224)\n",
    "output = model(img)\n",
    "print(output.shape)  # Should be (1, 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89d347d8-3f2a-4dd2-bc73-8dc1efc9489c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nihar\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Create a tensor of shape [max_len, dim] for encoding\n",
    "        pe = torch.zeros(max_len, dim)\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)  # [max_len, 1]\n",
    "        div_term = torch.exp(torch.arange(0, dim, 2).float() * -(math.log(10000.0) / dim))  # [dim/2]\n",
    "\n",
    "        # Apply sin and cos functions for positional encoding\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # even indices (sine)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # odd indices (cosine)\n",
    "\n",
    "        pe = pe.unsqueeze(0)  # Shape becomes [1, max_len, dim]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to the input tensor\n",
    "        return x + self.pe[:, :x.size(1)]  # x.size(1) is the length of the sequence\n",
    "\n",
    "# Example usage for ViT\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, dim=768, num_classes=1000):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        self.dim = dim\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Calculate the number of patches\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        # Define the embedding layer for patches\n",
    "        self.patch_embeddings = nn.Conv2d(3, dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "        # Positional encoding\n",
    "        self.positional_encoding = PositionalEncoding(dim, max_len=self.num_patches)\n",
    "\n",
    "        # Transformer layers (simplified version)\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=dim, nhead=8),\n",
    "            num_layers=12\n",
    "        )\n",
    "\n",
    "        # Classification head\n",
    "        self.fc = nn.Linear(dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract patches and embed them\n",
    "        x = self.patch_embeddings(x)  # Shape: [batch_size, dim, num_patches, num_patches]\n",
    "        x = x.flatten(2).transpose(1, 2)  # Shape: [batch_size, num_patches, dim]\n",
    "\n",
    "        # Add positional encoding\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        # Pass through transformer encoder\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # Classification head (take the output of the [CLS] token)\n",
    "        x = x.mean(dim=1)  # Global average pooling\n",
    "\n",
    "        # Final classification layer\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Example input (batch_size=2, img_size=224)\n",
    "model = VisionTransformer(img_size=224, patch_size=16)\n",
    "sample_input = torch.randn(2, 3, 224, 224)  # Batch of 2 images with 3 channels (RGB)\n",
    "output = model(sample_input)\n",
    "print(output.shape)  # Should output: torch.Size([2, 1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d15ba41-7f2f-49ae-88ef-7a1c7ffbeb6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(5000, 768).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b790c537-1b53-49a8-93f0-197e2abecb01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = torch.arange(0, 5000).float().unsqueeze(1)\n",
    "position.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5a3ceec-88a8-4f96-ac0c-865a157f9eff",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (22) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(B, H, W, C)\n\u001b[1;32m     47\u001b[0m attn_layer \u001b[38;5;241m=\u001b[39m DilatedNeighborhoodAttention(dim\u001b[38;5;241m=\u001b[39mC, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m, dilation\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mattn_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(out\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Expected output: (B, H, W, C)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[7], line 25\u001b[0m, in \u001b[0;36mDilatedNeighborhoodAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m qkv[\u001b[38;5;241m0\u001b[39m], qkv[\u001b[38;5;241m1\u001b[39m], qkv[\u001b[38;5;241m2\u001b[39m]  \u001b[38;5;66;03m# Split into queries, keys, and values\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Compute attention with dilation\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m attn_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_dilated_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m (attn_map \u001b[38;5;241m@\u001b[39m v)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B, H, W, C)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(attn_output)\n",
      "Cell \u001b[0;32mIn[7], line 38\u001b[0m, in \u001b[0;36mDilatedNeighborhoodAttention.compute_dilated_attention\u001b[0;34m(self, q, k, H, W)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size):\n\u001b[1;32m     37\u001b[0m         k_slice \u001b[38;5;241m=\u001b[39m k_padded[:, :, i::\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, j::\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, :]\n\u001b[0;32m---> 38\u001b[0m         attn_weights\u001b[38;5;241m.\u001b[39mappend((\u001b[43mq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk_slice\u001b[49m)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     40\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(attn_weights, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     41\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(attn_weights \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (22) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DilatedNeighborhoodAttention(nn.Module):\n",
    "    def __init__(self, dim, kernel_size=7, dilation=1, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation = dilation\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, H, W, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, H, W, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(3, 0, 4, 1, 2, 5)  # (3, B, num_heads, H, W, head_dim)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # Split into queries, keys, and values\n",
    "        \n",
    "        # Compute attention with dilation\n",
    "        attn_map = self.compute_dilated_attention(q, k, H, W)\n",
    "        attn_output = (attn_map @ v).permute(0, 2, 3, 1, 4).reshape(B, H, W, C)\n",
    "        return self.proj(attn_output)\n",
    "    \n",
    "    def compute_dilated_attention(self, q, k, H, W):\n",
    "        \"\"\"Computes dilated attention weights\"\"\"\n",
    "        pad = self.dilation * (self.kernel_size // 2)\n",
    "        k_padded = F.pad(k, (0, 0, pad, pad, pad, pad))\n",
    "        attn_weights = []\n",
    "        \n",
    "        for i in range(self.kernel_size):\n",
    "            for j in range(self.kernel_size):\n",
    "                k_slice = k_padded[:, :, i::self.dilation, j::self.dilation, :]\n",
    "                attn_weights.append((q * k_slice).sum(-1))\n",
    "        \n",
    "        attn_weights = torch.stack(attn_weights, dim=-1)\n",
    "        attn_weights = F.softmax(attn_weights * self.scale, dim=-1)\n",
    "        return attn_weights\n",
    "\n",
    "# Example Usage\n",
    "B, H, W, C = 1, 32, 32, 64  # Batch size, Height, Width, Channels\n",
    "x = torch.randn(B, H, W, C)\n",
    "attn_layer = DilatedNeighborhoodAttention(dim=C, kernel_size=7, dilation=2, num_heads=8)\n",
    "out = attn_layer(x)\n",
    "print(out.shape)  # Expected output: (B, H, W, C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35974363-2516-4724-ac41-8a0e84f054b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [-1,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c01634a1-280c-4a84-81f3-42a6bff6b04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting timm\n",
      "  Downloading timm-1.0.14-py3-none-any.whl.metadata (50 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from timm) (2.2.0+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from timm) (0.17.0+cu118)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from timm) (6.0.1)\n",
      "Collecting huggingface_hub (from timm)\n",
      "  Downloading huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting safetensors (from timm)\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from huggingface_hub->timm) (3.9.0)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface_hub->timm)\n",
      "  Downloading fsspec-2025.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from huggingface_hub->timm) (23.1)\n",
      "Requirement already satisfied: requests in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from huggingface_hub->timm) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from huggingface_hub->timm) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from huggingface_hub->timm) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from torch->timm) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from torch->timm) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from torch->timm) (3.1.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from torchvision->timm) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from torchvision->timm) (10.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub->timm) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from jinja2->torch->timm) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from requests->huggingface_hub->timm) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from requests->huggingface_hub->timm) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from requests->huggingface_hub->timm) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from requests->huggingface_hub->timm) (2024.12.14)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\nihar\\anaconda3\\envs\\tf\\lib\\site-packages (from sympy->torch->timm) (1.3.0)\n",
      "Downloading timm-1.0.14-py3-none-any.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 67.5 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.28.1-py3-none-any.whl (464 kB)\n",
      "Downloading safetensors-0.5.2-cp38-abi3-win_amd64.whl (303 kB)\n",
      "Downloading fsspec-2025.2.0-py3-none-any.whl (184 kB)\n",
      "Installing collected packages: safetensors, fsspec, huggingface_hub, timm\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "Successfully installed fsspec-2025.2.0 huggingface_hub-0.28.1 safetensors-0.5.2 timm-1.0.14\n"
     ]
    }
   ],
   "source": [
    "pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b6e0c85-d7e8-4b65-b627-1d4ca58f75cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3743640526.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip3 install natten==0.15.1+torch220cu121 -f https://shi-labs.com/natten/wheels/\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip3 install natten==0.15.1+torch220cu121 -f https://shi-labs.com/natten/wheels/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44ba3f84-62d7-43a0-b5d1-2897c4b39ea4",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (8) must match the size of tensor b (20) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 48\u001b[0m\n\u001b[0;32m     46\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(B, H, W, C)\n\u001b[0;32m     47\u001b[0m attn_layer \u001b[38;5;241m=\u001b[39m DilatedNeighborhoodAttention(dim\u001b[38;5;241m=\u001b[39mC, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m, dilation\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m---> 48\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mattn_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(out\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[14], line 25\u001b[0m, in \u001b[0;36mDilatedNeighborhoodAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     22\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m qkv[\u001b[38;5;241m0\u001b[39m], qkv[\u001b[38;5;241m1\u001b[39m], qkv[\u001b[38;5;241m2\u001b[39m]  \u001b[38;5;66;03m# Split into queries, keys, and values\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Compute attention with dilation\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m attn_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_dilated_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m (attn_map \u001b[38;5;241m@\u001b[39m v)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B, H, W, C)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(attn_output)\n",
      "Cell \u001b[1;32mIn[14], line 38\u001b[0m, in \u001b[0;36mDilatedNeighborhoodAttention.compute_dilated_attention\u001b[1;34m(self, q, k, H, W)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size):\n\u001b[0;32m     37\u001b[0m         k_slice \u001b[38;5;241m=\u001b[39m k_padded[:, :, i:i\u001b[38;5;241m+\u001b[39mH, j:j\u001b[38;5;241m+\u001b[39mW, :]  \u001b[38;5;66;03m# Extract neighborhood\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m         attn_weights\u001b[38;5;241m.\u001b[39mappend((\u001b[43mq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk_slice\u001b[49m)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     40\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(attn_weights, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (B, num_heads, H, W, kernel_size**2)\u001b[39;00m\n\u001b[0;32m     41\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(attn_weights \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Apply softmax\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (8) must match the size of tensor b (20) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DilatedNeighborhoodAttention(nn.Module):\n",
    "    def __init__(self, dim, kernel_size=7, dilation=1, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation = dilation\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, H, W, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, H, W, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(3, 0, 4, 1, 2, 5)  # (3, B, num_heads, H, W, head_dim)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # Split into queries, keys, and values\n",
    "\n",
    "        # Compute attention with dilation\n",
    "        attn_map = self.compute_dilated_attention(q, k, H, W)\n",
    "        attn_output = (attn_map @ v).permute(0, 2, 3, 1, 4).reshape(B, H, W, C)\n",
    "        return self.proj(attn_output)\n",
    "\n",
    "    def compute_dilated_attention(self, q, k, H, W):\n",
    "        \"\"\"Computes dilated attention weights\"\"\"\n",
    "        pad = self.dilation * (self.kernel_size // 2)\n",
    "        k_padded = F.pad(k, (0, 0, 0, 0, pad, pad, pad, pad))  # Correct padding order\n",
    "\n",
    "        attn_weights = []\n",
    "        for i in range(self.kernel_size):\n",
    "            for j in range(self.kernel_size):\n",
    "                k_slice = k_padded[:, :, i:i+H, j:j+W, :]  # Extract neighborhood\n",
    "                attn_weights.append((q * k_slice).sum(-1))\n",
    "\n",
    "        attn_weights = torch.stack(attn_weights, dim=-1)  # (B, num_heads, H, W, kernel_size**2)\n",
    "        attn_weights = F.softmax(attn_weights * self.scale, dim=-1)  # Apply softmax\n",
    "        return attn_weights.view(q.shape[0], q.shape[1], q.shape[2], self.kernel_size, self.kernel_size)\n",
    "\n",
    "# Example Usage\n",
    "B, H, W, C = 1, 32, 32, 64  # Batch size, Height, Width, Channels\n",
    "x = torch.randn(B, H, W, C)\n",
    "attn_layer = DilatedNeighborhoodAttention(dim=C, kernel_size=7, dilation=2, num_heads=8)\n",
    "out = attn_layer(x)\n",
    "print(out.shape)  # Expected output: (B, H,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b301ce0-5315-4051-9345-690fb8127c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Input: (Batch, Channels, Height, Width)\n",
    "x = torch.rand(1,3,224,224)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "645110b2-c460-44a5-a740-4b024626da6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 147, 30976])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "un_fold = nn.Unfold(kernel_size= 7, dilation=8, padding=0, stride=1)\n",
    "un_fold(x).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69c4b5fc-ce68-41f7-9736-88786125c6f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8289, 0.2952],\n",
       "        [0.4193, 0.6381]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2,2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b41da26-748b-47be-bb20-57f974386782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8289, 0.2952, 0.4193, 0.6381])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6664deb-3e7a-483f-b8a3-4a8942311330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /root/miniconda3/envs/tf/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Collecting timm\n",
      "  Downloading timm-1.0.14-py3-none-any.whl.metadata (50 kB)\n",
      "Requirement already satisfied: torch in /root/miniconda3/envs/tf/lib/python3.9/site-packages (from timm) (2.0.1+cu118)\n",
      "Requirement already satisfied: torchvision in /root/miniconda3/envs/tf/lib/python3.9/site-packages (from timm) (0.15.2+cu118)\n",
      "Requirement already satisfied: pyyaml in /root/miniconda3/envs/tf/lib/python3.9/site-packages (from timm) (6.0.1)\n",
      "Collecting huggingface_hub (from timm)\n",
      "  Downloading huggingface_hub-0.29.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting safetensors (from timm)\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/envs/tf/lib/python3.9/site-packages (from huggingface_hub->timm) (3.12.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /root/miniconda3/envs/tf/lib/python3.9/site-packages (from huggingface_hub->timm) (2023.12.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /root/miniconda3/envs/tf/lib/python3.9/site-packages (from huggingface_hub->timm) (23.1)\n",
      "Requirement already satisfied: requests in /root/miniconda3/envs/tf/lib/python3.9/site-packages (from huggingface_hub->timm) (2.30.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /root/miniconda3/envs/tf/lib/python3.9/site-packages (from huggingface_hub->timm) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/envs/tf/lib/python3.9/site-packages (from huggingface_hub->timm) (4.5.0)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/envs/tf/lib/python3.9/site-packages (from torch->timm) (1.11.1)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/envs/tf/lib/python3.9/site-packages (from torch->timm) (3.1)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/envs/tf/lib/python3.9/site-packages (from torch->timm) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /root/miniconda3/envs/tf/lib/python3.9/site-packages (from torch->timm) (2.0.0)\n",
      "Requirement already satisfied: cmake in /root/miniconda3/envs/tf/lib/python3.9/site-packages (from triton==2.0.0->torch->timm) (3.20.3)\n",
      "Requirement already satisfied: lit in /root/miniconda3/envs/tf/lib/python3.9/site-packages (from triton==2.0.0->torch->timm) (15.0.7)\n",
      "Requirement already satisfied: numpy in /root/miniconda3/envs/tf/lib/python3.9/site-packages (from torchvision->timm) (1.24.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /root/miniconda3/envs/tf/lib/python3.9/site-packages (from torchvision->timm) (9.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/envs/tf/lib/python3.9/site-packages (from jinja2->torch->timm) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/envs/tf/lib/python3.9/site-packages (from requests->huggingface_hub->timm) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/tf/lib/python3.9/site-packages (from requests->huggingface_hub->timm) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/envs/tf/lib/python3.9/site-packages (from requests->huggingface_hub->timm) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/envs/tf/lib/python3.9/site-packages (from requests->huggingface_hub->timm) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /root/miniconda3/envs/tf/lib/python3.9/site-packages (from sympy->torch->timm) (1.3.0)\n",
      "Downloading timm-1.0.14-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.29.0-py3-none-any.whl (468 kB)\n",
      "Downloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (461 kB)\n",
      "Installing collected packages: safetensors, huggingface_hub, timm\n",
      "Successfully installed huggingface_hub-0.29.0 safetensors-0.5.2 timm-1.0.14\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8140674c-8fbe-4674-8780-99d5b3c0d628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import natten\n",
    "from natten import NeighborhoodAttention2D as NeighborhoodAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d161125a-a0f9-4c50-8774-a643e5536089",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = NeighborhoodAttention(\n",
    "            64,\n",
    "            kernel_size=7,\n",
    "            dilation=2,\n",
    "            num_heads=12,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aceb27fb-2abd-4bf4-876c-d512aace6cd5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (143360x224 and 64x192)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m64\u001b[39m,\u001b[38;5;241m224\u001b[39m,\u001b[38;5;241m224\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/natten/na2d.py:135\u001b[0m, in \u001b[0;36mNeighborhoodAttention2D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_experimental_ops:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly fused NA is included in experimental support for torch.compile and torch\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms FLOP counter.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m     )\n\u001b[1;32m    134\u001b[0m qkv \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqkv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;241m.\u001b[39mreshape(B, H, W, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m    138\u001b[0m )\n\u001b[1;32m    139\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m qkv[\u001b[38;5;241m0\u001b[39m], qkv[\u001b[38;5;241m1\u001b[39m], qkv[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    140\u001b[0m q \u001b[38;5;241m=\u001b[39m q \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (143360x224 and 64x192)"
     ]
    }
   ],
   "source": [
    "x = torch.rand(10,64,224,224)\n",
    "attn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3baf4250-5257-40e3-89bd-f9abc5a3cef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/tf/lib/python3.9/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnatten\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NeighborhoodAttention2D \u001b[38;5;28;01mas\u001b[39;00m NeighborhoodAttention\n\u001b[1;32m     17\u001b[0m is_natten_post_017 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mhasattr\u001b[39m(natten, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Mlp\n\u001b[1;32m     21\u001b[0m model_urls \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# ImageNet-1K\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdinat_s_tiny_1k\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://shi-labs.com/projects/dinat/checkpoints/imagenet1k/dinat_s_tiny_in1k_224.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdinat_s_large_21k\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://shi-labs.com/projects/dinat/checkpoints/imagenet22k/dinat_s_large_in22k_224.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     30\u001b[0m }\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mNATransformerLayer\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nat'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Dilated Neighborhood Attention Transformer.\n",
    "https://arxiv.org/abs/2209.15001\n",
    "\n",
    "DiNAT_s -- our alternative model.\n",
    "\n",
    "This source code is licensed under the license found in the\n",
    "LICENSE file in the root directory of this source tree.\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import pad\n",
    "from timm.models.layers import trunc_normal_, DropPath, to_2tuple\n",
    "from timm.models.registry import register_model\n",
    "import natten\n",
    "from natten import NeighborhoodAttention2D as NeighborhoodAttention\n",
    "is_natten_post_017 = hasattr(natten, \"context\")\n",
    "\n",
    "from nat import Mlp\n",
    "\n",
    "model_urls = {\n",
    "    # ImageNet-1K\n",
    "    \"dinat_s_tiny_1k\": \"https://shi-labs.com/projects/dinat/checkpoints/imagenet1k/dinat_s_tiny_in1k_224.pth\",\n",
    "    \"dinat_s_small_1k\": \"https://shi-labs.com/projects/dinat/checkpoints/imagenet1k/dinat_s_small_in1k_224.pth\",\n",
    "    \"dinat_s_base_1k\": \"https://shi-labs.com/projects/dinat/checkpoints/imagenet1k/dinat_s_base_in1k_224.pth\",\n",
    "    \"dinat_s_large_1k\": \"https://shi-labs.com/projects/dinat/checkpoints/imagenet1k/dinat_s_large_in1k_224.pth\",\n",
    "    \"dinat_s_large_1k_384\": \"https://shi-labs.com/projects/dinat/checkpoints/imagenet1k/dinat_s_large_in1k_384.pth\",\n",
    "    # ImageNet-22K\n",
    "    \"dinat_s_large_21k\": \"https://shi-labs.com/projects/dinat/checkpoints/imagenet22k/dinat_s_large_in22k_224.pth\",\n",
    "}\n",
    "\n",
    "\n",
    "class NATransformerLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_heads,\n",
    "        kernel_size=7,\n",
    "        dilation=1,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop=0.0,\n",
    "        attn_drop=0.0,\n",
    "        drop_path=0.0,\n",
    "        act_layer=nn.GELU,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        extra_args = {\"rel_pos_bias\": True} if is_natten_post_017 else {\"bias\": True}\n",
    "        self.attn = NeighborhoodAttention(\n",
    "            dim,\n",
    "            kernel_size=kernel_size,\n",
    "            dilation=dilation,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            qk_scale=qk_scale,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=drop,\n",
    "            **extra_args,\n",
    "        )\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(\n",
    "            in_features=dim,\n",
    "            hidden_features=mlp_hidden_dim,\n",
    "            act_layer=act_layer,\n",
    "            drop=drop,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attn(x)\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchMerging(nn.Module):\n",
    "    \"\"\"\n",
    "    Based on Swin Transformer\n",
    "    https://arxiv.org/abs/2103.14030\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, H, W, C = x.shape\n",
    "\n",
    "        # padding\n",
    "        pad_input = (H % 2 == 1) or (W % 2 == 1)\n",
    "        if pad_input:\n",
    "            x = pad(x, (0, 0, 0, W % 2, 0, H % 2))\n",
    "            _, H, W, _ = x.shape\n",
    "\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "        x = x.view(B, (H + 1) // 2, (W + 1) // 2, 4 * C)  # B H/2 W/2 4*C\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasicLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Based on Swin Transformer\n",
    "    https://arxiv.org/abs/2103.14030\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        depth,\n",
    "        num_heads,\n",
    "        kernel_size,\n",
    "        dilations=None,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop=0.0,\n",
    "        attn_drop=0.0,\n",
    "        drop_path=0.0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        downsample=None,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.depth = depth\n",
    "\n",
    "        # build blocks\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                NATransformerLayer(\n",
    "                    dim=dim,\n",
    "                    num_heads=num_heads,\n",
    "                    kernel_size=kernel_size,\n",
    "                    dilation=1 if dilations is None else dilations[i],\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    qk_scale=qk_scale,\n",
    "                    drop=drop,\n",
    "                    attn_drop=attn_drop,\n",
    "                    drop_path=drop_path[i]\n",
    "                    if isinstance(drop_path, list)\n",
    "                    else drop_path,\n",
    "                    norm_layer=norm_layer,\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # patch merging layer\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(dim=dim, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    From Swin Transformer\n",
    "    https://arxiv.org/abs/2103.14030\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        self.patch_size = to_2tuple(patch_size)\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_chans, embed_dim, kernel_size=self.patch_size, stride=self.patch_size\n",
    "        )\n",
    "        self.norm = None if norm_layer is None else norm_layer(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        if W % self.patch_size[1] != 0:\n",
    "            x = pad(x, (0, self.patch_size[1] - W % self.patch_size[1]))\n",
    "        if H % self.patch_size[0] != 0:\n",
    "            x = pad(x, (0, 0, 0, self.patch_size[0] - H % self.patch_size[0]))\n",
    "\n",
    "        x = self.proj(x)\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DiNAT_s(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        patch_size=4,\n",
    "        in_chans=3,\n",
    "        num_classes=1000,\n",
    "        embed_dim=96,\n",
    "        depths=[2, 2, 6, 2],\n",
    "        num_heads=[3, 6, 12, 24],\n",
    "        kernel_size=7,\n",
    "        dilations=None,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        drop_path_rate=0.2,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        patch_norm=True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.patch_norm = patch_norm\n",
    "        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        # split image into non-overlapping patches\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            patch_size=patch_size,\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None,\n",
    "        )\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # stochastic depth\n",
    "        dpr = [\n",
    "            x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))\n",
    "        ]  # stochastic depth decay rule\n",
    "\n",
    "        # build layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = BasicLayer(\n",
    "                dim=int(embed_dim * 2**i_layer),\n",
    "                depth=depths[i_layer],\n",
    "                num_heads=num_heads[i_layer],\n",
    "                kernel_size=kernel_size,\n",
    "                dilations=None if dilations is None else dilations[i_layer],\n",
    "                mlp_ratio=self.mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                qk_scale=qk_scale,\n",
    "                drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate,\n",
    "                drop_path=dpr[sum(depths[:i_layer]) : sum(depths[: i_layer + 1])],\n",
    "                norm_layer=norm_layer,\n",
    "                downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.norm = norm_layer(self.num_features)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.head = (\n",
    "            nn.Linear(self.num_features, num_classes)\n",
    "            if num_classes > 0\n",
    "            else nn.Identity()\n",
    "        )\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=0.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {\"rpb\"}\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.norm(x).flatten(1, 2)\n",
    "        x = self.avgpool(x.transpose(1, 2))\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "@register_model\n",
    "def dinat_s_tiny(pretrained=False, **kwargs):\n",
    "    model = DiNAT_s(\n",
    "        depths=[2, 2, 6, 2],\n",
    "        num_heads=[3, 6, 12, 24],\n",
    "        embed_dim=96,\n",
    "        mlp_ratio=4,\n",
    "        drop_path_rate=0.2,\n",
    "        kernel_size=7,\n",
    "        dilations=[\n",
    "            [1, 8],\n",
    "            [1, 4],\n",
    "            [1, 2, 1, 2, 1, 2],\n",
    "            [1, 1],\n",
    "        ],\n",
    "        **kwargs\n",
    "    )\n",
    "    if pretrained:\n",
    "        url = model_urls[\"dinat_s_tiny_1k\"]\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\")\n",
    "        model.load_state_dict(checkpoint)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def dinat_s_small(pretrained=False, **kwargs):\n",
    "    model = DiNAT_s(\n",
    "        depths=[2, 2, 18, 2],\n",
    "        num_heads=[3, 6, 12, 24],\n",
    "        embed_dim=96,\n",
    "        mlp_ratio=4,\n",
    "        drop_path_rate=0.3,\n",
    "        kernel_size=7,\n",
    "        dilations=[\n",
    "            [1, 8],\n",
    "            [1, 4],\n",
    "            [1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2],\n",
    "            [1, 1],\n",
    "        ],\n",
    "        **kwargs\n",
    "    )\n",
    "    if pretrained:\n",
    "        url = model_urls[\"dinat_s_small_1k\"]\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\")\n",
    "        model.load_state_dict(checkpoint)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def dinat_s_base(pretrained=False, **kwargs):\n",
    "    model = DiNAT_s(\n",
    "        depths=[2, 2, 18, 2],\n",
    "        num_heads=[4, 8, 16, 32],\n",
    "        embed_dim=128,\n",
    "        mlp_ratio=4,\n",
    "        drop_path_rate=0.5,\n",
    "        kernel_size=7,\n",
    "        dilations=[\n",
    "            [1, 8],\n",
    "            [1, 4],\n",
    "            [1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2],\n",
    "            [1, 1],\n",
    "        ],\n",
    "        **kwargs\n",
    "    )\n",
    "    if pretrained:\n",
    "        url = model_urls[\"dinat_s_base_1k\"]\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\")\n",
    "        model.load_state_dict(checkpoint)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def dinat_s_large(pretrained=False, **kwargs):\n",
    "    model = DiNAT_s(\n",
    "        depths=[2, 2, 18, 2],\n",
    "        num_heads=[6, 12, 24, 48],\n",
    "        embed_dim=192,\n",
    "        mlp_ratio=4,\n",
    "        drop_path_rate=0.35,\n",
    "        kernel_size=7,\n",
    "        dilations=[\n",
    "            [1, 8],\n",
    "            [1, 4],\n",
    "            [1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2],\n",
    "            [1, 1],\n",
    "        ],\n",
    "        **kwargs\n",
    "    )\n",
    "    if pretrained:\n",
    "        url = model_urls[\"dinat_s_large_1k\"]\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\")\n",
    "        model.load_state_dict(checkpoint)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def dinat_s_large_384(pretrained=False, **kwargs):\n",
    "    model = DiNAT_s(\n",
    "        depths=[2, 2, 18, 2],\n",
    "        num_heads=[6, 12, 24, 48],\n",
    "        embed_dim=192,\n",
    "        mlp_ratio=4,\n",
    "        drop_path_rate=0.35,\n",
    "        kernel_size=7,\n",
    "        dilations=[\n",
    "            [1, 13],\n",
    "            [1, 6],\n",
    "            [1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3],\n",
    "            [1, 1],\n",
    "        ],\n",
    "        **kwargs\n",
    "    )\n",
    "    if pretrained:\n",
    "        url = model_urls[\"dinat_s_large_1k_384\"]\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\")\n",
    "        model.load_state_dict(checkpoint)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def dinat_s_large_21k(pretrained=False, **kwargs):\n",
    "    model = DiNAT_s(\n",
    "        depths=[2, 2, 18, 2],\n",
    "        num_heads=[6, 12, 24, 48],\n",
    "        embed_dim=192,\n",
    "        mlp_ratio=4,\n",
    "        drop_path_rate=0.2,\n",
    "        kernel_size=7,\n",
    "        dilations=[\n",
    "            [1, 8],\n",
    "            [1, 4],\n",
    "            [1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2],\n",
    "            [1, 1],\n",
    "        ],\n",
    "        **kwargs\n",
    "    )\n",
    "    if pretrained:\n",
    "        url = model_urls[\"dinat_s_large_21k\"]\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\")\n",
    "        model.load_state_dict(checkpoint)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb2d254-ec15-4eb8-bc59-ffee4d401f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NATransformerLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_heads,\n",
    "        kernel_size=7,\n",
    "        dilation=1,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop=0.0,\n",
    "        attn_drop=0.0,\n",
    "        act_layer=nn.GELU,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        extra_args = {\"rel_pos_bias\": True} if is_natten_post_017 else {\"bias\": True}\n",
    "        self.attn = NeighborhoodAttention(\n",
    "            dim,\n",
    "            kernel_size=kernel_size,\n",
    "            dilation=dilation,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            qk_scale=qk_scale,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=drop,\n",
    "            **extra_args,\n",
    "        )\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(\n",
    "            in_features=dim,\n",
    "            hidden_features=mlp_hidden_dim,\n",
    "            act_layer=act_layer,\n",
    "            drop=drop,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attn(x)\n",
    "        x =  \n",
    "        x = shortcut + (self.mlp(self.norm2(x)))\n",
    "        return x\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
