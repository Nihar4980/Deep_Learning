{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "429bf434-0d62-4ba0-b9f9-31a8dbcca3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eee704dc-4306-4477-9641-4eeb13655104",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"Split image into patches and then embed them.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_size : int\n",
    "        Size of the image (it is a square).\n",
    "\n",
    "    patch_size : int\n",
    "        Size of the patch (it is a square).\n",
    "\n",
    "    in_chans : int\n",
    "        Number of input channels.\n",
    "\n",
    "    embed_dim : int\n",
    "        The embedding dimension.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    n_patches : int\n",
    "        Number of patches inside of our image.\n",
    "\n",
    "    proj : nn.Conv2d\n",
    "        Convolutional layer that does both the splitting into patches and their embedding.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size, patch_size, in_channels = 3, embed_dim = 768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels, embed_dim, kernel_size = patch_size, stride = patch_size,)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Run forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape '(n_samples, in_chans, img_size, img_size)'.\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Shape '(n_samples, n_patches, embed_dim)'.\n",
    "        \"\"\"\n",
    "        x = self.proj(x) # (n_samples, emed_dim, n_patches ** 0.5, n_patches ** 0.5)\n",
    "       # print(x.shape)\n",
    "        x = x.flatten(2) # (n_samples, emed_dim, n_patches)\n",
    "       # print(x.shape)\n",
    "        x = x.transpose(1,2) # n_samples, n_patches, embed_dim)\n",
    "        #print(x.shape)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23118314-bfd5-44a2-9b8e-b5b87db8fd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1,3,224,224)\n",
    "\n",
    "\n",
    "model = PatchEmbed(img_size=224,patch_size=3,embed_dim=768,in_channels=3)\n",
    "x1 =model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f559cc7-d7fc-4093-9ad2-cfc7b4e89bb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4e44498-90b0-48b1-b5c1-91f369da1572",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\" Attention mechanism\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim : int\n",
    "        The input and out dimension of per token features.\n",
    "\n",
    "    n_heads: int\n",
    "        Number of attention heads.\n",
    "\n",
    "    qkv_bias : bool\n",
    "        If True, then we include bias to the query, key and value projections.\n",
    "\n",
    "    attn_p : float\n",
    "        Dropout probability applied to the query, key and value tensors.\n",
    "\n",
    "    proj_p : float\n",
    "        Dropout probability applied to the output tensor.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, n_heads=12, qkv_bias=True, attn_p=0., proj_p=0.):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim//n_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim*3, bias = qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_p)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Run forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: torch.Tensor\n",
    "            Shape '(n_samples, n_patches +1, dim)'.\n",
    "\n",
    "        Returns \n",
    "        ------\n",
    "        torch.Tensor\n",
    "            Shape '(n_samples, n_patches +1, dim)'.\n",
    "        \"\"\"\n",
    "        n_samples, n_tokens, dim = x.shape\n",
    "        print(x.shape)\n",
    "\n",
    "        if dim != self.dim:\n",
    "            raise ValueError\n",
    "\n",
    "        qkv = self.qkv(x) # (n_samples, n_patches +1, 3 * dim)\n",
    "        print(qkv.shape)\n",
    "        qkv = qkv.reshape(\n",
    "                n_samples, n_tokens, 3, self.n_heads, self.head_dim)\n",
    "        print(qkv.shape)\n",
    "        qkv = qkv.permute(\n",
    "            2, 0, 3, 1, 4)\n",
    "        # (3, n_samples, n_heads, n_patches + 1,head_dim )\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        k_t = k.transpose(-2, -1) # (n_samples, n_heads, head_dim, n_patches +1)\n",
    "\n",
    "        dp = (q @ k_t) * self.scale\n",
    "\n",
    "        attn = dp.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        weighted_avg = attn @ v\n",
    "        weighted_avg = weighted_avg.transpose(1, 2)\n",
    "        weighted_avg = weighted_avg.flatten(2)\n",
    "\n",
    "        x = self.proj(weighted_avg)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8c6fc99-ffb4-4eec-9ea2-430c013dd751",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, p=0.):\n",
    "        super().__init()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.drop(self.act(self.fc1(x)))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "543fb994-eab1-4162-869b-53fe168e6273",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim : int\n",
    "        Embedding dimensions.\n",
    "\n",
    "    n_heads : int\n",
    "        number of attention heads.\n",
    "\n",
    "    mlp_ratio : float\n",
    "        Determines the hidden dimension size of the \"MLP' module with rewspect to 'dim'.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, n_heads, mlp_ratio= 4.0, qkv_bias=True, p=0., attn_p = 0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.attn = Attention(dim,n_heads=n_heads, qkv_bias = qkv_bias,attn_p = attn_p, proj_p = p)\n",
    "        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        hidden_features = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(in_features=dim, hidden_features = hidden_features, out_features=dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x += self.attn(self.norm1(x))\n",
    "        x += self.mlp(self.norm2(x))\n",
    "\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f33f2fb-8669-48a7-8d34-76e7115588a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c90d2d8-6ff8-4961-a478-6cc9c4260787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d295183-9086-406b-b965-93a334b189ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1daf44-a2ee-453e-a466-94a3bd3edb17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd998977-ab81-4bca-9e72-ccaab544a798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ebc7d2-b7b7-4336-a6bb-6df23ca20ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf743ce-53f3-4126-b98e-b4c7f0d16153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3572da6e-7369-4d63-ad3b-97c3a874061a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cb27cc-7ac7-4a11-8008-ed9be9ba821f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37747268-7b73-4f68-9a21-2f3d25c1d969",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89538c1f-9e0b-4806-8eb5-ebf23d955e30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d777ba4-2275-46ad-ba16-64b678bb1a33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272cce59-0676-4bbb-863c-1accd7c539e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debfcc31-9a9b-4940-b52c-770ca192e261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ab14db-9f16-42df-96fb-d2082c19a509",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a51eb0-0d99-46b1-9a03-1408ca9acac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7f4fed-a5a4-4d5a-b863-627137ed10e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc5e012-ecc0-4825-a8df-88465d308007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753e3b98-9b55-4ed6-aa54-54064d6ae973",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbe6d0cf-e4f9-47c1-a557-16b787fd1f28",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'timm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtimm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DropPath\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnatten\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnatten\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NeighborhoodAttention2D \u001b[38;5;28;01mas\u001b[39;00m NeighborhoodAttention\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'timm'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from timm.models.layers import DropPath\n",
    "import natten\n",
    "from natten import NeighborhoodAttention2D as NeighborhoodAttention\n",
    "\n",
    "class NATransformerLayer(nn.Module):\n",
    "    \"\"\"Neighborhood Attention Transformer Layer with MLP.\"\"\"\n",
    "    def __init__(self, dim, num_heads, kernel_size=7, dilation=1, drop_path=0.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = NeighborhoodAttention(dim, kernel_size, dilation, num_heads)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0 else nn.Identity()\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, 4 * dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * dim, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"Converts input image into patches.\"\"\"\n",
    "    def __init__(self, patch_size=4, in_chans=3, embed_dim=96):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return self.norm(x)\n",
    "\n",
    "class DiNAT(nn.Module):\n",
    "    \"\"\"Simplified DiNAT model.\"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=4, embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24]):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbed(patch_size, 3, embed_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            NATransformerLayer(embed_dim * (2**i), num_heads[i]) for i in range(len(depths))\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim * (2**(len(depths) - 1)))\n",
    "        self.head = nn.Linear(embed_dim * (2**(len(depths) - 1)), 1000)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.norm(x.mean(dim=1))\n",
    "        return self.head(x)\n",
    "\n",
    "# Example usage\n",
    "model = DiNAT()\n",
    "img = torch.randn(1, 3, 224, 224)\n",
    "output = model(img)\n",
    "print(output.shape)  # Should be (1, 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804d2fe6-adb3-4187-b7ad-79dc01c1016e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95de9ecb-5fb9-4511-8944-109ee61479a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ebeec8-c365-40e0-b0e6-e2abc9f523fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aeecf0-4184-46f4-8281-5ef47844af12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e1eafc-0cd8-45b5-ac1d-015b17f270c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724971b5-e20b-4053-a611-192beca1303f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1dc4fc-70b7-4e31-af8e-7aaf823619f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8533e9ca-6391-44d1-8fcf-5ea1331cd40b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7636eb5e-3119-4f25-8874-f20120860037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de24387-cdab-40f7-bd55-36439e260e47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e73154-2d22-4919-b717-55d1b34d5283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894321dd-bbf6-4c58-af0f-ec24a8d8651e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96591651-37b4-40e8-af83-9fa4da5a83d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5476, 768])\n",
      "torch.Size([1, 5476, 2304])\n",
      "torch.Size([1, 5476, 3, 12, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0835,  0.0374,  0.0326,  ..., -0.0293, -0.0212, -0.0847],\n",
       "         [-0.0843,  0.0316,  0.0285,  ..., -0.0309, -0.0163, -0.0860],\n",
       "         [-0.0800,  0.0344,  0.0293,  ..., -0.0298, -0.0223, -0.0860],\n",
       "         ...,\n",
       "         [-0.0826,  0.0343,  0.0286,  ..., -0.0394, -0.0215, -0.0875],\n",
       "         [-0.0852,  0.0263,  0.0308,  ..., -0.0275, -0.0236, -0.0843],\n",
       "         [-0.0801,  0.0323,  0.0303,  ..., -0.0304, -0.0171, -0.0851]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod = Attention(768)\n",
    "mod(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2f9c98d-9daa-4fff-a3c9-f5dd00f77545",
   "metadata": {},
   "outputs": [],
   "source": [
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b839574-a125-4597-9706-4cc3a298752a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (1090054001.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[7], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class Block(nn.Module):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009322ce-32e4-4266-bd26-ebc98625fbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min(list1):\n",
    "    list1.sort()\n",
    "    return list1[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1db482-912f-4ed9-aba0-cafc854d3a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "min([1,2,1,1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af978e4f-6747-4a4d-9576-3c36f03d4b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "nums = [-1,0,1,2,-1,-4]\n",
    "\n",
    "l = []\n",
    "for i in range(len(nums) - 2):  # Step 2: Iterate over the array\n",
    "        if i > 0 : #and nums[i] == nums[i - 1]:\n",
    "            print(i)\n",
    "    \n",
    "#Output: [[-1,-1,2],[-1,0,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb03d4d6-d912-483c-a2b7-93f6dee299c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(224/16) **2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2004c5d4-ae1d-4e0f-9fa7-754e204bca0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "448"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "224 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "595fff58-0f2e-4080-b19d-9056d3a63a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0017, 0.0028, 0.0016,  ..., 0.0015, 0.0035, 0.0019],\n",
       "          [0.0046, 0.0038, 0.0026,  ..., 0.0045, 0.0021, 0.0049],\n",
       "          [0.0057, 0.0009, 0.0025,  ..., 0.0009, 0.0015, 0.0025],\n",
       "          ...,\n",
       "          [0.0008, 0.0137, 0.0059,  ..., 0.0019, 0.0012, 0.0075],\n",
       "          [0.0027, 0.0179, 0.0078,  ..., 0.0026, 0.0029, 0.0016],\n",
       "          [0.0025, 0.0016, 0.0030,  ..., 0.0005, 0.0011, 0.0045]],\n",
       "\n",
       "         [[0.0015, 0.0010, 0.0102,  ..., 0.0014, 0.0011, 0.0050],\n",
       "          [0.0004, 0.0061, 0.0025,  ..., 0.0053, 0.0006, 0.0011],\n",
       "          [0.0053, 0.0087, 0.0018,  ..., 0.0044, 0.0005, 0.0041],\n",
       "          ...,\n",
       "          [0.0013, 0.0034, 0.0025,  ..., 0.0003, 0.0017, 0.0007],\n",
       "          [0.0025, 0.0070, 0.0012,  ..., 0.0103, 0.0150, 0.0017],\n",
       "          [0.0022, 0.0013, 0.0013,  ..., 0.0004, 0.0057, 0.0118]],\n",
       "\n",
       "         [[0.0067, 0.0021, 0.0043,  ..., 0.0007, 0.0003, 0.0017],\n",
       "          [0.0022, 0.0174, 0.0058,  ..., 0.0034, 0.0139, 0.0026],\n",
       "          [0.0052, 0.0098, 0.0132,  ..., 0.0008, 0.0012, 0.0052],\n",
       "          ...,\n",
       "          [0.0015, 0.0025, 0.0016,  ..., 0.0011, 0.0039, 0.0018],\n",
       "          [0.0027, 0.0042, 0.0011,  ..., 0.0080, 0.0049, 0.0010],\n",
       "          [0.0029, 0.0006, 0.0019,  ..., 0.0013, 0.0161, 0.0010]]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb3e976e-87d1-42ac-ac5c-c69d4a96bf84",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x\u001b[38;5;241m=\u001b[39m \u001b[43mconv\u001b[49m(x)\n\u001b[0;32m      2\u001b[0m x\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'conv' is not defined"
     ]
    }
   ],
   "source": [
    " x= conv(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9186878-f986-4d44-8f51-e98dfae0fb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.flatten(2)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e2306b-2ec5-4482-a66e-af521d9af062",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.transpose(1,2)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d0cf01-944b-4ec5-b2b1-62b12507c8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5683f34-c810-4476-9a33-099ea1471f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sqrt(torch.tensor(768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea3038f5-90aa-49b6-8fcd-d1f02d537ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n",
       "           -0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "           -0.0000e+00, -0.0000e+00],\n",
       "          ...,\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
       "           -0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
       "           -0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[-0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
       "           -0.0000e+00,  0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n",
       "           -0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n",
       "           -0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
       "           -0.0000e+00, -0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 1.5987e+00, -6.8810e-01,  7.2089e-01,  ..., -2.7993e+00,\n",
       "           -4.7694e+00, -1.1910e+00],\n",
       "          [-3.6001e-01,  3.8258e+00,  1.6121e+00,  ...,  5.3697e-01,\n",
       "            3.3726e+00, -1.0565e-03],\n",
       "          [ 1.2894e+00,  2.5545e+00,  3.1587e+00,  ..., -2.4136e+00,\n",
       "           -1.6565e+00,  1.2924e+00],\n",
       "          ...,\n",
       "          [-1.3652e+00, -3.9664e-01, -1.2679e+00,  ..., -1.9759e+00,\n",
       "            4.6450e-01, -1.0275e+00],\n",
       "          [ 2.1954e-01,  1.1262e+00, -1.4655e+00,  ...,  2.4160e+00,\n",
       "            1.4463e+00, -1.7516e+00],\n",
       "          [ 4.4007e-01, -2.6675e+00, -3.7295e-01,  ..., -1.1068e+00,\n",
       "            3.8865e+00, -1.6741e+00]]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = nn.Dropout2d(0.5)\n",
    "p(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598ae2bd-a93b-4796-bc79-d02dfe00a195",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/position-embeddings-for-vision-transformers-explained-a6f9add341d5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "503008ca-6a50-4207-9ed1-74f56aacdf6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.training\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89d347d8-3f2a-4dd2-bc73-8dc1efc9489c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nihar\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Create a tensor of shape [max_len, dim] for encoding\n",
    "        pe = torch.zeros(max_len, dim)\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)  # [max_len, 1]\n",
    "        div_term = torch.exp(torch.arange(0, dim, 2).float() * -(math.log(10000.0) / dim))  # [dim/2]\n",
    "\n",
    "        # Apply sin and cos functions for positional encoding\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # even indices (sine)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # odd indices (cosine)\n",
    "\n",
    "        pe = pe.unsqueeze(0)  # Shape becomes [1, max_len, dim]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to the input tensor\n",
    "        return x + self.pe[:, :x.size(1)]  # x.size(1) is the length of the sequence\n",
    "\n",
    "# Example usage for ViT\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, dim=768, num_classes=1000):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        self.dim = dim\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Calculate the number of patches\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        # Define the embedding layer for patches\n",
    "        self.patch_embeddings = nn.Conv2d(3, dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "        # Positional encoding\n",
    "        self.positional_encoding = PositionalEncoding(dim, max_len=self.num_patches)\n",
    "\n",
    "        # Transformer layers (simplified version)\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=dim, nhead=8),\n",
    "            num_layers=12\n",
    "        )\n",
    "\n",
    "        # Classification head\n",
    "        self.fc = nn.Linear(dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract patches and embed them\n",
    "        x = self.patch_embeddings(x)  # Shape: [batch_size, dim, num_patches, num_patches]\n",
    "        x = x.flatten(2).transpose(1, 2)  # Shape: [batch_size, num_patches, dim]\n",
    "\n",
    "        # Add positional encoding\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        # Pass through transformer encoder\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # Classification head (take the output of the [CLS] token)\n",
    "        x = x.mean(dim=1)  # Global average pooling\n",
    "\n",
    "        # Final classification layer\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Example input (batch_size=2, img_size=224)\n",
    "model = VisionTransformer(img_size=224, patch_size=16)\n",
    "sample_input = torch.randn(2, 3, 224, 224)  # Batch of 2 images with 3 channels (RGB)\n",
    "output = model(sample_input)\n",
    "print(output.shape)  # Should output: torch.Size([2, 1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d15ba41-7f2f-49ae-88ef-7a1c7ffbeb6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(5000, 768).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b790c537-1b53-49a8-93f0-197e2abecb01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = torch.arange(0, 5000).float().unsqueeze(1)\n",
    "position.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5a3ceec-88a8-4f96-ac0c-865a157f9eff",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (22) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 48\u001b[0m\n\u001b[0;32m     46\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(B, H, W, C)\n\u001b[0;32m     47\u001b[0m attn_layer \u001b[38;5;241m=\u001b[39m DilatedNeighborhoodAttention(dim\u001b[38;5;241m=\u001b[39mC, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m, dilation\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m---> 48\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mattn_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(out\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[18], line 25\u001b[0m, in \u001b[0;36mDilatedNeighborhoodAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     22\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m qkv[\u001b[38;5;241m0\u001b[39m], qkv[\u001b[38;5;241m1\u001b[39m], qkv[\u001b[38;5;241m2\u001b[39m]  \u001b[38;5;66;03m# Split into queries, keys, and values\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Compute attention with dilation\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m attn_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_dilated_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m (attn_map \u001b[38;5;241m@\u001b[39m v)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B, H, W, C)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(attn_output)\n",
      "Cell \u001b[1;32mIn[18], line 38\u001b[0m, in \u001b[0;36mDilatedNeighborhoodAttention.compute_dilated_attention\u001b[1;34m(self, q, k, H, W)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size):\n\u001b[0;32m     37\u001b[0m         k_slice \u001b[38;5;241m=\u001b[39m k_padded[:, :, i::\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, j::\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, :]\n\u001b[1;32m---> 38\u001b[0m         attn_weights\u001b[38;5;241m.\u001b[39mappend((\u001b[43mq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk_slice\u001b[49m)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     40\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(attn_weights, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     41\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(attn_weights \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (22) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DilatedNeighborhoodAttention(nn.Module):\n",
    "    def __init__(self, dim, kernel_size=7, dilation=1, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation = dilation\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, H, W, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, H, W, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(3, 0, 4, 1, 2, 5)  # (3, B, num_heads, H, W, head_dim)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # Split into queries, keys, and values\n",
    "        \n",
    "        # Compute attention with dilation\n",
    "        attn_map = self.compute_dilated_attention(q, k, H, W)\n",
    "        attn_output = (attn_map @ v).permute(0, 2, 3, 1, 4).reshape(B, H, W, C)\n",
    "        return self.proj(attn_output)\n",
    "    \n",
    "    def compute_dilated_attention(self, q, k, H, W):\n",
    "        \"\"\"Computes dilated attention weights\"\"\"\n",
    "        pad = self.dilation * (self.kernel_size // 2)\n",
    "        k_padded = F.pad(k, (0, 0, pad, pad, pad, pad))\n",
    "        attn_weights = []\n",
    "        \n",
    "        for i in range(self.kernel_size):\n",
    "            for j in range(self.kernel_size):\n",
    "                k_slice = k_padded[:, :, i::self.dilation, j::self.dilation, :]\n",
    "                attn_weights.append((q * k_slice).sum(-1))\n",
    "        \n",
    "        attn_weights = torch.stack(attn_weights, dim=-1)\n",
    "        attn_weights = F.softmax(attn_weights * self.scale, dim=-1)\n",
    "        return attn_weights\n",
    "\n",
    "# Example Usage\n",
    "B, H, W, C = 1, 32, 32, 64  # Batch size, Height, Width, Channels\n",
    "x = torch.randn(B, H, W, C)\n",
    "attn_layer = DilatedNeighborhoodAttention(dim=C, kernel_size=7, dilation=2, num_heads=8)\n",
    "out = attn_layer(x)\n",
    "print(out.shape)  # Expected output: (B, H, W, C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35974363-2516-4724-ac41-8a0e84f054b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [-1,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01634a1-280c-4a84-81f3-42a6bff6b04a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6e0c85-d7e8-4b65-b627-1d4ca58f75cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ba3f84-62d7-43a0-b5d1-2897c4b39ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b301ce0-5315-4051-9345-690fb8127c64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
