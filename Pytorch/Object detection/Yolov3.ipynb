{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07da02e6-753b-4885-b5ea-72ef7a21e257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg\n",
    "# https://github.com/aladdinpersson/Machine-Learning-Collection\n",
    "\n",
    "\"\"\" \n",
    "Information about architecture config:\n",
    "Tuple is structured by (filters, kernel_size, stride) \n",
    "Every conv is a same convolution. \n",
    "List is structured by \"B\" indicating a residual block followed by the number of repeats\n",
    "\"S\" is for scale prediction block and computing the yolo loss\n",
    "\"U\" is for upsampling the feature map and concatenating with a previous layer\n",
    "\"\"\"\n",
    "\n",
    "config = [\n",
    "    (32, 3, 1),\n",
    "    (64, 3, 2),\n",
    "    [\"B\", 1],\n",
    "    (128, 3, 2),\n",
    "    [\"B\", 2],\n",
    "    (256, 3, 2),\n",
    "    [\"B\", 8],\n",
    "    (512, 3, 2),\n",
    "    [\"B\", 8],\n",
    "    (1024, 3, 2),\n",
    "    [\"B\", 4],\n",
    "    (512, 1, 1),\n",
    "    (1024, 3, 1),\n",
    "    \"S\",\n",
    "    (256, 1, 1),\n",
    "    \"U\",\n",
    "    (256, 1, 1),\n",
    "    (512, 3, 1),\n",
    "    \"S\",\n",
    "    (128, 1, 1),\n",
    "    \"U\",\n",
    "    (128, 1, 1),\n",
    "    (256, 3, 1),\n",
    "    \"S\",\n",
    "]\n",
    "\n",
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bn_act = True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias= not bn_act, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.1)\n",
    "        self.use_bn_act = bn_act\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_bn_act:\n",
    "            return self.leaky(self.bn(self.conv(x)))\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, use_residual=True, num_repeats=1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(num_repeats):\n",
    "            self.layers += [\n",
    "                nn.Sequential(\n",
    "                    CNNBlock(channels, channels//2, kernel_size = 1),\n",
    "                    CNNBlock(channels//2, channels, kernel_size=3, padding=1),\n",
    "                )\n",
    "            ]\n",
    "        self.use_residual = use_residual\n",
    "        self.num_repeats = num_repeats\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            if self.use_residual:\n",
    "                x = layer(x) + x\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "\n",
    "class ScalePrediction(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.pred = nn.Sequential(\n",
    "            CNNBlock(in_channels, 2 * in_channels, kernel_size=3, padding=1),\n",
    "            CNNBlock(2 * in_channels, 3 * (num_classes + 5), bn_act=False, kernel_size=1 ), #[po, x, y, w, h]\n",
    "        )\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (\n",
    "            self.pred(x)\n",
    "            .reshape(x.shape[0], 3, self.num_classes + 5, x.shape[2], x.shape[3])\n",
    "            .permute(0, 1, 3, 4, 2) # Example, Anchor,  \n",
    "        )\n",
    "\n",
    "    # N x 3 X 26 X 26 X (num_classes + 5)\n",
    "\n",
    "class YOLOv3(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=20):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.in_channels = in_channels\n",
    "        self.layers = self._create_conv_layers()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        route_connections = []\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, ScalePrediction):\n",
    "                outputs.append(layer(x))\n",
    "                continue\n",
    "\n",
    "            x = layer(x)\n",
    "            #print(x.shape)\n",
    "\n",
    "\n",
    "            if isinstance(layer, ResidualBlock) and layer.num_repeats == 8:\n",
    "                route_connections.append(x)\n",
    "\n",
    "            elif isinstance(layer, nn.Upsample):\n",
    "                x = torch.cat([x, route_connections[-1]], dim=1)\n",
    "\n",
    "                route_connections.pop()\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def _create_conv_layers(self):\n",
    "        layers = nn.ModuleList()\n",
    "        in_channels = self.in_channels\n",
    "\n",
    "        for module in config:\n",
    "            \"\"\"\n",
    "            Tuple: CNN block\n",
    "            List: Residual block\n",
    "            String: Upsample / ScalePrediction \"\"\"\n",
    "\n",
    "            if isinstance(module, tuple):\n",
    "                out_channels, kernel_size, stride = module\n",
    "\n",
    "                layers.append(\n",
    "                    CNNBlock(\n",
    "                        in_channels,\n",
    "                        out_channels,\n",
    "                        kernel_size = kernel_size,\n",
    "                        stride = stride,\n",
    "                        padding = 1 if kernel_size == 3 else 0,\n",
    "                   )\n",
    "                )\n",
    "\n",
    "                in_channels = out_channels\n",
    "            elif isinstance(module, list):\n",
    "                num_repeats = module[1]\n",
    "                layers.append(ResidualBlock(in_channels, num_repeats=num_repeats,))\n",
    "\n",
    "            elif isinstance(module, str):\n",
    "                if module == \"S\":\n",
    "                    layers += [ResidualBlock(in_channels, use_residual=False, num_repeats=1),\n",
    "                               CNNBlock(in_channels, in_channels//2, kernel_size= 1),\n",
    "                               ScalePrediction(in_channels//2, num_classes=self.num_classes)]\n",
    "\n",
    "                    in_channels = in_channels // 2\n",
    "\n",
    "                if module == \"U\":\n",
    "                    layers.append(nn.Upsample(scale_factor = 2),)\n",
    "\n",
    "                    in_channels = in_channels * 3\n",
    "\n",
    "        return layers\n",
    "\n",
    "\n",
    "num_classes = 20\n",
    "IMAGE_SIZE = 416\n",
    "model = YOLOv3(num_classes=num_classes)\n",
    "x = torch.randn((2, 3, IMAGE_SIZE, IMAGE_SIZE))\n",
    "out = model(x)\n",
    "assert model(x)[0].shape == (2, 3, IMAGE_SIZE//32, IMAGE_SIZE//32, num_classes + 5)\n",
    "assert model(x)[1].shape == (2, 3, IMAGE_SIZE//16, IMAGE_SIZE//16, num_classes + 5)\n",
    "assert model(x)[2].shape == (2, 3, IMAGE_SIZE//8, IMAGE_SIZE//8, num_classes + 5)\n",
    "print(\"Success!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b0cfd17-76e9-4801-a895-724293a5a1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nihar\\AppData\\Local\\Temp\\ipykernel_18456\\3234625295.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000001.jpg</th>\n",
       "      <th>000001.txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000002.jpg</td>\n",
       "      <td>000002.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000003.jpg</td>\n",
       "      <td>000003.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000004.jpg</td>\n",
       "      <td>000004.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000006.jpg</td>\n",
       "      <td>000006.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000008.jpg</td>\n",
       "      <td>000008.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4946</th>\n",
       "      <td>009956.jpg</td>\n",
       "      <td>009956.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4947</th>\n",
       "      <td>009957.jpg</td>\n",
       "      <td>009957.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4948</th>\n",
       "      <td>009960.jpg</td>\n",
       "      <td>009960.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4949</th>\n",
       "      <td>009962.jpg</td>\n",
       "      <td>009962.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4950</th>\n",
       "      <td>009963.jpg</td>\n",
       "      <td>009963.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4951 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      000001.jpg  000001.txt\n",
       "0     000002.jpg  000002.txt\n",
       "1     000003.jpg  000003.txt\n",
       "2     000004.jpg  000004.txt\n",
       "3     000006.jpg  000006.txt\n",
       "4     000008.jpg  000008.txt\n",
       "...          ...         ...\n",
       "4946  009956.jpg  009956.txt\n",
       "4947  009957.jpg  009957.txt\n",
       "4948  009960.jpg  009960.txt\n",
       "4949  009962.jpg  009962.txt\n",
       "4950  009963.jpg  009963.txt\n",
       "\n",
       "[4951 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(\"D:/Object detction/data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18238c5f-99da-4104-b96c-9695cebadb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\",GIoU=False, UIoU= False, DIoU=False, CIoU=False, AIoU= False, ICIoU=False ,beta =1, eps=1e-7):\n",
    "    # boxes_preds shape is (N, 4) where N is the number of bboxes\n",
    "    # bboxes_labels shape is (N, 4)\n",
    "\n",
    "    if box_format==\"midpoint\":\n",
    "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
    "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
    "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
    "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
    "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
    "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
    "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
    "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
    "\n",
    "    elif box_format == \"corners\":\n",
    "        box1_x1 = boxes_preds[..., 0:1]\n",
    "        box1_y1 = boxes_preds[..., 1:2]\n",
    "        box1_x2 = boxes_preds[..., 2:3]\n",
    "        box1_y2 = boxes_preds[..., 3:4] # (N, 1)\n",
    "        box2_x1 = boxes_labels[..., 0:1]\n",
    "        box2_y1 = boxes_labels[..., 1:2]\n",
    "        box2_x2 = boxes_labels[..., 2:3]\n",
    "        box2_y2 = boxes_labels[..., 3:4]\n",
    "\n",
    "    x1 = box1_x1.max(box2_x1)\n",
    "    y1 = box1_y1.max(box2_y1)\n",
    "    x2 = box1_x2.min(box2_x2)\n",
    "    y2 = box1_y2.min(box2_y2)\n",
    "\n",
    "    w1, h1 = box1_x2 - box1_x1, box1_y2 - box1_y1 + eps\n",
    "    \n",
    "    w2, h2 = box2_x2 - box2_x1, box2_y2 - box2_y1 + eps\n",
    "\n",
    "     #.clamp(0) is for the case when they donot intersect\n",
    "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
    "\n",
    "    box1_area = abs((box1_x2 - box1_x1) * (box1_y1 - box1_y2))\n",
    "    box2_area = abs((box2_x2 - box2_x1) * (box2_y1 - box2_y2))\n",
    "\n",
    "    union = box1_area + box2_area - intersection\n",
    "\n",
    "    iou = intersection/(union + 1e-6)\n",
    "\n",
    "    if ICIoU or AIoU or CIoU or  DIoU or UIoU or GIoU :\n",
    "        cw = box1_x2.max(box2_x2) - box1_x1.min(box2_x1)  # convex (smallest enclosing box) width\n",
    "        ch = box1_y2.max(box2_y2) - box1_y1.min(box2_y1)  # convex height\n",
    "\n",
    "        c2 = cw.pow(2) + ch.pow(2) + eps  # convex diagonal squared\n",
    "            \n",
    "        rho2 = (\n",
    "                (box2_x1 + box2_x2 - box1_x1 - box1_x2).pow(2) + (box2_y1 + box2_y2 - box1_y1 - box1_y2).pow(2))/ 4  # center dist**2\n",
    "        \n",
    "        if CIoU or DIoU or ICIoU:  # Distance or Complete IoU https://arxiv.org/abs/1911.08287v1\n",
    "            \n",
    "            if CIoU:  # https://github.com/Zzh-tju/DIoU-SSD-pytorch/blob/master/utils/box/box_utils.py#L47\n",
    "                v = (4 / math.pi**2) * ((w2 / h2).atan() - (w1 / h1).atan()).pow(2)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    alpha = v / (v - iou + (1 + eps))\n",
    "                if AIoU:\n",
    "                    A_Agt2 = (box1_x1 - box2_x1).pow(2) + (box1_y1 - box2_y1).pow(2)\n",
    "                    B_Bgt2 = (box1_x2 - box2_x2).pow(2) + (box1_y2 - box2_y2).pow(2)\n",
    "                    return iou - (rho2 / c2) + (v * alpha) + beta * ((A_Agt2 + B_Bgt2)/c2)   # AIoU\n",
    "                \n",
    "                return iou - (rho2 / c2) + (v * alpha)  # CIoU\n",
    "            elif ICIoU:\n",
    "                vv = (8 / math.pi**2) * (((w2 / w1).atan() - (math.pi/4)).pow(2)  - ((h2 / h1).atan() - (math.pi/4)).pow(2))\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    alpha = vv / (vv - iou + (1 + eps))\n",
    "                return iou - (rho2 / c2 + vv * alpha)  # ICIoU\n",
    "            return iou - (rho2 / c2)  # DIoU\n",
    "        c_area = cw * ch + eps  # convex area\n",
    "        giou = iou - (c_area - union) / c_area  # GIoU https://arxiv.org/pdf/1902.09630.pdf\n",
    "        if UIoU:\n",
    "            normalized_distance = c2/rho2\n",
    "\n",
    "            similarity = math.sqrt(min(box1_area, box1_area) / max(box1_area, box1_area)) if max(box1_area, box1_area) > 0 else 0\n",
    "            if iou == 0:  # Non-overlapping case\n",
    "                uiou = 0 + 0.5 * (1 - normalized_distance)\n",
    "            elif iou > 0 and giou < 0.98:  # Partial overlap\n",
    "                uiou = 0.5 + 0.48 * (1 + giou) / 2\n",
    "            else:  # One box inside another\n",
    "                uiou = 0.98 + 0.02 * ((1 / similarity ** 2) + (1 - normalized_distance)) / 2\n",
    "        \n",
    "            return uiou \n",
    "            \n",
    "        return giou\n",
    "    return iou  # IoU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9d42b78-ecae-4cd6-b1bc-8153a250e913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1429])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "box1 = torch.tensor([1,1,3,3])\n",
    "box2 = torch.tensor([2,2,4,4])\n",
    "iou = intersection_over_union(box1, box2, box_format=\"corners\",)\n",
    "iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "669d261f-f245-4e02-a386-21828dd4b4e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7210])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uiou = intersection_over_union(box1, box2, box_format=\"corners\",UIoU=True)\n",
    "uiou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2da5281e-f488-4006-ad56-ef24de2bf6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression(\n",
    "        bboxes,\n",
    "        iou_threshold,\n",
    "        threshold,\n",
    "        box_format=\"corners\"\n",
    "    ):\n",
    "    \"\"\"\n",
    "       Does Non Max Suppression given bboxes\n",
    "\n",
    "       Parameters:\n",
    "           bboxes (list): list of lists containing all bboxes with each bboxes\n",
    "           specified as [class_pred, prob_score, x1, y1, x2, y2]\n",
    "           iou_threshold (float): threshold where predicted bboxes is correct\n",
    "           threshold (float): threshold to remove predicted bboxes (independent of IoU)\n",
    "           box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
    "\n",
    "       Returns:\n",
    "           list: bboxes after performing NMS given a specific IoU threshold\n",
    "       \"\"\"\n",
    "    # predictions = [[class_name=1, probablity_bounding_box=0.9, X1, y1, x2, y2], [], []]\n",
    "\n",
    "    assert type(bboxes) == list\n",
    "\n",
    "    bboxes = [box for box in bboxes if box[1] > threshold]\n",
    "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
    "    bboxes_after_nms = []\n",
    "\n",
    "    while bboxes:\n",
    "        chosen_box = bboxes.pop(0)\n",
    "\n",
    "        bboxes = [\n",
    "            box\n",
    "            for box in bboxes\n",
    "            if box[0] != chosen_box[0]\n",
    "            or intersection_over_union(torch.tensor(chosen_box[2:]),\n",
    "                                       torch.tensor(box[2:]),\n",
    "                                       box_format=box_format) < iou_threshold\n",
    "            ]\n",
    "\n",
    "        bboxes_after_nms.append(chosen_box)\n",
    "\n",
    "    return bboxes_after_nms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14349e95-806c-4ca8-901e-3e053610efec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from PIL import Image, ImageFile\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# from utils import ( iou_width_height as iou,\n",
    "#                     non_max_supression as nms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "daa89735-421d-48e1-92bf-5100ace1a39e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def iou_width_height(boxes1, boxes2):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        boxes1 (tensor): width and height of the first bounding boxes\n",
    "        boxes2 (tensor): width and height of the second bounding boxes\n",
    "    Returns:\n",
    "        tensor: Intersection over union of the corresponding boxes\n",
    "    \"\"\"\n",
    "    intersection = torch.min(boxes1[..., 0], boxes2[..., 0]) * torch.min(\n",
    "        boxes1[..., 1], boxes2[..., 1]\n",
    "    )\n",
    "    union = (\n",
    "        boxes1[..., 0] * boxes1[..., 1] + boxes2[..., 0] * boxes2[..., 1] - intersection\n",
    "    )\n",
    "    return intersection / union\n",
    "\n",
    "x = torch.tensor([1,2])\n",
    "iou_width_height(x,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7a48ad34-5bdb-4d09-9086-858b4d481f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "class YOLODataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_file, \n",
    "        img_dir, \n",
    "        label_dir, \n",
    "        anchors, \n",
    "        image_size = 416,\n",
    "        S = [13, 26, 52],\n",
    "        C = 20,\n",
    "        transform = None,\n",
    "    ):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.S = S\n",
    "        self.anchors = torch.tensor(anchors[0] + anchors[1] + anchors[2]) # for all 3 scales\n",
    "        self.num_anchors = self.anchors.shape[0]\n",
    "        self.num_anchors_per_scale = self.num_anchors // 3,\n",
    "        self.C = C\n",
    "        self.ignore_iou_thresh = 0.5\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label_path = os.path.join(self.label_dir, self.annotations.iloc[index, 1])\n",
    "        bboxes = np.roll(np.loadtxt(fname=label_path, delimiter= \" \",ndmin=2), 4, axis=1).tolist() # [class, x,y, w,h] -> [x,y,w,h, class]\n",
    "        img_path = os.path.join(self.img_dir, self.annotations.iloc[index, 0])\n",
    "        image = np.array(Image.open(img_path).convert('RGB'))\n",
    "\n",
    "        if self.transform:\n",
    "            augmentations = self.transform(image=image, bboxes=bboxes)\n",
    "            image = augmentations[\"image\"]\n",
    "            bboxes = augmentations[\"bboxes\"]\n",
    "\n",
    "        targets = [torch.zeros((self.num_anchors // 3, S, S, 6 )) for S in self.S] # [p_o, x, y, w, h, c]\n",
    "\n",
    "        for box in bboxes:\n",
    "            iou_anchors = iou_width_height(torch.tensor(box[2:4]), self.anchors)\n",
    "            anchor_indices = iou_anchors.argsort(descending=True, dim==0)\n",
    "            x , y, width, height, class_label = box\n",
    "            has_anchor = [False, False, False]\n",
    "\n",
    "            for anchor_idx in anchor_indices:\n",
    "                scale_idx = anchor_idx // self.num_anchors_per_scale # 0, 1, 2\n",
    "                anchor_on_scale = anchor_idx % self.num_anchors_per_scale # 0, 1, 2\n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5bf54d0-cb1e-4be1-9fc3-2dd8f188ecc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"example.txt\"\n",
    "\n",
    "# Open the file in write mode and write content\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(\"1,2,3,2.5,4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e36ed2da-611a-45bb-bfdf-37ab69334dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2.0, 3.0, 2.5, 4.0, 1.0]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.roll(np.loadtxt(\"example.txt\",delimiter= \",\",ndmin=2),4, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6501496-808b-4d33-b7ea-ce9d526cfc94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Desktop/Deep-Learning/Pytorch'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join('C:/Desktop/Deep-Learning/', 'Pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e17fb99-b8be-471d-882c-5155d629fc74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ntpath' from 'C:\\\\Users\\\\Nihar\\\\anaconda3\\\\envs\\\\tf\\\\lib\\\\ntpath.py'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f901489b-db94-4798-a503-4302786cf029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.46737193, 0.68258588],\n",
       "       [0.64996854, 0.72084505],\n",
       "       [0.99715751, 0.73937604],\n",
       "       [0.55488625, 0.66447921],\n",
       "       [0.6444267 , 0.78911178]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.rand(10).reshape(5,2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b4b1da64-0070-4da0-8e38-ca978c906363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.64996854, 0.72084505],\n",
       "       [0.99715751, 0.73937604],\n",
       "       [0.55488625, 0.66447921],\n",
       "       [0.6444267 , 0.78911178],\n",
       "       [0.46737193, 0.68258588]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.roll(x, -1,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "17309ca5-2ce0-4070-8489-0edb5d4aea19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2de47de3-4b46-4269-8cce-81c0002375db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1155000"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ecef7a-607f-4060-8637-5a7dc060932e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
