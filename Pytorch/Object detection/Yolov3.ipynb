{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07da02e6-753b-4885-b5ea-72ef7a21e257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg\n",
    "# https://github.com/aladdinpersson/Machine-Learning-Collection\n",
    "\n",
    "\"\"\" \n",
    "Information about architecture config:\n",
    "Tuple is structured by (filters, kernel_size, stride) \n",
    "Every conv is a same convolution. \n",
    "List is structured by \"B\" indicating a residual block followed by the number of repeats\n",
    "\"S\" is for scale prediction block and computing the yolo loss\n",
    "\"U\" is for upsampling the feature map and concatenating with a previous layer\n",
    "\"\"\"\n",
    "\n",
    "config = [\n",
    "    (32, 3, 1),\n",
    "    (64, 3, 2),\n",
    "    [\"B\", 1],\n",
    "    (128, 3, 2),\n",
    "    [\"B\", 2],\n",
    "    (256, 3, 2),\n",
    "    [\"B\", 8],\n",
    "    (512, 3, 2),\n",
    "    [\"B\", 8],\n",
    "    (1024, 3, 2),\n",
    "    [\"B\", 4],\n",
    "    (512, 1, 1),\n",
    "    (1024, 3, 1),\n",
    "    \"S\",\n",
    "    (256, 1, 1),\n",
    "    \"U\",\n",
    "    (256, 1, 1),\n",
    "    (512, 3, 1),\n",
    "    \"S\",\n",
    "    (128, 1, 1),\n",
    "    \"U\",\n",
    "    (128, 1, 1),\n",
    "    (256, 3, 1),\n",
    "    \"S\",\n",
    "]\n",
    "\n",
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bn_act = True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias= not bn_act, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.1)\n",
    "        self.use_bn_act = bn_act\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_bn_act:\n",
    "            return self.leaky(self.bn(self.conv(x)))\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, use_residual=True, num_repeats=1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(num_repeats):\n",
    "            self.layers += [\n",
    "                nn.Sequential(\n",
    "                    CNNBlock(channels, channels//2, kernel_size = 1),\n",
    "                    CNNBlock(channels//2, channels, kernel_size=3, padding=1),\n",
    "                )\n",
    "            ]\n",
    "        self.use_residual = use_residual\n",
    "        self.num_repeats = num_repeats\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            if self.use_residual:\n",
    "                x = layer(x) + x\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "\n",
    "class ScalePrediction(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.pred = nn.Sequential(\n",
    "            CNNBlock(in_channels, 2 * in_channels, kernel_size=3, padding=1),\n",
    "            CNNBlock(2 * in_channels, 3 * (num_classes + 5), bn_act=False, kernel_size=1 ), #[po, x, y, w, h]\n",
    "        )\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (\n",
    "            self.pred(x)\n",
    "            .reshape(x.shape[0], 3, self.num_classes + 5, x.shape[2], x.shape[3])\n",
    "            .permute(0, 1, 3, 4, 2) # Example, Anchor,  \n",
    "        )\n",
    "\n",
    "    # N x 3 X 26 X 26 X (num_classes + 5)\n",
    "\n",
    "class YOLOv3(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=20):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.in_channels = in_channels\n",
    "        self.layers = self._create_conv_layers()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        route_connections = []\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, ScalePrediction):\n",
    "                outputs.append(layer(x))\n",
    "                continue\n",
    "\n",
    "            x = layer(x)\n",
    "            #print(x.shape)\n",
    "\n",
    "\n",
    "            if isinstance(layer, ResidualBlock) and layer.num_repeats == 8:\n",
    "                route_connections.append(x)\n",
    "\n",
    "            elif isinstance(layer, nn.Upsample):\n",
    "                x = torch.cat([x, route_connections[-1]], dim=1)\n",
    "\n",
    "                route_connections.pop()\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def _create_conv_layers(self):\n",
    "        layers = nn.ModuleList()\n",
    "        in_channels = self.in_channels\n",
    "\n",
    "        for module in config:\n",
    "            \"\"\"\n",
    "            Tuple: CNN block\n",
    "            List: Residual block\n",
    "            String: Upsample / ScalePrediction \"\"\"\n",
    "\n",
    "            if isinstance(module, tuple):\n",
    "                out_channels, kernel_size, stride = module\n",
    "\n",
    "                layers.append(\n",
    "                    CNNBlock(\n",
    "                        in_channels,\n",
    "                        out_channels,\n",
    "                        kernel_size = kernel_size,\n",
    "                        stride = stride,\n",
    "                        padding = 1 if kernel_size == 3 else 0,\n",
    "                   )\n",
    "                )\n",
    "\n",
    "                in_channels = out_channels\n",
    "            elif isinstance(module, list):\n",
    "                num_repeats = module[1]\n",
    "                layers.append(ResidualBlock(in_channels, num_repeats=num_repeats,))\n",
    "\n",
    "            elif isinstance(module, str):\n",
    "                if module == \"S\":\n",
    "                    layers += [ResidualBlock(in_channels, use_residual=False, num_repeats=1),\n",
    "                               CNNBlock(in_channels, in_channels//2, kernel_size= 1),\n",
    "                               ScalePrediction(in_channels//2, num_classes=self.num_classes)]\n",
    "\n",
    "                    in_channels = in_channels // 2\n",
    "\n",
    "                if module == \"U\":\n",
    "                    layers.append(nn.Upsample(scale_factor = 2),)\n",
    "\n",
    "                    in_channels = in_channels * 3\n",
    "\n",
    "        return layers\n",
    "\n",
    "\n",
    "num_classes = 20\n",
    "IMAGE_SIZE = 416\n",
    "model = YOLOv3(num_classes=num_classes)\n",
    "x = torch.randn((2, 3, IMAGE_SIZE, IMAGE_SIZE))\n",
    "out = model(x)\n",
    "assert model(x)[0].shape == (2, 3, IMAGE_SIZE//32, IMAGE_SIZE//32, num_classes + 5)\n",
    "assert model(x)[1].shape == (2, 3, IMAGE_SIZE//16, IMAGE_SIZE//16, num_classes + 5)\n",
    "assert model(x)[2].shape == (2, 3, IMAGE_SIZE//8, IMAGE_SIZE//8, num_classes + 5)\n",
    "print(\"Success!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5b0cfd17-76e9-4801-a895-724293a5a1d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000001.jpg</th>\n",
       "      <th>000001.txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000002.jpg</td>\n",
       "      <td>000002.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000003.jpg</td>\n",
       "      <td>000003.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000004.jpg</td>\n",
       "      <td>000004.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000006.jpg</td>\n",
       "      <td>000006.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000008.jpg</td>\n",
       "      <td>000008.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4946</th>\n",
       "      <td>009956.jpg</td>\n",
       "      <td>009956.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4947</th>\n",
       "      <td>009957.jpg</td>\n",
       "      <td>009957.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4948</th>\n",
       "      <td>009960.jpg</td>\n",
       "      <td>009960.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4949</th>\n",
       "      <td>009962.jpg</td>\n",
       "      <td>009962.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4950</th>\n",
       "      <td>009963.jpg</td>\n",
       "      <td>009963.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4951 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      000001.jpg  000001.txt\n",
       "0     000002.jpg  000002.txt\n",
       "1     000003.jpg  000003.txt\n",
       "2     000004.jpg  000004.txt\n",
       "3     000006.jpg  000006.txt\n",
       "4     000008.jpg  000008.txt\n",
       "...          ...         ...\n",
       "4946  009956.jpg  009956.txt\n",
       "4947  009957.jpg  009957.txt\n",
       "4948  009960.jpg  009960.txt\n",
       "4949  009962.jpg  009962.txt\n",
       "4950  009963.jpg  009963.txt\n",
       "\n",
       "[4951 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(\"D:/Object detction/data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18238c5f-99da-4104-b96c-9695cebadb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\",GIoU=False, UIoU= False, DIoU=False, CIoU=False, AIoU= False, ICIoU=False ,beta =1, eps=1e-7):\n",
    "    # boxes_preds shape is (N, 4) where N is the number of bboxes\n",
    "    # bboxes_labels shape is (N, 4)\n",
    "\n",
    "    if box_format==\"midpoint\":\n",
    "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
    "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
    "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
    "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
    "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
    "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
    "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
    "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
    "\n",
    "    elif box_format == \"corners\":\n",
    "        box1_x1 = boxes_preds[..., 0:1]\n",
    "        box1_y1 = boxes_preds[..., 1:2]\n",
    "        box1_x2 = boxes_preds[..., 2:3]\n",
    "        box1_y2 = boxes_preds[..., 3:4] # (N, 1)\n",
    "        box2_x1 = boxes_labels[..., 0:1]\n",
    "        box2_y1 = boxes_labels[..., 1:2]\n",
    "        box2_x2 = boxes_labels[..., 2:3]\n",
    "        box2_y2 = boxes_labels[..., 3:4]\n",
    "\n",
    "    x1 = box1_x1.max(box2_x1)\n",
    "    y1 = box1_y1.max(box2_y1)\n",
    "    x2 = box1_x2.min(box2_x2)\n",
    "    y2 = box1_y2.min(box2_y2)\n",
    "\n",
    "    w1, h1 = box1_x2 - box1_x1, box1_y2 - box1_y1 + eps\n",
    "    \n",
    "    w2, h2 = box2_x2 - box2_x1, box2_y2 - box2_y1 + eps\n",
    "\n",
    "     #.clamp(0) is for the case when they donot intersect\n",
    "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
    "\n",
    "    box1_area = abs((box1_x2 - box1_x1) * (box1_y1 - box1_y2))\n",
    "    box2_area = abs((box2_x2 - box2_x1) * (box2_y1 - box2_y2))\n",
    "\n",
    "    union = box1_area + box2_area - intersection\n",
    "\n",
    "    iou = intersection/(union + 1e-6)\n",
    "\n",
    "    if ICIoU or AIoU or CIoU or  DIoU or UIoU or GIoU :\n",
    "        cw = box1_x2.max(box2_x2) - box1_x1.min(box2_x1)  # convex (smallest enclosing box) width\n",
    "        ch = box1_y2.max(box2_y2) - box1_y1.min(box2_y1)  # convex height\n",
    "\n",
    "        c2 = cw.pow(2) + ch.pow(2) + eps  # convex diagonal squared\n",
    "            \n",
    "        rho2 = (\n",
    "                (box2_x1 + box2_x2 - box1_x1 - box1_x2).pow(2) + (box2_y1 + box2_y2 - box1_y1 - box1_y2).pow(2))/ 4  # center dist**2\n",
    "        \n",
    "        if CIoU or DIoU or ICIoU:  # Distance or Complete IoU https://arxiv.org/abs/1911.08287v1\n",
    "            \n",
    "            if CIoU:  # https://github.com/Zzh-tju/DIoU-SSD-pytorch/blob/master/utils/box/box_utils.py#L47\n",
    "                v = (4 / math.pi**2) * ((w2 / h2).atan() - (w1 / h1).atan()).pow(2)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    alpha = v / (v - iou + (1 + eps))\n",
    "                if AIoU:\n",
    "                    A_Agt2 = (box1_x1 - box2_x1).pow(2) + (box1_y1 - box2_y1).pow(2)\n",
    "                    B_Bgt2 = (box1_x2 - box2_x2).pow(2) + (box1_y2 - box2_y2).pow(2)\n",
    "                    return iou - (rho2 / c2) + (v * alpha) + beta * ((A_Agt2 + B_Bgt2)/c2)   # AIoU\n",
    "                \n",
    "                return iou - (rho2 / c2) + (v * alpha)  # CIoU\n",
    "            elif ICIoU:\n",
    "                vv = (8 / math.pi**2) * (((w2 / w1).atan() - (math.pi/4)).pow(2)  - ((h2 / h1).atan() - (math.pi/4)).pow(2))\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    alpha = vv / (vv - iou + (1 + eps))\n",
    "                return iou - (rho2 / c2 + vv * alpha)  # ICIoU\n",
    "            return iou - (rho2 / c2)  # DIoU\n",
    "        c_area = cw * ch + eps  # convex area\n",
    "        giou = iou - (c_area - union) / c_area  # GIoU https://arxiv.org/pdf/1902.09630.pdf\n",
    "        if UIoU:\n",
    "            normalized_distance = c2/rho2\n",
    "\n",
    "            similarity = math.sqrt(min(box1_area, box1_area) / max(box1_area, box1_area)) if max(box1_area, box1_area) > 0 else 0\n",
    "            if iou == 0:  # Non-overlapping case\n",
    "                uiou = 0 + 0.5 * (1 - normalized_distance)\n",
    "            elif iou > 0 and giou < 0.98:  # Partial overlap\n",
    "                uiou = 0.5 + 0.48 * (1 + giou) / 2\n",
    "            else:  # One box inside another\n",
    "                uiou = 0.98 + 0.02 * ((1 / similarity ** 2) + (1 - normalized_distance)) / 2\n",
    "        \n",
    "            return uiou \n",
    "            \n",
    "        return giou\n",
    "    return iou  # IoU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9d42b78-ecae-4cd6-b1bc-8153a250e913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1429])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "box1 = torch.tensor([1,1,3,3])\n",
    "box2 = torch.tensor([2,2,4,4])\n",
    "iou = intersection_over_union(box1, box2, box_format=\"corners\",)\n",
    "iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "669d261f-f245-4e02-a386-21828dd4b4e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7210])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uiou = intersection_over_union(box1, box2, box_format=\"corners\",UIoU=True)\n",
    "uiou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2da5281e-f488-4006-ad56-ef24de2bf6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression(\n",
    "        bboxes,\n",
    "        iou_threshold,\n",
    "        threshold,\n",
    "        box_format=\"corners\"\n",
    "    ):\n",
    "    \"\"\"\n",
    "       Does Non Max Suppression given bboxes\n",
    "\n",
    "       Parameters:\n",
    "           bboxes (list): list of lists containing all bboxes with each bboxes\n",
    "           specified as [class_pred, prob_score, x1, y1, x2, y2]\n",
    "           iou_threshold (float): threshold where predicted bboxes is correct\n",
    "           threshold (float): threshold to remove predicted bboxes (independent of IoU)\n",
    "           box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
    "\n",
    "       Returns:\n",
    "           list: bboxes after performing NMS given a specific IoU threshold\n",
    "       \"\"\"\n",
    "    # predictions = [[class_name=1, probablity_bounding_box=0.9, X1, y1, x2, y2], [], []]\n",
    "\n",
    "    assert type(bboxes) == list\n",
    "\n",
    "    bboxes = [box for box in bboxes if box[1] > threshold]\n",
    "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
    "    bboxes_after_nms = []\n",
    "\n",
    "    while bboxes:\n",
    "        chosen_box = bboxes.pop(0)\n",
    "\n",
    "        bboxes = [\n",
    "            box\n",
    "            for box in bboxes\n",
    "            if box[0] != chosen_box[0]\n",
    "            or intersection_over_union(torch.tensor(chosen_box[2:]),\n",
    "                                       torch.tensor(box[2:]),\n",
    "                                       box_format=box_format) < iou_threshold\n",
    "            ]\n",
    "\n",
    "        bboxes_after_nms.append(chosen_box)\n",
    "\n",
    "    return bboxes_after_nms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14349e95-806c-4ca8-901e-3e053610efec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from PIL import Image, ImageFile\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# from utils import ( iou_width_height as iou,\n",
    "#                     non_max_supression as nms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daa89735-421d-48e1-92bf-5100ace1a39e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def iou_width_height(boxes1, boxes2):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        boxes1 (tensor): width and height of the first bounding boxes\n",
    "        boxes2 (tensor): width and height of the second bounding boxes\n",
    "    Returns:\n",
    "        tensor: Intersection over union of the corresponding boxes\n",
    "    \"\"\"\n",
    "    intersection = torch.min(boxes1[..., 0], boxes2[..., 0]) * torch.min(\n",
    "        boxes1[..., 1], boxes2[..., 1]\n",
    "    )\n",
    "    union = (\n",
    "        boxes1[..., 0] * boxes1[..., 1] + boxes2[..., 0] * boxes2[..., 1] - intersection\n",
    "    )\n",
    "    return intersection / union\n",
    "\n",
    "x = torch.tensor([1,2])\n",
    "iou_width_height(x,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "39172a56-1190-402e-be93-6335a678c7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "class YOLODataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_file,\n",
    "        img_dir,\n",
    "        label_dir,\n",
    "        anchors,\n",
    "        image_size=416,\n",
    "        S=[13, 26, 52],\n",
    "        C=20,\n",
    "        transform=None,\n",
    "    ):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.image_size = image_size\n",
    "        self.transform = transform\n",
    "        self.S = S\n",
    "        self.anchors = torch.tensor(anchors[0] + anchors[1] + anchors[2])  # for all 3 scales\n",
    "        self.num_anchors = self.anchors.shape[0]\n",
    "        self.num_anchors_per_scale = self.num_anchors // 3\n",
    "        self.C = C\n",
    "        self.ignore_iou_thresh = 0.5\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label_path = os.path.join(self.label_dir, self.annotations.iloc[index, 1])\n",
    "        bboxes = np.roll(np.loadtxt(fname=label_path, delimiter=\" \", ndmin=2), 4, axis=1).tolist()\n",
    "        img_path = os.path.join(self.img_dir, self.annotations.iloc[index, 0])\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "        if self.transform:\n",
    "            augmentations = self.transform(image=image, bboxes=bboxes)\n",
    "            image = augmentations[\"image\"]\n",
    "            bboxes = augmentations[\"bboxes\"]\n",
    "\n",
    "        # Below assumes 3 scale predictions (as paper) and same num of anchors per scale\n",
    "        targets = [torch.zeros((self.num_anchors // 3, S, S, 6)) for S in self.S]\n",
    "        for box in bboxes:\n",
    "            iou_anchors = iou_width_height(torch.tensor(box[2:4]), self.anchors)\n",
    "            anchor_indices = iou_anchors.argsort(descending=True, dim=0)\n",
    "            x, y, width, height, class_label = box\n",
    "            has_anchor = [False] * 3  # each scale should have one anchor\n",
    "            for anchor_idx in anchor_indices:\n",
    "                scale_idx = anchor_idx // self.num_anchors_per_scale # which scale\n",
    "                anchor_on_scale = anchor_idx % self.num_anchors_per_scale # which anchor\n",
    "                S = self.S[scale_idx]\n",
    "                i, j = int(S * y), int(S * x)  # which cell xz\n",
    "                anchor_taken = targets[scale_idx][anchor_on_scale, i, j, 0]\n",
    "                if not anchor_taken and not has_anchor[scale_idx]:\n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 0] = 1\n",
    "                    x_cell, y_cell = S * x - j, S * y - i  # both between [0,1]\n",
    "                    width_cell, height_cell = (\n",
    "                        width * S,\n",
    "                        height * S,\n",
    "                    )  # can be greater than 1 since it's relative to cell\n",
    "                    box_coordinates = torch.tensor(\n",
    "                        [x_cell, y_cell, width_cell, height_cell]\n",
    "                    )\n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 1:5] = box_coordinates\n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 5] = int(class_label)\n",
    "                    has_anchor[scale_idx] = True\n",
    "\n",
    "                elif not anchor_taken and iou_anchors[anchor_idx] > self.ignore_iou_thresh:\n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 0] = -1  # ignore prediction\n",
    "\n",
    "        return image, tuple(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dd837ee8-12ec-452f-a6e9-e0f0d26c60cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(image, boxes):\n",
    "    \"\"\"Plots predicted bounding boxes on the image\"\"\"\n",
    "    cmap = plt.get_cmap(\"tab20b\")\n",
    "    class_labels = config.COCO_LABELS if config.DATASET=='COCO' else config.PASCAL_CLASSES\n",
    "    colors = [cmap(i) for i in np.linspace(0, 1, len(class_labels))]\n",
    "    im = np.array(image)\n",
    "    height, width, _ = im.shape\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots(1)\n",
    "    # Display the image\n",
    "    ax.imshow(im)\n",
    "\n",
    "    # box[0] is x midpoint, box[2] is width\n",
    "    # box[1] is y midpoint, box[3] is height\n",
    "\n",
    "    # Create a Rectangle patch\n",
    "    for box in boxes:\n",
    "        assert len(box) == 6, \"box should contain class pred, confidence, x, y, width, height\"\n",
    "        class_pred = box[0]\n",
    "        box = box[2:]\n",
    "        upper_left_x = box[0] - box[2] / 2\n",
    "        upper_left_y = box[1] - box[3] / 2\n",
    "        rect = patches.Rectangle(\n",
    "            (upper_left_x * width, upper_left_y * height),\n",
    "            box[2] * width,\n",
    "            box[3] * height,\n",
    "            linewidth=2,\n",
    "            edgecolor=colors[int(class_pred)],\n",
    "            facecolor=\"none\",\n",
    "        )\n",
    "        # Add the patch to the Axes\n",
    "        ax.add_patch(rect)\n",
    "        plt.text(\n",
    "            upper_left_x * width,\n",
    "            upper_left_y * height,\n",
    "            s=class_labels[int(class_pred)],\n",
    "            color=\"white\",\n",
    "            verticalalignment=\"top\",\n",
    "            bbox={\"color\": colors[int(class_pred)], \"pad\": 0},\n",
    "        )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3759a919-8d87-40ad-bb79-1f462f413b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cells_to_bboxes(predictions, anchors, S, is_preds=True):\n",
    "    \"\"\"\n",
    "    Scales the predictions coming from the model to\n",
    "    be relative to the entire image such that they for example later\n",
    "    can be plotted or.\n",
    "    INPUT:\n",
    "    predictions: tensor of size (N, 3, S, S, num_classes+5)\n",
    "    anchors: the anchors used for the predictions\n",
    "    S: the number of cells the image is divided in on the width (and height)\n",
    "    is_preds: whether the input is predictions or the true bounding boxes\n",
    "    OUTPUT:\n",
    "    converted_bboxes: the converted boxes of sizes (N, num_anchors, S, S, 1+5) with class index,\n",
    "                      object score, bounding box coordinates\n",
    "    \"\"\"\n",
    "    BATCH_SIZE = predictions.shape[0]\n",
    "    num_anchors = len(anchors)\n",
    "    box_predictions = predictions[..., 1:5]\n",
    "    if is_preds:\n",
    "        anchors = anchors.reshape(1, len(anchors), 1, 1, 2)\n",
    "        box_predictions[..., 0:2] = torch.sigmoid(box_predictions[..., 0:2])\n",
    "        box_predictions[..., 2:] = torch.exp(box_predictions[..., 2:]) * anchors\n",
    "        scores = torch.sigmoid(predictions[..., 0:1])\n",
    "        best_class = torch.argmax(predictions[..., 5:], dim=-1).unsqueeze(-1)\n",
    "    else:\n",
    "        scores = predictions[..., 0:1]\n",
    "        best_class = predictions[..., 5:6]\n",
    "\n",
    "    cell_indices = (\n",
    "        torch.arange(S)\n",
    "        .repeat(predictions.shape[0], 3, S, 1)\n",
    "        .unsqueeze(-1)\n",
    "        .to(predictions.device)\n",
    "    )\n",
    "    x = 1 / S * (box_predictions[..., 0:1] + cell_indices)\n",
    "    y = 1 / S * (box_predictions[..., 1:2] + cell_indices.permute(0, 1, 3, 2, 4))\n",
    "    w_h = 1 / S * box_predictions[..., 2:4]\n",
    "    converted_bboxes = torch.cat((best_class, scores, x, y, w_h), dim=-1).reshape(BATCH_SIZE, num_anchors * S * S, 6)\n",
    "    return converted_bboxes.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e0d94c-b496-4f73-ac97-d7fd9e512e06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca3b0d0-ed97-4554-8722-982e547e91bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03010b90-3981-4388-aeda-f7fe53e4abae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87334315-916f-4377-b640-841d86cad1a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e89aabd-8fa0-44a6-9b7b-ce1964ca2901",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733335c7-7838-49a4-a286-752c0a096296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e247cb2-d24c-4403-a323-b98fc2e019d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5499f514-56a7-4cdc-80e1-f2214fb5b434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9816f339-8855-47a8-b743-d05449b7abfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c605efe-d794-4af0-8c98-09055ccb2647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f5a7d7-fea0-4a97-a425-f9a02da1eb96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40903071-ba5f-4226-ba1b-5a6f15c455d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fbd876-c1c0-4fae-854e-d72116e65c74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1cfa71-42b4-43ba-b55b-ae02943bcc69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d98c6b9-d757-41eb-b37a-4321df7e85e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8621748-0d59-4ef0-81ad-ef0de452bb51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b11e27-4b57-46c9-858a-25ae3e4cf027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59e0ffa-52c7-4cce-abed-77db3f4554c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8bfd88-f8ff-4a60-b64f-d6d2b5018159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a6ca18-10d7-4adb-9f95-9b291b422ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce72b00d-bef9-4bc2-94b2-28b6e97e18e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c371574-8c90-4757-9899-8583e0550d51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e21333-932e-4ed7-8340-703bd5832359",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce966a94-fc86-488a-9b1e-fb4ab3ccc767",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b1e987-c2cd-4f88-a75d-615293518eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dd0f1cec-e1a8-4d74-9075-1888bafd1bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test():\n",
    "    anchors = ANCHORS\n",
    "\n",
    "    transform = test_transforms\n",
    "\n",
    "    dataset = YOLODataset(\n",
    "        \"D:/Object detction/data/train.csv\",\n",
    "        \"D:/Object detction/data/images/\",\n",
    "        \"D:/Object detction/data/labels/\",\n",
    "        S=[13, 26, 52],\n",
    "        anchors=anchors,\n",
    "        transform=transform,\n",
    "    )\n",
    "    S = [13, 26, 52]\n",
    "    scaled_anchors = torch.tensor(anchors) / (\n",
    "        1 / torch.tensor(S).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2)\n",
    "    )\n",
    "    loader = DataLoader(dataset=dataset, batch_size=1, shuffle=True)\n",
    "    for x, y in loader:\n",
    "        boxes = []\n",
    "\n",
    "        for i in range(y[0].shape[1]):\n",
    "            anchor = scaled_anchors[i]\n",
    "            print(anchor.shape)\n",
    "            print(y[i].shape)\n",
    "            boxes += cells_to_bboxes(\n",
    "                y[i], is_preds=False, S=y[i].shape[2], anchors=anchor\n",
    "            )[0]\n",
    "        boxes = non_max_suppression(boxes, iou_threshold=1, threshold=0.7, box_format=\"midpoint\")\n",
    "        print(boxes)\n",
    "        plot_image(x[0].permute(1, 2, 0).to(\"cpu\"), boxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "753ee3f1-1247-48bf-b336-87f259f7ce15",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANCHORS = [\n",
    "    [(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],\n",
    "    [(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)],\n",
    "    [(0.02, 0.03), (0.04, 0.07), (0.08, 0.06)],\n",
    "]  # Note these have been rescaled to be between [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c79ee332-de50-4c76-9e9a-9821344ca86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4a8262c1-6ba4-47f7-abd3-399e35ab883f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "test_transforms = A.Compose(\n",
    "    [\n",
    "        A.LongestMaxSize(max_size=IMAGE_SIZE),\n",
    "        A.PadIfNeeded(\n",
    "            min_height=IMAGE_SIZE, min_width=IMAGE_SIZE, border_mode=cv2.BORDER_CONSTANT\n",
    "        ),\n",
    "        A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255,),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format=\"yolo\", min_visibility=0.4, label_fields=[]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "629144d2-1ec2-4638-bbff-62c2ff14c8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n",
      "torch.Size([1, 3, 13, 13, 6])\n",
      "torch.Size([3, 2])\n",
      "torch.Size([1, 3, 26, 26, 6])\n",
      "torch.Size([3, 2])\n",
      "torch.Size([1, 3, 52, 52, 6])\n",
      "[[3.0, 1.0, 0.4520000219345093, 0.42493072152137756, 0.14400000870227814, 0.12577413022518158], [2.0, 1.0, 0.9229999780654907, 0.5447155833244324, 0.05799997225403786, 0.05789604038000107], [2.0, 1.0, 0.9229999780654907, 0.5447155833244324, 0.05799997225403786, 0.05789604038000107], [3.0, 1.0, 0.4520000219345093, 0.42493072152137756, 0.14400000870227814, 0.12577413022518158], [3.0, 1.0, 0.4520000219345093, 0.42493072152137756, 0.14400000870227814, 0.12577413022518158], [2.0, 1.0, 0.9229999780654907, 0.5447155833244324, 0.05799997225403786, 0.05789604038000107]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'DATASET'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[54], line 31\u001b[0m, in \u001b[0;36mtest\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m boxes \u001b[38;5;241m=\u001b[39m non_max_suppression(boxes, iou_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m, box_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmidpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(boxes)\n\u001b[1;32m---> 31\u001b[0m \u001b[43mplot_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[52], line 4\u001b[0m, in \u001b[0;36mplot_image\u001b[1;34m(image, boxes)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Plots predicted bounding boxes on the image\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m cmap \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mget_cmap(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtab20b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m class_labels \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mCOCO_LABELS \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDATASET\u001b[49m\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCOCO\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m config\u001b[38;5;241m.\u001b[39mPASCAL_CLASSES\n\u001b[0;32m      5\u001b[0m colors \u001b[38;5;241m=\u001b[39m [cmap(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(class_labels))]\n\u001b[0;32m      6\u001b[0m im \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(image)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'DATASET'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3aed0077-c40c-4340-8a25-d1a1f7b49839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "label_file = 'D:/Object detction/data\\labels/009620.txt'\n",
    "print(os.path.exists(label_file))  # Should return True if the file exists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbd40ed-3dba-4a47-8d55-d3e19009252e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5bf54d0-cb1e-4be1-9fc3-2dd8f188ecc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"example.txt\"\n",
    "\n",
    "# Open the file in write mode and write content\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(\"1,2,3,2.5,4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e36ed2da-611a-45bb-bfdf-37ab69334dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2.0, 3.0, 2.5, 4.0, 1.0]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.roll(np.loadtxt(\"example.txt\",delimiter= \",\",ndmin=2),4, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6501496-808b-4d33-b7ea-ce9d526cfc94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Desktop/Deep-Learning/Pytorch'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join('C:/Desktop/Deep-Learning/', 'Pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e17fb99-b8be-471d-882c-5155d629fc74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ntpath' from 'C:\\\\Users\\\\Nihar\\\\anaconda3\\\\envs\\\\tf\\\\lib\\\\ntpath.py'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f901489b-db94-4798-a503-4302786cf029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.98512046, 0.56246035],\n",
       "       [0.39358461, 0.07049141],\n",
       "       [0.31577928, 0.71035451],\n",
       "       [0.82516357, 0.98282931],\n",
       "       [0.68615355, 0.68998152]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.rand(10).reshape(5,2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4b1da64-0070-4da0-8e38-ca978c906363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.39358461, 0.07049141],\n",
       "       [0.31577928, 0.71035451],\n",
       "       [0.82516357, 0.98282931],\n",
       "       [0.68615355, 0.68998152],\n",
       "       [0.98512046, 0.56246035]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.roll(x, -1,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17309ca5-2ce0-4070-8489-0edb5d4aea19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de47de3-4b46-4269-8cce-81c0002375db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02bc21fb-f9d8-4464-8a56-650bb8798dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13, 26, 52])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = np.array([13, 26, 52])\n",
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27d13caa-eb5f-413b-8036-3370b7dbe0f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13, 13, 6])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72a4c9ca-d31f-4182-b801-45d591f257e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 26, 26, 6])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d726aa0-f641-414e-86cb-9e682fdbbd5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 52, 52, 6])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4ea546e-e0ba-4038-8e7d-6336f0f58ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9627, 0.0426, 0.1572],\n",
       "       [0.9534, 0.8682, 0.0138],\n",
       "       [0.0743, 0.4189, 0.6775]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f65b7ec-b408-4bba-acb6-bb56b338f9b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],\n",
       " [(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)],\n",
       " [(0.02, 0.03), (0.04, 0.07), (0.08, 0.06)]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchors = [\n",
    "    [(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],\n",
    "    [(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)],\n",
    "    [(0.02, 0.03), (0.04, 0.07), (0.08, 0.06)],\n",
    "] \n",
    "anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d2ed7f7b-607e-47d1-bafa-d03d8ba2b95d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.25, 0.29, 0.9, 0.7]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "box = [.25,.29,.9,.7]\n",
    "box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f0eef121-4049-4ddc-9676-0bf6a8eb1ed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2800, 0.2200],\n",
       "        [0.3800, 0.4800],\n",
       "        [0.9000, 0.7800],\n",
       "        [0.0700, 0.1500],\n",
       "        [0.1500, 0.1100],\n",
       "        [0.1400, 0.2900],\n",
       "        [0.0200, 0.0300],\n",
       "        [0.0400, 0.0700],\n",
       "        [0.0800, 0.0600]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchors = torch.tensor(anchors[0] + anchors[1] + anchors[2])\n",
    "anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "12241c35-68e9-44a6-a45d-af0cbd898dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0978, 0.2895, 0.8974, 0.0167, 0.0262, 0.0644, 0.0010, 0.0044, 0.0076])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ious = iou_width_height(torch.tensor(box[2:4]), anchors)\n",
    "ious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "21bd116b-b721-432b-9f67-aa856bf744f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9000, 0.7000])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(box[2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b0d2365e-4b3e-476e-94a9-bcaa42c2f53e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1, 0, 5, 4, 3, 8, 7, 6])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_indices = ious.argsort(descending=True,dim=0)\n",
    "anchor_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f3f6d2c-7bee-456a-b046-f3c41f9ae54d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_anchors= anchors.shape[0]\n",
    "num_anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b745801-99b6-43ad-9cca-ff8217cdc17e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_anchors_per_scale = num_anchors // 3\n",
    "num_anchors_per_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "794156db-7131-42fd-af09-56ad47443859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]]]]),\n",
       " tensor([[[[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]]]]),\n",
       " tensor([[[[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]]]])]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = np.array([13, 26, 52])\n",
    "targets = [torch.zeros((num_anchors // 3, s, s, 6 )) for s in S] \n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5f7f40e9-51e8-40d9-9d04-a01cbfcedea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 13, 13, 6]),\n",
       " torch.Size([3, 26, 26, 6]),\n",
       " torch.Size([3, 52, 52, 6]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[0].shape, targets[1].shape, targets[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ca9e46e8-1fe6-46b4-bcdf-7a1f80a32c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0978, 0.2895, 0.8974, 0.0167, 0.0262, 0.0644, 0.0010, 0.0044, 0.0076]),\n",
       " tensor([2, 1, 0, 5, 4, 3, 8, 7, 6]),\n",
       " 3)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ious, anchor_indices, num_anchors_per_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d3fcd37a-f15b-4b0e-85d6-25696208883e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(2)\n",
      "tensor(0) tensor(1)\n",
      "tensor(0) tensor(0)\n",
      "tensor(1) tensor(2)\n",
      "tensor(1) tensor(1)\n",
      "tensor(1) tensor(0)\n",
      "tensor(2) tensor(2)\n",
      "tensor(2) tensor(1)\n",
      "tensor(2) tensor(0)\n"
     ]
    }
   ],
   "source": [
    "for anchor_idx in anchor_indices:\n",
    "                scale_idx = anchor_idx // num_anchors_per_scale # 0, 1, 2\n",
    "                anchor_on_scale = anchor_idx % num_anchors_per_scale\n",
    "                print(scale_idx, anchor_on_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7aeae821-27c8-445f-b8a1-6a4f56d08ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287587ea-9150-46a5-8850-cf7e34b3af34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
