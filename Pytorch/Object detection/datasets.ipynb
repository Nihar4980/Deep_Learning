{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1938e3c-2419-409b-8ccc-74e5b17a67e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Dictionary of class labels\n",
    "classes = {\n",
    "    0: \"person\",\n",
    "    1: \"bicycle\",\n",
    "    2: \"car\",\n",
    "    3: \"motorcycle\",\n",
    "    4: \"bus\",\n",
    "    5: \"truck\",\n",
    "    6: \"traffic_light\",\n",
    "    7: \"stop_sign\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e5cd56a-a853-46c2-8152-843896085c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Check_images_no_bounding_boxes(image_roots, label_roots):\n",
    "    \n",
    "    image_root = Path(image_roots)\n",
    "    label_root = Path(label_roots)\n",
    "    image_dirs = sorted([d for d in image_root.iterdir() if d.is_dir()])\n",
    "    label_dirs = sorted([d for d in label_root.iterdir() if d.is_dir()])\n",
    "    \n",
    "    # Build full list of image-label pairs across all file folders\n",
    "    # data_pairs = []\n",
    "    \n",
    "    for i,j in zip(image_dirs, label_dirs):\n",
    "        img_folder = i/\"ring_front_center\"\n",
    "        lbl_folder = j/ \"ring_front_center\"\n",
    "        #data_pairs.append(img_folder)\n",
    "        images = [d for d in img_folder.iterdir()]\n",
    "        labels = [d for d in lbl_folder.iterdir()]   \n",
    "        if len(images) != len(labels):\n",
    "            ima = [ i.stem.split(\"_\")[-1] for i in images]\n",
    "            lbl = [ i.stem.split(\"_\")[-1] for i in labels]\n",
    "            difference = [(i,x) for i,x in enumerate(ima) if x not in lbl]\n",
    "            for i,_ in difference:\n",
    "                image_path = images[i]\n",
    "                \n",
    "                # Load image using OpenCV\n",
    "                img = cv2.imread(image_path)\n",
    "                \n",
    "                if img is None:\n",
    "                    print(f\"Error loading image: {image_path}\")\n",
    "                    continue\n",
    "            \n",
    "                # Convert BGR to RGB (since OpenCV loads images in BGR format)\n",
    "                img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "                # Display the image using Matplotlib\n",
    "                plt.figure(figsize=(20, 10))\n",
    "                plt.imshow(img_rgb)\n",
    "                plt.axis(\"off\")  # Hide axis\n",
    "                plt.title(f\"Image: {image_path}\")\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98b32289-9bcb-4d55-b30a-5ac74801b19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_nbx (image_roots, label_roots):\n",
    "    \n",
    "    image_root = Path(image_roots)\n",
    "    label_root = Path(label_roots)\n",
    "    image_dirs = sorted([d for d in image_root.iterdir() if d.is_dir()])\n",
    "    label_dirs = sorted([d for d in label_root.iterdir() if d.is_dir()])\n",
    "    \n",
    "    # Build full list of image-label pairs across all file folders\n",
    "    data_pairs = []\n",
    "    \n",
    "    for i,j in zip(image_dirs, label_dirs):\n",
    "        img_folder = i/\"ring_front_center\"\n",
    "        lbl_folder = j/ \"ring_front_center\"\n",
    "        #data_pairs.append(img_folder)\n",
    "        images = [d for d in img_folder.iterdir()]\n",
    "        labels = [d for d in lbl_folder.iterdir()]   \n",
    "        if len(images) != len(labels):\n",
    "            data_pairs.append((img_folder,lbl_folder))\n",
    "    return data_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "980989dd-6353-4631-966c-a4712181b3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train paths\n",
    "image_train = r\"/home/nihar22951/Online_detection/Online_detection/Datasets/Agroverse/Argoverse-1.1/images/train\"\n",
    "label_train = r\"/home/nihar22951/Online_detection/Online_detection/Datasets/Agroverse/Argoverse-1.1/labels/train\"\n",
    "\n",
    "# Val paths\n",
    "image_val = r\"/home/nihar22951/Online_detection/Online_detection/Datasets/Agroverse/Argoverse-1.1/images/val\"\n",
    "label_val = r\"/home/nihar22951/Online_detection/Online_detection/Datasets/Agroverse/Argoverse-1.1/labels/val\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "babebe6f-8f61-4c33-bad4-2f8e49a7c53f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Check_images_no_bounding_boxes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mCheck_images_no_bounding_boxes\u001b[49m(image_train, label_train)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Check_images_no_bounding_boxes' is not defined"
     ]
    }
   ],
   "source": [
    "Check_images_no_bounding_boxes(image_train, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef6daf15-47c1-4df6-9b9d-7bc70563510d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check_images_no_bounding_boxes(image_val, label_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4666e5fe-81d6-44ff-86da-e4517468f137",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_nbx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m image_nbx1 \u001b[38;5;241m=\u001b[39m \u001b[43mimage_nbx\u001b[49m(image_train, label_train)\n\u001b[1;32m      2\u001b[0m image_nbx1\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_nbx' is not defined"
     ]
    }
   ],
   "source": [
    "image_nbx1 = image_nbx(image_train, label_train)\n",
    "image_nbx1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54bddbed-8f11-4b14-93a2-a53229f7f785",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_nbx1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m img_folder, lbl_folder \u001b[38;5;241m=\u001b[39m \u001b[43mimage_nbx1\u001b[49m[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m], image_nbx1[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      3\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m img_folder\u001b[38;5;241m.\u001b[39miterdir() \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39msuffix \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      4\u001b[0m labels \u001b[38;5;241m=\u001b[39m {f\u001b[38;5;241m.\u001b[39mstem: f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m lbl_folder\u001b[38;5;241m.\u001b[39miterdir() \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39msuffix \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m} \u001b[38;5;28;01mif\u001b[39;00m lbl_folder\u001b[38;5;241m.\u001b[39mexists() \u001b[38;5;28;01melse\u001b[39;00m {}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_nbx1' is not defined"
     ]
    }
   ],
   "source": [
    "img_folder, lbl_folder = image_nbx1[1][0], image_nbx1[1][1]\n",
    "    \n",
    "images = sorted([f for f in img_folder.iterdir() if f.suffix == \".jpg\"])\n",
    "labels = {f.stem: f for f in lbl_folder.iterdir() if f.suffix == \".txt\"} if lbl_folder.exists() else {}\n",
    "\n",
    "data_pairs = []\n",
    "for i in range(len(images)):  \n",
    "    img_prev = images[i] if i == 0 else images[i - 1]\n",
    "    img_curr = images[i]\n",
    "    img_next = images[i] if i == len(images) - 1 else images[i + 1]\n",
    "\n",
    "    # Get corresponding label for img_curr (instead of img_next)\n",
    "    lbl_next = labels.get(img_next.stem, \"EMPTY_BOUNDING_BOX\")  \n",
    "\n",
    "    data_pairs.append(str(lbl_next))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a75d239-5643-49ca-ae0f-b5924680b09c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mimages\u001b[49m), \u001b[38;5;28mlen\u001b[39m(labels)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'images' is not defined"
     ]
    }
   ],
   "source": [
    "len(images), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8d822c6-5214-4f50-8c72-ad7b2c7e1642",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_pairs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata_pairs\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_pairs' is not defined"
     ]
    }
   ],
   "source": [
    "data_pairs[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6bc231d5-27d0-4ae2-8bfb-74f5b465fb43",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_pairs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m empty_label_pairs \u001b[38;5;241m=\u001b[39m [pair \u001b[38;5;28;01mfor\u001b[39;00m pair \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdata_pairs\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m pair[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEMPTY_BOUNDING_BOX\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal empty label pairs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(empty_label_pairs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_pairs' is not defined"
     ]
    }
   ],
   "source": [
    "empty_label_pairs = [pair for pair in data_pairs if pair[2] == \"EMPTY_BOUNDING_BOX\"]\n",
    "\n",
    "print(f\"Total empty label pairs: {len(empty_label_pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df68a1d7-5430-43b7-be88-13657fcc6a31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daaadf71-850b-4c75-837d-228191b78fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def dataset(image_path, label_path):\n",
    "    image_root = Path(image_path)\n",
    "    label_root = Path(label_path)\n",
    "    \n",
    "    # Get sorted list of subdirectories\n",
    "    image_dirs = sorted([d for d in image_root.iterdir() if d.is_dir()])\n",
    "    label_dirs = sorted([d for d in label_root.iterdir() if d.is_dir()])\n",
    "    \n",
    "    data_pairs = []\n",
    "    \n",
    "    for img_dir, lbl_dir in zip(image_dirs, label_dirs):\n",
    "        img_folder = img_dir / \"ring_front_center\"\n",
    "        lbl_folder = lbl_dir / \"ring_front_center\"\n",
    "        \n",
    "        if not img_folder.exists():\n",
    "            continue  # Skip if images folder doesn't exist\n",
    "    \n",
    "        # Get sorted list of image and label files\n",
    "        images = sorted([f for f in img_folder.iterdir() if f.suffix == \".jpg\"])\n",
    "        labels = {f.stem: f for f in lbl_folder.iterdir() if f.suffix == \".txt\"} if lbl_folder.exists() else {}\n",
    "        \n",
    "        for i in range(len(images)):  \n",
    "            img_prev = images[i] if i == 0 else images[i - 1]\n",
    "            img_curr = images[i]\n",
    "            img_next = images[i] if i == len(images) - 1 else images[i + 1]\n",
    "            \n",
    "            # Get corresponding label for img_curr (instead of img_next)\n",
    "            lbl_next = labels.get(img_next.stem, \"EMPTY_BOUNDING_BOX\")  \n",
    "        \n",
    "            data_pairs.append((str(img_prev), str(img_curr), str(lbl_next)))\n",
    "\n",
    "    return data_pairs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f74e94de-bdf8-47b4-ba6c-743428d9b499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39384, 15062)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = dataset(image_train, label_train)\n",
    "val_data = dataset(image_val, label_val)\n",
    "\n",
    "len(train_data), len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd28ceac-3975-4dcb-b0c5-b7c56e2badd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3862efa-8ebe-497a-99ca-968e2687c2f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m img_curr \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(img_curr, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Convert numpy array to torch tensor (HWC → CHW)\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m img_prev \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_numpy(img_prev)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m  \u001b[38;5;66;03m# Normalize to [0,1]\u001b[39;00m\n\u001b[1;32m     23\u001b[0m img_curr \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(img_curr)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m  \u001b[38;5;66;03m# Normalize\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Load label or assign empty bounding box\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "def load_label( label_path):\n",
    "        \"\"\"Loads bounding box labels from a text file.\"\"\"\n",
    "        boxes = []\n",
    "        with open(label_path, \"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                values = list(map(float, line.strip().split()))\n",
    "                boxes.append(values)  # Each row: [class, x, y, w, h]\n",
    "        \n",
    "        return torch.tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "data = []\n",
    "\n",
    "for img_prev_path,img_curr_path, lbl_curr_path in train_data:\n",
    "    img_prev = cv2.imread(str(img_prev_path))  #  BGR format\n",
    "    img_curr = cv2.imread(str(img_curr_path))\n",
    "    \n",
    "    # Convert BGR to RGB\n",
    "    img_prev = cv2.cvtColor(img_prev, cv2.COLOR_BGR2RGB)\n",
    "    img_curr = cv2.cvtColor(img_curr, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Convert numpy array to torch tensor (HWC → CHW)\n",
    "    img_prev = torch.from_numpy(img_prev).float().permute(2, 0, 1) / 255.0  # Normalize to [0,1]\n",
    "    img_curr = torch.from_numpy(img_curr).float().permute(2, 0, 1) / 255.0  # Normalize\n",
    "    \n",
    "    # Load label or assign empty bounding box\n",
    "    if lbl_curr_path == \"EMPTY_BOUNDING_BOX\":\n",
    "        label = torch.zeros((1, 5))  # Empty label with shape (1, 5) [class, x, y, w, h]\n",
    "    else:\n",
    "        label = load_label(lbl_curr_path)\n",
    "    \n",
    "    data.append((img_curr, label))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13d55a2-ba77-47db-9a3b-a34db8b3a088",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cca2779a-e12f-463d-817b-ce8053ec1b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total empty label pairs for train: 12 and val : 7\n"
     ]
    }
   ],
   "source": [
    "empty_label_train = [pair for pair in train_data if pair[2] == \"EMPTY_BOUNDING_BOX\"]\n",
    "empty_label_val = [pair for pair in val_data if pair[2] == \"EMPTY_BOUNDING_BOX\"]\n",
    "print(f\"Total empty label pairs for train: {len(empty_label_train)} and val : {len(empty_label_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "534271d3-5741-4b26-8639-fe52a53af214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "889 882\n"
     ]
    }
   ],
   "source": [
    "image_root = Path(image_val)\n",
    "label_root = Path(label_val)\n",
    "image_dirs = sorted([d for d in image_root.iterdir() if d.is_dir()])\n",
    "label_dirs = sorted([d for d in label_root.iterdir() if d.is_dir()])\n",
    "\n",
    "# Build full list of image-label pairs across all file folders\n",
    "data = []\n",
    "\n",
    "for i,j in zip(image_dirs, label_dirs):\n",
    "    img_folder = i/\"ring_front_center\"\n",
    "    lbl_folder = j/ \"ring_front_center\"\n",
    "    # Get sorted list of image and label files\n",
    "    images = sorted([f for f in img_folder.iterdir() if f.suffix == \".jpg\"])\n",
    "    labels = {f.stem: f for f in lbl_folder.iterdir() if f.suffix == \".txt\"} if lbl_folder.exists() else {} \n",
    "    if len(images)!= len(labels):\n",
    "        print(len(images),len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1678e8f3-6f7d-4d57-98b6-acd1ea6bf81d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "AgroverseDataset.__init__() got an unexpected keyword argument 'image_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 98\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(boxes, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Create dataset\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mAgroverseDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m640\u001b[39;49m\n\u001b[1;32m    102\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n",
      "\u001b[0;31mTypeError\u001b[0m: AgroverseDataset.__init__() got an unexpected keyword argument 'image_size'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Train paths\n",
    "image_train = r\"/home/nihar22951/Online_detection/Online_detection/Datasets/Agroverse/Argoverse-1.1/images/train\"\n",
    "label_train = r\"/home/nihar22951/Online_detection/Online_detection/Datasets/Agroverse/Argoverse-1.1/labels/train\"\n",
    "\n",
    "# Val paths\n",
    "image_val = r\"/home/nihar22951/Online_detection/Online_detection/Datasets/Agroverse/Argoverse-1.1/images/val\"\n",
    "label_val = r\"/home/nihar22951/Online_detection/Online_detection/Datasets/Agroverse/Argoverse-1.1/labels/val\"\n",
    "\n",
    "class AgroverseDataset(Dataset):\n",
    "    def __init__(self, image_path, label_path, transform=None):\n",
    "        \n",
    "        self.image_root = Path(image_path)\n",
    "        self.label_root = Path(label_path)\n",
    "        self.transform = transform  # Optional image transformations\n",
    "\n",
    "        # Get sorted list of subdirectories\n",
    "        self.image_dirs = sorted([d for d in self.image_root.iterdir() if d.is_dir()])\n",
    "        self.label_dirs = sorted([d for d in self.label_root.iterdir() if d.is_dir()])\n",
    "\n",
    "        # Build list of (prev_img, curr_img, label) pairs\n",
    "        self.data_pairs = self.create_data_pairs()\n",
    "\n",
    "    def create_data_pairs(self):\n",
    "        \"\"\"Builds (prev, curr, label) pairs.\"\"\"\n",
    "        data_pairs = []\n",
    "        \n",
    "        for img_dir, lbl_dir in zip(self.image_dirs, self.label_dirs):\n",
    "            img_folder = img_dir / \"ring_front_center\"\n",
    "            lbl_folder = lbl_dir / \"ring_front_center\"\n",
    "\n",
    "            if not img_folder.exists():\n",
    "                continue  # Skip if images folder doesn't exist\n",
    "            \n",
    "            # Get sorted list of image and label files\n",
    "            images = sorted([f for f in img_folder.iterdir() if f.suffix == \".jpg\"])\n",
    "            labels = {f.stem: f for f in lbl_folder.iterdir() if f.suffix == \".txt\"} if lbl_folder.exists() else {}\n",
    "            for i in range(len(images)):  \n",
    "                img_prev = images[i] if i == 0 else images[i - 1]\n",
    "                img_curr = images[i]\n",
    "                img_next = images[i] if i == len(images) - 1 else images[i + 1]\n",
    "                \n",
    "                # Get corresponding label for img_curr (instead of img_next)\n",
    "                lbl_next = labels.get(img_next.stem, \"EMPTY_BOUNDING_BOX\")  \n",
    "                data_pairs.append((str(img_prev), str(img_curr), str(lbl_next)))\n",
    "\n",
    "        return data_pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of data pairs.\"\"\"\n",
    "        return len(self.data_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Loads an image pair and its corresponding label.\"\"\"\n",
    "        img_prev_path, img_curr_path, lbl_curr_path = self.data_pairs[idx]\n",
    "\n",
    "        # Load images\n",
    "        img_prev = cv2.imread(str(img_prev_path))  # BGR format\n",
    "        img_curr = cv2.imread(str(img_curr_path))\n",
    "\n",
    "        # Convert BGR to RGB\n",
    "        img_prev = cv2.cvtColor(img_prev, cv2.COLOR_BGR2RGB)\n",
    "        img_curr = cv2.cvtColor(img_curr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Convert numpy array to torch tensor (HWC → CHW)\n",
    "        img_prev = torch.from_numpy(img_prev).float().permute(2, 0, 1) / 255.0  # Normalize to [0,1]\n",
    "        img_curr = torch.from_numpy(img_curr).float().permute(2, 0, 1) / 255.0  # Normalize\n",
    "\n",
    "        # Apply transformations if provided\n",
    "        if self.transform:\n",
    "            img_prev = self.transform(img_prev)\n",
    "            img_curr = self.transform(img_curr)\n",
    "\n",
    "        # Load label or assign empty bounding box\n",
    "        if lbl_curr_path == \"EMPTY_BOUNDING_BOX\":\n",
    "            label = torch.zeros((1, 5))  # Empty label with shape (1, 5) [class, x, y, w, h]\n",
    "        else:\n",
    "            label = self.load_label(lbl_curr_path)\n",
    "\n",
    "        return img_prev, img_curr, label\n",
    "\n",
    "    def load_label(self, label_path):\n",
    "        \"\"\"Loads bounding box labels from a text file.\"\"\"\n",
    "        boxes = []\n",
    "        with open(label_path, \"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                values = list(map(float, line.strip().split()))\n",
    "                boxes.append(values)  # Each row: [class, x, y, w, h]\n",
    "        \n",
    "        return torch.tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "# Create dataset\n",
    "dataset = AgroverseDataset(\n",
    "    image_path=image_train, \n",
    "    label_path=label_train, \n",
    "    image_size= 640\n",
    ")\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs_prev, imgs_curr, labels = zip(*batch)  # Unpack batch\n",
    "\n",
    "    imgs_prev = torch.stack(imgs_prev)  # Stack images into a tensor\n",
    "    imgs_curr = torch.stack(imgs_curr)\n",
    "\n",
    "    # Pad labels to the max number of bounding boxes in the batch\n",
    "    labels = [torch.tensor(lbl) for lbl in labels]  # Convert lists to tensors\n",
    "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=-1)  # Padding value -1\n",
    "\n",
    "    return imgs_prev, imgs_curr, labels_padded\n",
    "\n",
    "\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Iterate over data\n",
    "\n",
    "for img_prev, img_curr, labels in dataloader:\n",
    "    print(img_prev.shape, img_curr.shape, labels.shape)  # Example output\n",
    "    break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dcb6a9-950a-4c5e-ac7b-f91239ca2c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae935a0-795c-4a2c-b756-0712fd167b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs_prev, imgs_curr, labels = zip(*batch)  # Unpack batch\n",
    "\n",
    "    imgs_prev = torch.stack(imgs_prev)  # Stack images into a tensor\n",
    "    imgs_curr = torch.stack(imgs_curr)\n",
    "\n",
    "    # Pad labels to the max number of bounding boxes in the batch\n",
    "    labels = [torch.tensor(lbl) for lbl in labels]  # Convert lists to tensors\n",
    "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=-1)  # Padding value -1\n",
    "\n",
    "    return imgs_prev, imgs_curr, labels_padded\n",
    "\n",
    "\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Iterate over data\n",
    "for img_prev, img_curr, labels in dataloader:\n",
    "    print(img_prev.shape, img_curr.shape, labels.shape)  # Example output\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "6c64a5a8-1637-485b-9fc6-b9bd79edf41b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nvidia' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[164], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnvidia\u001b[49m\u001b[38;5;241m-\u001b[39msmi\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nvidia' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f01bdb2-f552-49c1-91d6-6b7fc985a383",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 151\u001b[0m\n\u001b[1;32m    148\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# Iterate over data\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_prev, img_curr, labels \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28mprint\u001b[39m(img_prev\u001b[38;5;241m.\u001b[39mshape, img_curr\u001b[38;5;241m.\u001b[39mshape, labels\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Expected: (32, 3, 640, 640)\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/nihar22951_environment/lib/python3.10/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/.conda/envs/nihar22951_environment/lib/python3.10/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.conda/envs/nihar22951_environment/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.conda/envs/nihar22951_environment/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[5], line 69\u001b[0m, in \u001b[0;36mAgroverseDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     66\u001b[0m img_curr \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(img_curr, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Resize images & adjust bounding boxes\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m img_prev, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize_image_with_bbox\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_prev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m img_curr, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresize_image_with_bbox(img_curr, lbl_curr_path)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Convert numpy array to torch tensor (HWC → CHW)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 101\u001b[0m, in \u001b[0;36mAgroverseDataset.resize_image_with_bbox\u001b[0;34m(self, image, label_path)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image_padded, torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m))  \u001b[38;5;66;03m# Empty label with shape (1,5)\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Load bounding boxes\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m bboxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Adjust bounding boxes\u001b[39;00m\n\u001b[1;32m    104\u001b[0m new_bboxes \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[5], line 121\u001b[0m, in \u001b[0;36mAgroverseDataset.load_label\u001b[0;34m(self, label_path)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loads bounding box labels from a text file.\"\"\"\u001b[39;00m\n\u001b[1;32m    120\u001b[0m boxes \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mreadlines():\n\u001b[1;32m    123\u001b[0m         values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mfloat\u001b[39m, line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit()))\n",
      "File \u001b[0;32m~/.conda/envs/nihar22951_environment/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Define target image size\n",
    "IMAGE_SIZE = (640, 640)  # (height, width)\n",
    "\n",
    "class AgroverseDataset(Dataset):\n",
    "    def __init__(self, image_path, label_path, transform=None):\n",
    "        self.image_root = Path(image_path)\n",
    "        self.label_root = Path(label_path)\n",
    "        self.transform = transform  # Optional image transformations\n",
    "\n",
    "        # Get sorted list of subdirectories\n",
    "        self.image_dirs = sorted([d for d in self.image_root.iterdir() if d.is_dir()])\n",
    "        self.label_dirs = sorted([d for d in self.label_root.iterdir() if d.is_dir()])\n",
    "\n",
    "        # Build list of (prev_img, curr_img, label) pairs\n",
    "        self.data_pairs = self.create_data_pairs()\n",
    "\n",
    "    def create_data_pairs(self):\n",
    "        \"\"\"Builds (prev, curr, label) pairs.\"\"\"\n",
    "        data_pairs = []\n",
    "        \n",
    "        for img_dir, lbl_dir in zip(self.image_dirs, self.label_dirs):\n",
    "            img_folder = img_dir / \"ring_front_center\"\n",
    "            lbl_folder = lbl_dir / \"ring_front_center\"\n",
    "\n",
    "            if not img_folder.exists():\n",
    "                continue  # Skip if images folder doesn't exist\n",
    "            \n",
    "            # Get sorted list of image and label files\n",
    "            images = sorted([f for f in img_folder.iterdir() if f.suffix == \".jpg\"])\n",
    "            labels = {f.stem: f for f in lbl_folder.iterdir() if f.suffix == \".txt\"} if lbl_folder.exists() else {}\n",
    "\n",
    "            for i in range(len(images)):  \n",
    "                img_prev = images[i] if i == 0 else images[i - 1]\n",
    "                img_curr = images[i]\n",
    "                \n",
    "                img_next = images[i] if i == len(images) - 1 else images[i + 1]\n",
    "                \n",
    "                # Get corresponding label for img_curr (instead of img_next)\n",
    "                lbl_next = labels.get(img_next.stem, \"EMPTY_BOUNDING_BOX\")  \n",
    "                data_pairs.append((str(img_prev), str(img_curr), str(lbl_next)))\n",
    "\n",
    "        return data_pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of data pairs.\"\"\"\n",
    "        return len(self.data_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Loads an image pair and its corresponding label.\"\"\"\n",
    "        img_prev_path, img_curr_path, lbl_curr_path = self.data_pairs[idx]\n",
    "\n",
    "        # Load images\n",
    "        img_prev = cv2.imread(str(img_prev_path))  # BGR format\n",
    "        img_curr = cv2.imread(str(img_curr_path))\n",
    "\n",
    "        # Convert BGR to RGB\n",
    "        img_prev = cv2.cvtColor(img_prev, cv2.COLOR_BGR2RGB)\n",
    "        img_curr = cv2.cvtColor(img_curr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Resize images & adjust bounding boxes\n",
    "        img_prev, _ = self.resize_image_with_bbox(img_prev, None)\n",
    "        img_curr, label = self.resize_image_with_bbox(img_curr, lbl_curr_path)\n",
    "\n",
    "        # Convert numpy array to torch tensor (HWC → CHW)\n",
    "        img_prev = torch.from_numpy(img_prev).float().permute(2, 0, 1) / 255.0  # Normalize\n",
    "        img_curr = torch.from_numpy(img_curr).float().permute(2, 0, 1) / 255.0  # Normalize\n",
    "\n",
    "        return img_prev, img_curr, label\n",
    "\n",
    "    def resize_image_with_bbox(self, image, label_path):\n",
    "        \"\"\"Resize image while keeping aspect ratio and adjust bounding boxes.\"\"\"\n",
    "        h, w, _ = image.shape\n",
    "        new_w, new_h = IMAGE_SIZE\n",
    "\n",
    "        # Compute scaling factor while keeping aspect ratio\n",
    "        scale = min(new_w / w, new_h / h)\n",
    "        resized_w, resized_h = int(w * scale), int(h * scale)\n",
    "        image_resized = cv2.resize(image, (resized_w, resized_h))\n",
    "\n",
    "        # Create a blank canvas (padded image)\n",
    "        image_padded = np.zeros((new_h, new_w, 3), dtype=np.uint8)\n",
    "        pad_x = (new_w - resized_w) // 2\n",
    "        pad_y = (new_h - resized_h) // 2\n",
    "\n",
    "        # Paste resized image onto padded canvas\n",
    "        image_padded[pad_y:pad_y + resized_h, pad_x:pad_x + resized_w, :] = image_resized\n",
    "\n",
    "        # If no labels exist, return empty tensor\n",
    "        if label_path == \"EMPTY_BOUNDING_BOX\":\n",
    "            return image_padded, torch.zeros((1, 5))  # Empty label with shape (1,5)\n",
    "\n",
    "        # Load bounding boxes\n",
    "        bboxes = self.load_label(label_path)\n",
    "\n",
    "        # Adjust bounding boxes\n",
    "        new_bboxes = []\n",
    "        for bbox in bboxes:\n",
    "            class_id, x, y, w, h = bbox  # YOLO format: [class, x_center, y_center, width, height]\n",
    "\n",
    "            # Scale bounding box coordinates\n",
    "            x = x * scale + pad_x\n",
    "            y = y * scale + pad_y\n",
    "            w = w * scale\n",
    "            h = h * scale\n",
    "\n",
    "            new_bboxes.append([class_id, x, y, w, h])\n",
    "\n",
    "        return image_padded, torch.tensor(new_bboxes, dtype=torch.float32)\n",
    "\n",
    "    def load_label(self, label_path):\n",
    "        \"\"\"Loads bounding box labels from a text file.\"\"\"\n",
    "        boxes = []\n",
    "        with open(label_path, \"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                values = list(map(float, line.strip().split()))\n",
    "                boxes.append(values)  # Each row: [class, x_center, y_center, width, height]\n",
    "        \n",
    "        return torch.tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "# Create dataset\n",
    "dataset = AgroverseDataset(\n",
    "    image_path=image_train, \n",
    "    label_path=label_train\n",
    ")\n",
    "\n",
    "# Define Collate Function for Padding Labels\n",
    "def collate_fn(batch):\n",
    "    imgs_prev, imgs_curr, labels = zip(*batch)  # Unpack batch\n",
    "\n",
    "    imgs_prev = torch.stack(imgs_prev)  # Stack images into a tensor\n",
    "    imgs_curr = torch.stack(imgs_curr)\n",
    "\n",
    "    # Pad labels to the max number of bounding boxes in the batch\n",
    "    labels = [torch.tensor(lbl) for lbl in labels]  # Convert lists to tensors\n",
    "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=-1)  # Padding value -1\n",
    "\n",
    "    return imgs_prev, imgs_curr, labels_padded\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Iterate over data\n",
    "for img_prev, img_curr, labels in dataloader:\n",
    "    print(img_prev.shape, img_curr.shape, labels.shape)  # Expected: (32, 3, 640, 640)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23b1a70c-ce9b-4cbd-9576-feabe82f5329",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x_max is less than or equal to x_min for bbox [2.5865342e-04 4.8576389e-04 5.7237412e-05 1.0625000e-04 2.0000000e+00].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 136\u001b[0m\n\u001b[1;32m    133\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Iterate over data\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_prev, img_curr, labels \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mprint\u001b[39m(img_prev\u001b[38;5;241m.\u001b[39mshape, img_curr\u001b[38;5;241m.\u001b[39mshape, labels\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Expected: (32, 3, 640, 640)\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/nihar22951_environment/lib/python3.10/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/.conda/envs/nihar22951_environment/lib/python3.10/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.conda/envs/nihar22951_environment/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.conda/envs/nihar22951_environment/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[1], line 87\u001b[0m, in \u001b[0;36mAgroverseDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     84\u001b[0m     bboxes, class_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_label(lbl_curr_path)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Apply Albumentations transform (resize images + scale bounding boxes)\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m transformed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_prev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbboxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m img_prev \u001b[38;5;241m=\u001b[39m transformed[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     89\u001b[0m transformed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image\u001b[38;5;241m=\u001b[39mimg_curr, bboxes\u001b[38;5;241m=\u001b[39mbboxes, class_labels\u001b[38;5;241m=\u001b[39mclass_labels)\n",
      "File \u001b[0;32m~/.conda/envs/nihar22951_environment/lib/python3.10/site-packages/albumentations/core/composition.py:493\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, force_apply, *args, **data)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m need_to_run:\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[0;32m--> 493\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m    496\u001b[0m     data \u001b[38;5;241m=\u001b[39m t(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata)\n",
      "File \u001b[0;32m~/.conda/envs/nihar22951_environment/lib/python3.10/site-packages/albumentations/core/composition.py:533\u001b[0m, in \u001b[0;36mCompose.preprocess\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrict:\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(data)\n\u001b[0;32m--> 533\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preprocess_processors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_arrays(data)\n",
      "File \u001b[0;32m~/.conda/envs/nihar22951_environment/lib/python3.10/site-packages/albumentations/core/composition.py:560\u001b[0m, in \u001b[0;36mCompose._preprocess_processors\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    558\u001b[0m     processor\u001b[38;5;241m.\u001b[39mensure_data_valid(data)\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m--> 560\u001b[0m     \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/nihar22951_environment/lib/python3.10/site-packages/albumentations/core/utils.py:155\u001b[0m, in \u001b[0;36mDataProcessor.preprocess\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    153\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_label_fields_to_data(data)\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_fields) \u001b[38;5;241m&\u001b[39m \u001b[38;5;28mset\u001b[39m(data\u001b[38;5;241m.\u001b[39mkeys()):\n\u001b[0;32m--> 155\u001b[0m     data[data_name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_and_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/nihar22951_environment/lib/python3.10/site-packages/albumentations/core/bbox_utils.py:306\u001b[0m, in \u001b[0;36mBboxProcessor.check_and_convert\u001b[0;34m(self, data, shape, direction)\u001b[0m\n\u001b[1;32m    296\u001b[0m         converted_data \u001b[38;5;241m=\u001b[39m filter_bboxes(\n\u001b[1;32m    297\u001b[0m             converted_data,\n\u001b[1;32m    298\u001b[0m             shape,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    302\u001b[0m             min_height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    303\u001b[0m         )\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;66;03m# Finally check the remaining boxes\u001b[39;00m\n\u001b[0;32m--> 306\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconverted_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m converted_data\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck(data, shape)\n",
      "File \u001b[0;32m~/.conda/envs/nihar22951_environment/lib/python3.10/site-packages/albumentations/core/bbox_utils.py:314\u001b[0m, in \u001b[0;36mBboxProcessor.check\u001b[0;34m(self, data, shape)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcheck\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: np\u001b[38;5;241m.\u001b[39mndarray, shape: ShapeType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 314\u001b[0m     \u001b[43mcheck_bboxes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/nihar22951_environment/lib/python3.10/site-packages/albumentations/augmentations/utils.py:189\u001b[0m, in \u001b[0;36mhandle_empty_array.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(array) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m array\n\u001b[0;32m--> 189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/nihar22951_environment/lib/python3.10/site-packages/albumentations/core/bbox_utils.py:560\u001b[0m, in \u001b[0;36mcheck_bboxes\u001b[0;34m(bboxes)\u001b[0m\n\u001b[1;32m    558\u001b[0m invalid_bbox \u001b[38;5;241m=\u001b[39m bboxes[invalid_idx]\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m invalid_bbox[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m invalid_bbox[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 560\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_max is less than or equal to x_min for bbox \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvalid_bbox\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_max is less than or equal to y_min for bbox \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvalid_bbox\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x_max is less than or equal to x_min for bbox [2.5865342e-04 4.8576389e-04 5.7237412e-05 1.0625000e-04 2.0000000e+00]."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Define target image size\n",
    "IMAGE_SIZE = (600, 640)  # (height, width)\n",
    "\n",
    "# Define augmentation pipeline (Resize + Normalize + Convert to Tensor)\n",
    "transform = A.Compose([\n",
    "    A.Resize(IMAGE_SIZE[0], IMAGE_SIZE[1]),  # Resize images and bounding boxes\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),  # Normalize like ImageNet\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))\n",
    "\n",
    "# Dataset Paths\n",
    "image_train = \"/home/nihar22951/Online_detection/Online_detection/Datasets/Agroverse/Argoverse-1.1/images/train\"\n",
    "label_train = \"/home/nihar22951/Online_detection/Online_detection/Datasets/Agroverse/Argoverse-1.1/labels/train\"\n",
    "\n",
    "class AgroverseDataset(Dataset):\n",
    "    def __init__(self, image_path, label_path, transform=None):\n",
    "        self.image_root = Path(image_path)\n",
    "        self.label_root = Path(label_path)\n",
    "        self.transform = transform  # Optional image transformations\n",
    "\n",
    "        # Get sorted list of subdirectories\n",
    "        self.image_dirs = sorted([d for d in self.image_root.iterdir() if d.is_dir()])\n",
    "        self.label_dirs = sorted([d for d in self.label_root.iterdir() if d.is_dir()])\n",
    "\n",
    "        # Build list of (prev_img, curr_img, label) pairs\n",
    "        self.data_pairs = self.create_data_pairs()\n",
    "\n",
    "    def create_data_pairs(self):\n",
    "        \"\"\"Builds (prev, curr, label) pairs.\"\"\"\n",
    "        data_pairs = []\n",
    "        \n",
    "        for img_dir, lbl_dir in zip(self.image_dirs, self.label_dirs):\n",
    "            img_folder = img_dir / \"ring_front_center\"\n",
    "            lbl_folder = lbl_dir / \"ring_front_center\"\n",
    "\n",
    "            if not img_folder.exists():\n",
    "                continue  # Skip if images folder doesn't exist\n",
    "            \n",
    "            # Get sorted list of image and label files\n",
    "            images = sorted([f for f in img_folder.iterdir() if f.suffix == \".jpg\"])\n",
    "            labels = {f.stem: f for f in lbl_folder.iterdir() if f.suffix == \".txt\"} if lbl_folder.exists() else {}\n",
    "\n",
    "            for i in range(len(images)):  \n",
    "                img_prev = images[i] if i == 0 else images[i - 1]\n",
    "                img_curr = images[i]\n",
    "                img_next = images[i] if i == len(images) - 1 else images[i + 1]\n",
    "                \n",
    "                # Get corresponding label for img_curr (instead of img_next)\n",
    "                lbl_next = labels.get(img_next.stem, \"EMPTY_BOUNDING_BOX\")  \n",
    "                data_pairs.append((str(img_prev), str(img_curr), str(lbl_next)))\n",
    "\n",
    "        return data_pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of data pairs.\"\"\"\n",
    "        return len(self.data_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Loads an image pair and its corresponding label.\"\"\"\n",
    "        img_prev_path, img_curr_path, lbl_curr_path = self.data_pairs[idx]\n",
    "\n",
    "        # Load images\n",
    "        img_prev = cv2.imread(img_prev_path)  # BGR format\n",
    "        img_curr = cv2.imread(img_curr_path)\n",
    "\n",
    "        # Convert BGR to RGB\n",
    "        img_prev = cv2.cvtColor(img_prev, cv2.COLOR_BGR2RGB)\n",
    "        img_curr = cv2.cvtColor(img_curr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Load bounding boxes\n",
    "        if lbl_curr_path == \"EMPTY_BOUNDING_BOX\":\n",
    "            bboxes = []\n",
    "            class_labels = []\n",
    "        else:\n",
    "            bboxes, class_labels = self.load_label(lbl_curr_path)\n",
    "\n",
    "        # Apply Albumentations transform (resize images + scale bounding boxes)\n",
    "        transformed = self.transform(image=img_prev, bboxes=bboxes, class_labels=class_labels)\n",
    "        img_prev = transformed[\"image\"]\n",
    "        transformed = self.transform(image=img_curr, bboxes=bboxes, class_labels=class_labels)\n",
    "        img_curr = transformed[\"image\"]\n",
    "\n",
    "        # Convert bounding boxes to tensor\n",
    "        if bboxes:\n",
    "            label_tensor = torch.tensor([class_labels] + bboxes, dtype=torch.float32)\n",
    "        else:\n",
    "            label_tensor = torch.zeros((1, 5))  # Empty label\n",
    "\n",
    "        return img_prev, img_curr, label_tensor\n",
    "\n",
    "    def load_label(self, label_path):\n",
    "        \"\"\"Loads bounding box labels from a text file.\"\"\"\n",
    "        bboxes = []\n",
    "        class_labels = []\n",
    "        with open(label_path, \"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                values = list(map(float, line.strip().split()))\n",
    "                class_labels.append(values[0])  # Class label\n",
    "                bboxes.append(values[1:])  # x, y, w, h (Pascal VOC format)\n",
    "        \n",
    "        return bboxes, class_labels\n",
    "\n",
    "# Create dataset\n",
    "dataset = AgroverseDataset(\n",
    "    image_path=image_train, \n",
    "    label_path=label_train, \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Define Collate Function for Padding Labels\n",
    "def collate_fn(batch):\n",
    "    imgs_prev, imgs_curr, labels = zip(*batch)  # Unpack batch\n",
    "\n",
    "    imgs_prev = torch.stack(imgs_prev)  # Stack images into a tensor\n",
    "    imgs_curr = torch.stack(imgs_curr)\n",
    "\n",
    "    # Pad labels to the max number of bounding boxes in the batch\n",
    "    labels = [torch.tensor(lbl) for lbl in labels]  # Convert lists to tensors\n",
    "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=-1)  # Padding value -1\n",
    "\n",
    "    return imgs_prev, imgs_curr, labels_padded\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Iterate over data\n",
    "for img_prev, img_curr, labels in dataloader:\n",
    "    print(img_prev.shape, img_curr.shape, labels.shape)  # Expected: (32, 3, 640, 640)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0aecdaec-4f3a-49da-a98f-f132ccf7f34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c54d14bc-3ff4-4874-bd02-1fed2c3ef77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "# sampler = DistributedSampler(dataset, shuffle=True)\n",
    "\n",
    "# dataloader = DataLoader(dataset, batch_size=32, sampler=sampler, collate_fn=collate_fn, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84ff3cb1-03bc-4345-8599-aea0af305011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0000,  0.5000,  0.5000,  0.2000,  0.3000],\n",
      "         [ 1.0000,  0.2000,  0.3000,  0.1000,  0.1000],\n",
      "         [ 2.0000,  0.7000,  0.8000,  0.2000,  0.2000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.4000,  0.5000,  0.2000,  0.2000],\n",
      "         [ 1.0000,  0.3000,  0.2000,  0.1500,  0.1000],\n",
      "         [ 1.0000,  0.8000,  0.9000,  0.2500,  0.2500],\n",
      "         [ 2.0000,  0.6000,  0.6000,  0.2000,  0.2000],\n",
      "         [ 0.0000,  0.1000,  0.1000,  0.1000,  0.1000]]])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "# Example bounding box sequences (each tensor is an image's bboxes)\n",
    "bboxes = [\n",
    "    torch.tensor([[0, 0.5, 0.5, 0.2, 0.3],  # Image 1 (3 objects)\n",
    "                  [1, 0.2, 0.3, 0.1, 0.1],\n",
    "                  [2, 0.7, 0.8, 0.2, 0.2]]),\n",
    "    \n",
    "    torch.tensor([[0, 0.4, 0.5, 0.2, 0.2],  # Image 2 (5 objects)\n",
    "                  [1, 0.3, 0.2, 0.15, 0.1],\n",
    "                  [1, 0.8, 0.9, 0.25, 0.25],\n",
    "                  [2, 0.6, 0.6, 0.2, 0.2],\n",
    "                  [0, 0.1, 0.1, 0.1, 0.1]])\n",
    "]\n",
    "\n",
    "# Pad sequences to match the max number of bboxes (5 in this case)\n",
    "padded_bboxes = pad_sequence(bboxes, batch_first=True, padding_value=-1)\n",
    "\n",
    "print(padded_bboxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dd75e3-5ec7-4e48-a42e-dfdc749466a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nihar22951_environment",
   "language": "python",
   "name": "nihar22951_environment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
